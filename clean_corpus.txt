:Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: this https URL Current browse context: cs.CV References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. this https URL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn 1-step geometric task. From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations. Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning. Submission history From: Shahriar Noroozizadeh [view email][v1] Thu, 30 Oct 2025 17:40:22 UTC (16,391 KB) Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs). We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity. The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass. Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule. We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths. To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) have demonstrated exceptional capabilities across multiple domains by leveraging massive pre-training and curated fine-tuning data. However, in data-sensitive fields such as healthcare, the lack of high-quality, domain-specific training corpus hinders LLMs' adaptation for specialized applications. Meanwhile, domain experts have distilled domain wisdom into ontology rules, which formalize relationships among concepts and ensure the integrity of knowledge management repositories. Viewing LLMs as implicit repositories of human knowledge, we propose Evontree, a novel framework that leverages a small set of high-quality ontology rules to systematically extract, validate, and enhance domain knowledge within LLMs, without requiring extensive external datasets. Specifically, Evontree extracts domain ontology from raw models, detects inconsistencies using two core ontology rules, and reinforces the refined knowledge via self-distilled fine-tuning. Extensive experiments on medical QA benchmarks with Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both unmodified models and leading supervised baselines, achieving up to a 3.7% improvement in accuracy. These results confirm the effectiveness, efficiency, and robustness of our approach for low-resource domain adaptation of LLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent large language model (LLM) research has undergone an architectural shift from encoder-decoder modeling to nowadays the dominant decoder-only modeling. This rapid transition, however, comes without a rigorous comparative analysis especially \textit{from the scaling perspective}, raising concerns that the potential of encoder-decoder models may have been overlooked. To fill this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison between RedLLM, pretrained with prefix language modeling (LM), and DecLLM, pretrained with causal LM, at different model scales, ranging from $\sim$150M to $\sim$8B. Using RedPajama V1 (

:Multi-page visual documents such as manuals, brochures, presentations, and posters convey key information through layout, colors, icons, and cross-slide references. While large language models (LLMs) offer opportunities in document understanding, current systems struggle with complex, multi-page visual documents, particularly in fine-grained reasoning over elements and pages. We introduce SlideAgent, a versatile agentic framework for understanding multi-modal, multi-page, and multi-layout documents, especially slide decks. SlideAgent employs specialized agents and decomposes reasoning into three specialized levels-global, page, and element-to construct a structured, query-agnostic representation that captures both overarching themes and detailed visual or textual cues. During inference, SlideAgent selectively activates specialized agents for multi-level reasoning and integrates their outputs into coherent, context-aware answers. Extensive experiments show that SlideAgent achieves significant improvement over both proprietary (+7.9 overall) and open-source models (+9.8 overall). References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes. Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5% to 20%. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Diacritics restoration in Hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning. Despite the language's high degree of ambiguity when unvocalized, recent machine learning approaches have significantly advanced performance on this task. In this work, we present DIVRIT, a novel system for Hebrew diacritization that frames the task as a zero-shot classification problem. Our approach operates at the word level, selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set, conditioned on the surrounding textual context. A key innovation of DIVRIT is its use of a Hebrew Visual Language Model, which processes undiacritized text as an image, allowing diacritic information to be embedded directly within the input's vector representation. Through a comprehensive evaluation across various configurations, we demonstrate that the system effectively performs diacritization without relying on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting where the correct diacritized form is guaranteed to be among the provided candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic architectural enhancements and optimized training methodologies yield significant improvements in the system's overall generalization capabilities. These findings highlight the promising potential of visual representations for accurate and automated Hebrew diacritization. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Purpose: The purpose of this study was to determine if an ensemble of multiple LLM agents could be used collectively to provide a more reliable assessment of a pixel-based AI triage tool than a single LLM. Methods: 29,766 non-contrast CT head exams from fourteen hospitals were processed by a commercial intracranial hemorrhage (ICH) AI detection tool. Radiology reports were analyzed by an ensemble of eight open-source LLM models and a HIPAA compliant internal version of GPT-4o using a single multi-shot prompt that assessed for presence of ICH. 1,726 examples were manually reviewed. Performance characteristics of the eight open-source models and consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were tested for rating the performance of the triage tool. Results: The cohort consisted of 29,766 head CTs exam-report pairs. The highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78). The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76). Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3 Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522 (0.500-0.543). No statistically significant differences were observed between Top-3, Full-9, and Consensus (p > 0.05). Conclusion: An ensemble of medium to large sized open-source LLMs provides a more consistent and reliable method to derive a ground truth retrospective evaluation of a clinical AI triage tool over a single LLM alone. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at this https URL . References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Karl Marx once wrote that ``the human essence is the ensemble of social relations'', suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities, within which contexts play a constitutive and essential role. With the advent of computers and artificial intelligence, these contexts are no longer limited to purely human--human interactions: human--machine interactions are included as well. Then a central question emerges: How can machines better understand our situations and purposes? To address this challenge, researchers have recently introduced the concept of context engineering. Although it is often regarded as a recent innovation of the agent era, we argue that related practices can be traced back more than twenty years. Since the early 1990s, the field has evolved through distinct historical phases, each shaped by the intelligence level of machines: from early human--computer interaction frameworks built around primitive computers, to today's human--agent interaction paradigms driven by intelligent agents, and potentially to human--level or superhuman intelligence in the future. In this paper, we situate context engineering, provide a systematic definition, outline its historical and conceptual landscape, and examine key design considerations for practice. By addressing these questions, we aim to offer a conceptual foundation for context engineering and sketch its promising future. This paper is a stepping stone for a broader community effort toward systematic context engineering in AI systems. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) continue to advance, with an increasing number of domain-specific variants tailored for specialised tasks. However, these models often lack transparency and explainability, can be costly to fine-tune, require substantial prompt engineering, yield inconsistent results across domains, and impose significant adverse environmental impact due to their high computational demands. To address these challenges, we propose the Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three human-annotated financial corpora with distinct linguistic and contextual characteristics, BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs, underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average. Current browse context: cs.CV References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments. Current browse context: cs.SE References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) have demonstrated remarkable proficiency in language comprehension and generation; however, their widespread adoption is constrained by substantial bandwidth and computational demands. While pruning and low-rank approximation have each demonstrated promising performance individually, their synergy for LLMs remains underexplored. We introduce \underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank \underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths of both techniques: low-rank approximation compresses the model by retaining its essential structure with minimal information loss, whereas sparse optimization eliminates non-essential weights, preserving those crucial for generalization. Based on theoretical analysis, we first formulate the low-rank approximation and sparse optimization as a unified problem and solve it by iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models (7B-70B) show that SSLC, without any additional training steps, consistently surpasses standalone methods, achieving state-of-the-arts results. Notably, SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least

:Test oracle generation in non-regression testing is a longstanding challenge in software engineering, where the goal is to produce oracles that can accurately determine whether a function under test (FUT) behaves as intended for a given input. In this paper, we introduce Nexus, a novel multi-agent framework to address this challenge. Nexus generates test oracles by leveraging a diverse set of specialized agents that synthesize test oracles through a structured process of deliberation, validation, and iterative self-refinement. During the deliberation phase, a panel of four specialist agents, each embodying a distinct testing philosophy, collaboratively critiques and refines an initial set of test oracles. Then, in the validation phase, Nexus generates a plausible candidate implementation of the FUT and executes the proposed oracles against it in a secure sandbox. For any oracle that fails this execution-based check, Nexus activates an automated selfrefinement loop, using the specific runtime error to debug and correct the oracle before re-validation. Our extensive evaluation on seven diverse benchmarks demonstrates that Nexus consistently and substantially outperforms state-of-theart baselines. For instance, Nexus improves the test-level oracle accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The improved accuracy also significantly enhances downstream tasks: the bug detection rate of GPT4.1-Mini generated test oracles on HumanEval increases from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of automated program repair improves from 35.23% to 69.32%. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:With the rapid development of large language models (LLMs), various LLM-based works have been widely applied in educational fields. However, most existing LLMs and their benchmarks focus primarily on the knowledge dimension, largely neglecting the evaluation of cultivation capabilities that are essential for real-world educational scenarios. Additionally, current benchmarks are often limited to a single subject or question type, lacking sufficient diversity. This issue is particularly prominent within the Chinese context. To address this gap, we introduce OmniEduBench, a comprehensive Chinese educational benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs. The data is meticulously divided into two core dimensions: the knowledge dimension and the cultivation dimension, which contain 18.121K and 6.481K entries, respectively. Each dimension is further subdivided into 6 fine-grained categories, covering a total of 61 different subjects (41 in the knowledge and 20 in the cultivation). Furthermore, the dataset features a rich variety of question formats, including 11 common exam question types, providing a solid foundation for comprehensively evaluating LLMs' capabilities in education. Extensive experiments on 11 mainstream open-source and closed-source LLMs reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro surpassed 60\% accuracy, while in the cultivation dimension, the best-performing model, QWQ, still trailed human intelligence by nearly 30\%. These results highlight the substantial room for improvement and underscore the challenges of applying LLMs in education. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:With the increasing use of generative Artificial Intelligence (AI) methods to support science workflows, we are interested in the use of discourse-level information to find supporting evidence for AI generated scientific claims. A first step towards this objective is to examine the task of inferring discourse structure in scientific writing. In this work, we present a preliminary investigation of pretrained language model (PLM) and Large Language Model (LLM) approaches for Discourse Relation Classification (DRC), focusing on scientific publications, an under-studied genre for this task. We examine how context can help with the DRC task, with our experiments showing that context, as defined by discourse structure, is generally helpful. We also present an analysis of which scientific discourse relation types might benefit most from context. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While a multi-agent approach based on large language models (LLMs) represents a promising strategy to surpass the capabilities of single models, its success is critically dependent on synergistic team composition. However, forming optimal teams is a significant challenge, as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration. In this paper, we propose an interaction-centric framework for automatic team composition that does not require any prior knowledge including their internal architectures, training data, or task performances. Our method constructs a "language model graph" that maps relationships between models from the semantic coherence of pairwise conversations, and then applies community detection to identify synergistic model clusters. Our experiments with diverse LLMs demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations. Priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manually-curated teams based on known model specializations. Our findings provide a new basis for the automated design of collaborative multi-agent LLM teams. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on this https URL. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions. Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Language models can be used to provide interactive, personalized student feedback in educational settings. However, real-world deployment faces three key challenges: privacy concerns, limited computational resources, and the need for pedagogically valid responses. These constraints require small, open-source models that can run locally and reliably ground their outputs in correct information. We introduce SCRIBE, a framework for multi-hop, tool-augmented reasoning designed to generate valid responses to student questions about feedback reports. SCRIBE combines domain-specific tools with a self-reflective inference pipeline that supports iterative reasoning, tool use, and error recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models achieve comparable or superior quality to much larger models in key dimensions such as relevance and actionability, while being perceived on par with GPT-4o and Llama-3.3 70B by students. These findings demonstrate the viability of SCRIBE for low-resource, privacy-sensitive educational applications. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and this http URL. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent work has shown that different large language models (LLMs) converge to similar and accurate input embedding representations for numbers. These findings conflict with the documented propensity of LLMs to produce erroneous outputs when dealing with numeric information. In this work, we aim to explain this conflict by exploring how language models manipulate numbers and quantify the lower bounds of accuracy of these mechanisms. We find that despite surfacing errors, different language models learn interchangeable representations of numbers that are systematic, highly accurate and universal across their hidden states and the types of input contexts. This allows us to create universal probes for each LLM and to trace information -- including the causes of output errors -- to specific layers. Our results lay a fundamental understanding of how pre-trained LLMs manipulate numbers and outline the potential of more accurate probing techniques in addressed refinements of LLMs' architectures. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) commonly boost reasoning via sample-evaluate-ensemble decoders, achieving label free gains without ground truth. However, prevailing strategies score candidates using only external outputs such as token probabilities, entropies, or self evaluations, and these signals can be poorly calibrated after post training. We instead analyze internal behavior based on neuron activations and uncover three findings: (1) external signals are low dimensional projections of richer internal dynamics; (2) correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and (3) activations from correct responses exhibit stronger cross sample agreement, whereas incorrect ones diverge. Motivated by these observations, we propose Neuron Agreement Decoding (NAD), an unsupervised best-of-N method that selects candidates using activation sparsity and cross sample neuron agreement, operating solely on internal signals and without requiring comparable textual outputs. NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping. Across math and science benchmarks with verifiable answers, NAD matches majority voting; on open ended coding benchmarks where majority voting is inapplicable, NAD consistently outperforms Avg@64. By pruning unpromising trajectories early, NAD reduces token usage by 99% with minimal loss in generation quality, showing that internal signals provide reliable, scalable, and efficient guidance for label free ensemble decoding. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Watermarking schemes for large language models (LLMs) have been proposed to identify the source of the generated text, mitigating the potential threats emerged from model theft. However, current watermarking solutions hardly resolve the trust issue: the non-public watermark detection cannot prove itself faithfully conducting the detection. We observe that it is attributed to the secret key mostly used in the watermark detection -- it cannot be public, or the adversary may launch removal attacks provided the key; nor can it be private, or the watermarking detection is opaque to the public. To resolve the dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP), enabling the watermark detection process to be publicly verifiable by third parties without disclosing any secret key. PVMark hinges upon the proof of `correct execution' of watermark detection on which a set of ZKP constraints are built, including mapping, random number generation, comparison, and summation. We implement multiple variants of PVMark in Python, Rust and Circom, covering combinations of three watermarking schemes, three hash functions, and four ZKP protocols, to show our approach effectively works under a variety of circumstances. By experimental results, PVMark efficiently enables public verifiability on the state-of-the-art LLM watermarking schemes yet without compromising the watermarking performance, promising to be deployed in practice. Current browse context: cs.CR References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Vision-language models (VLMs) exhibit uneven performance across languages, a problem that is often exacerbated when the model size is reduced. While Knowledge distillation (KD) demonstrates promising results in transferring knowledge from larger to smaller VLMs, applying KD in multilingualism is an underexplored area. This paper presents a controlled empirical study of KD behavior across five distillation approaches, isolating their effects on cross-lingual representation consistency and downstream performance stability under model compression. We study five distillation formulations across CLIP and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual QA. We find that some configurations preserve or even improve multilingual retrieval robustness despite halving model size, but others fail to maintain cross-task stability, exposing design-sensitive trade-offs that aggregate accuracy alone does not reveal. Submission history From: Peerat Limkonchotiwat [view email][v1] Thu, 30 Oct 2025 08:56:06 UTC (27,294 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Throughout language history, words are borrowed from one language to another and gradually become integrated into the recipient's lexicon. Speakers can often differentiate these loanwords from native vocabulary, particularly in bilingual communities where a dominant language continuously imposes lexical items on a minority language. This paper investigates whether pretrained language models, including large language models, possess similar capabilities for loanword identification. We evaluate multiple models across 10 languages. Despite explicit instructions and contextual information, our results show that models perform poorly in distinguishing loanwords from native ones. These findings corroborate previous evidence that modern NLP systems exhibit a bias toward loanwords rather than native equivalents. Our work has implications for developing NLP tools for minority languages and supporting language preservation in communities under lexical pressure from dominant languages. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6\% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Retrieval-augmented generation (RAG) has emerged as a leading approach to reducing hallucinations in large language models (LLMs). Current RAG evaluation benchmarks primarily focus on what we call local RAG: retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks. However, many real-world applications require a fundamentally different capability -- global RAG -- which involves aggregating and analyzing information across entire document collections to derive corpus-level insights (for example, "What are the top 10 most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first benchmark specifically designed to evaluate global RAG capabilities, covering four core task types: counting, extremum queries, sorting, and top-k extraction. Through systematic evaluation across different models and baselines, we find that existing RAG methods perform poorly on global tasks, with the strongest baseline achieving only

:Human feedback can alter language models in unpredictable and undesirable ways, as practitioners lack a clear understanding of what feedback data encodes. While prior work studies preferences over certain attributes (e.g., length or sycophancy), automatically extracting relevant features without pre-specifying hypotheses remains challenging. We introduce What's In My Human Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders. WIMHF characterizes both (1) the preferences a dataset is capable of measuring and (2) the preferences that the annotators actually express. Across 7 datasets, WIMHF identifies a small number of human-interpretable features that account for the majority of the preference prediction signal achieved by black-box models. These features reveal a wide diversity in what humans prefer, and the role of dataset-level context: for example, users on Reddit prefer informality and jokes, while annotators in HH-RLHF and PRISM disprefer them. WIMHF also surfaces potentially unsafe preferences, such as that LMArena users tend to vote against refusals, often in favor of toxic content. The learned features enable effective data curation: re-labeling the harmful examples in Arena yields large safety gains (+37%) with no cost to general performance. They also allow fine-grained personalization: on the Community Alignment dataset, we learn annotator-specific weights over subjective features that improve preference prediction. WIMHF provides a human-centered analysis method for practitioners to better understand and use preference data. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While diffusion language models (DLMs) enable fine-grained refinement, their practical controllability remains fragile. We identify and formally characterize a central failure mode called update forgetting, in which uniform and context agnostic updates induce token level fluctuations across timesteps, erasing earlier semantic edits and disrupting the cumulative refinement process, thereby degrading fluency and coherence. As this failure originates in uniform and context agnostic updates, effective control demands explicit token ordering. We propose Token Timestep Allocation (TTA), which realizes soft and semantic token ordering via per token timestep schedules: critical tokens are frozen early, while uncertain tokens receive continued refinement. This timestep based ordering can be instantiated as either a fixed policy or an adaptive policy driven by task signals, thereby supporting a broad spectrum of refinement strategies. Because it operates purely at inference time, it applies uniformly across various DLMs and naturally extends to diverse supervision sources. Empirically, TTA improves controllability and fluency: on sentiment control, it yields more than 20 percent higher accuracy and nearly halves perplexity using less than one fifth the steps; in detoxification, it lowers maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0). Together, these results demonstrate that softened ordering via timestep allocation is the critical lever for mitigating update forgetting and achieving stable and controllable diffusion text generation. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Current LLM evaluations often rely on a single instruction template, overlooking models' sensitivity to instruction style-a critical aspect for real-world deployments. We present RCScore, a multi-dimensional framework quantifying how instruction formulation affects model responses. By systematically transforming benchmark problems into multiple instruction styles, RCScore reveals performance variations undetected by conventional metrics. Our experiments across ten LLMs on four reasoning benchmarks demonstrate that instruction style can shift accuracy by up to 16.7% points. We introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to measure stylistic self-consistency, and establish its strong correlation with task accuracy, suggesting consistency as a valuable proxy for model reliability. Additional findings show that deterministic decoding produces more stylistically stable outputs, and model scale correlates positively with cross-style consistency. RCScore offers a principled approach to assess instruction robustness. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility. Submission history From: Hitomi Jin Ling Tee [view email][v1] Thu, 30 Oct 2025 06:57:07 UTC (314 KB) Current browse context: cs.SD References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which are sequence prediction models fine-tuned to maximize the proportion of generations in the well-calibrated, high-probability region partitioned by a final-layer SDM activation layer used for binary classification of instruction-following. We demonstrate that existing pre-trained decoder-only Transformer LMs can be readily converted into SDM LMs via supervised fine-tuning, using the final-layer SDM activation layer during training to estimate a change-of-base for a supervised next-token loss over a contrastive input encoding scheme, with additional hard negative examples generated online during training. This results in reduced abstentions (i.e., improved statistical efficiency) compared to strong supervised baselines. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) have significantly advanced generative applications in natural language processing (NLP). Recent trends in model architectures revolve around efficient variants of transformers or state-space/gated-recurrent models (SSMs, GRMs). However, prevailing SSM/GRM-based methods often emulate only a single attention head, potentially limiting their expressiveness. In this work, we propose MossNet, a novel mixture-of-state-space-experts architecture that emulates a linear multi-head attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation not only in channel-mixing multi-layered perceptron (MLP) blocks but also in the time-mixing SSM kernels to realize multiple "attention heads." Extensive experiments on language modeling and downstream evaluations show that MossNet outperforms both transformer- and SSM-based architectures of similar model size and data budgets. Larger variants of MossNet, trained on trillions of tokens, further confirm its scalability and superior performance. In addition, real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU demonstrate favorable runtime speed and resource usage compared to similarly sized baselines. Our results suggest that MossNet is a compelling new direction for efficient, high-performing recurrent LLM architectures. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reinforcement learning (RL) can elicit strong reasoning in large language models (LLMs), yet most open efforts focus on math and code. We propose Reasoning Curriculum, a simple two-stage curriculum that first elicits reasoning skills in pretraining-aligned domains such as math, then adapts and refines these skills across other domains via joint RL. Stage 1 performs a brief cold start and then math-only RL with verifiable rewards to develop reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and consolidate these skills. The curriculum is minimal and backbone-agnostic, requiring no specialized reward models beyond standard verifiability checks. Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning curriculum yields consistent gains. Ablations and a cognitive-skill analysis indicate that both stages are necessary and that math-first elicitation increases cognitive behaviors important for solving complex problems. Reasoning Curriculum provides a compact, easy-to-adopt recipe for general reasoning. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:This paper investigates the relationship between Persuasion Techniques (PTs) and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and prompt engineering. Since no dataset annotated with both PTs and DRs exists, we took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point and developed LLM-based classifiers to label each instance of the dataset with one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10 different prompts, resulting in 40 unique DR classifiers. Ensemble models using different majority-pooling strategies were used to create 5 silver datasets of instances labelled with both persuasion techniques and level-2 PDTB senses. The silver dataset sizes vary from 1,281 instances to 204 instances, depending on the majority pooling technique used. Statistical analysis of these silver datasets shows that six discourse relations (namely Cause, Purpose, Contrast, Cause+Belief, Concession, and Condition) play a crucial role in persuasive texts, especially in the use of Loaded Language, Exaggeration/Minimisation, Repetition and to cast Doubt. This insight can contribute to detecting online propaganda and misinformation, as well as to our general understanding of effective communication. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common "one problem, one solution" (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a "one problem, multiple solutions" (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at this https URL . References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) have increasingly been applied to automatic programming code generation. This task can be viewed as a language generation task that bridges natural language, human knowledge, and programming logic. However, it remains underexplored in domains that require interaction with hardware devices, such as quantum programming, where human coders write Python code that is executed on a quantum computer. To address this gap, we introduce QCoder Benchmark, an evaluation framework that assesses LLMs on quantum programming with feedback from simulated hardware devices. Our benchmark offers two key features. First, it supports evaluation using a quantum simulator environment beyond conventional Python execution, allowing feedback of domain-specific metrics such as circuit depth, execution time, and error classification, which can be used to guide better generation. Second, it incorporates human-written code submissions collected from real programming contests, enabling both quantitative comparisons and qualitative analyses of LLM outputs against human-written codes. Our experiments reveal that even advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting the difficulty of the benchmark. In contrast, reasoning-based models such as o3 reach up to 78% accuracy, outperforming averaged success rates of human-written codes (39.98%). We release the QCoder Benchmark dataset and public evaluation API to support further research. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recommender systems are among the most impactful AI applications, interacting with billions of users every day, guiding them to relevant products, services, or information tailored to their preferences. However, the research and development of recommender systems are hindered by existing datasets that fail to capture realistic user behaviors and inconsistent evaluation settings that lead to ambiguous conclusions. This paper introduces the Open Recommendation Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified benchmark for consistent and realistic evaluation of recommendation models. ORBIT offers a standardized evaluation framework of public datasets with reproducible splits and transparent settings for its public leaderboard. Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco, featuring web browsing sequences from 87 million public, high-quality webpages. ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and privacy-guaranteed browsing data. It aligns with modern recommendation scenarios and is reserved as the hidden test part of our leaderboard to challenge recommendation models' generalization ability. ORBIT measures 12 representative recommendation models on its public benchmark and introduces a prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results reflect general improvements of recommender systems on the public datasets, with variable individual performances. The results on the hidden test reveal the limitations of existing approaches in large-scale webpage recommendation and highlight the potential for improvements with LLM integrations. ORBIT benchmark, leaderboard, and codebase are available at this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Knowledge distillation (KD) is an effective method for model compression and transferring knowledge between models. However, its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored. This study investigates the effect of knowledge distillation on the transferability of ``debiasing'' capabilities from teacher models to student models on natural language inference (NLI) and image classification tasks. Through extensive experiments, we illustrate several key findings: (i) overall the debiasing capability of a model is undermined post-KD; (ii) training a debiased model does not benefit from injecting teacher knowledge; (iii) although the overall robustness of a model may remain stable post-distillation, significant variations can occur across different types of biases; and (iv) we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD. Given the above findings, we propose three effective solutions to improve the distillability of debiasing methods: developing high quality data for augmentation, implementing iterative knowledge distillation, and initializing student models with weights obtained from teacher models. To the best of our knowledge, this is the first study on the effect of KD on debiasing and its interenal mechanism at scale. Our findings provide understandings on how KD works and how to design better debiasing methods. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models. Current browse context: cs.CR References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Importance Incidental thyroid findings (ITFs) are increasingly detected on imaging performed for non-thyroid indications. Their prevalence, features, and clinical consequences remain undefined. Objective To develop, validate, and deploy a natural language processing (NLP) pipeline to identify ITFs in radiology reports and assess their prevalence, features, and clinical outcomes. Design, Setting, and Participants Retrospective cohort of adults without prior thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline identified ITFs and extracted nodule characteristics from image reports from multiple modalities and body regions. Main Outcomes and Measures Prevalence of ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer diagnosis. Logistic regression identified demographic and imaging-related factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9% women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more likely in women, older adults, those with higher BMI, and when imaging was ordered by oncology or internal medicine. Compared with chest CT, ITFs were more likely via neck CT, PET, and nuclear medicine scans. Nodule characteristics were poorly documented, with size reported in 44% and other features in fewer than 15% (e.g. calcifications). Compared with patients without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis, biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were common and strongly associated with cascades leading to the detection of small, low-risk cancers. These findings underscore the role of ITFs in thyroid cancer overdiagnosis and the need for standardized reporting and more selective follow-up. Submission history From: Oscar Ponce-Ponte [view email][v1] Thu, 30 Oct 2025 00:15:07 UTC (1,000 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Cross-lingual alignment (CLA) aims to align multilingual representations, enabling Large Language Models (LLMs) to seamlessly transfer knowledge across languages. While intuitive, we hypothesize, this pursuit of representational convergence can inadvertently cause "cultural erasure", the functional loss of providing culturally-situated responses that should diverge based on the query language. In this work, we systematically analyze this trade-off by introducing a holistic evaluation framework, the transfer-localization plane, which quantifies both desirable knowledge transfer and undesirable cultural erasure. Using this framework, we re-evaluate recent CLA approaches and find that they consistently improve factual transfer at the direct cost of cultural localization across all six languages studied. Our investigation into the internal representations of these models reveals a key insight: universal factual transfer and culturally-specific knowledge are optimally steerable at different model layers. Based on this finding, we propose Surgical Steering, a novel inference-time method that disentangles these two objectives. By applying targeted activation steering to distinct layers, our approach achieves a better balance between the two competing dimensions, effectively overcoming the limitations of current alignment techniques. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Humans can naturally identify, reason about, and explain anomalies in their environment. In computer vision, this long-standing challenge remains limited to industrial defects or unrealistic, synthetically generated anomalies, failing to capture the richness and unpredictability of real-world anomalies. In this work, we introduce CAVE, the first benchmark of real-world visual anomalies. CAVE supports three open-ended tasks: anomaly description, explanation, and justification; with fine-grained annotations for visual grounding and categorizing anomalies based on their visual manifestations, their complexity, severity, and commonness. These annotations draw inspiration from cognitive science research on how humans identify and resolve anomalies, providing a comprehensive framework for evaluating Vision-Language Models (VLMs) in detecting and understanding anomalies. We show that state-of-the-art VLMs struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies. By offering a realistic and cognitively grounded benchmark, CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs. Submission history From: Syrielle Montariol [view email][v1] Wed, 29 Oct 2025 22:34:26 UTC (19,134 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of

:AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average

:Large Language Models (LLMs) often struggle with complex mathematical reasoning, where prose-based generation leads to unverified and arithmetically unsound solutions. Current prompting strategies like Chain of Thought still operate within this unreliable medium, lacking a mechanism for deterministic verification. To address these limitations, we introduce SymCode, a neurosymbolic framework that reframes mathematical problem-solving as a task of verifiable code generation using the SymPy library. We evaluate SymCode on challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating significant accuracy improvements of up to 13.6 percentage points over baselines. Our analysis shows that SymCode is not only more token-efficient but also fundamentally shifts model failures from opaque logical fallacies towards transparent, programmatic errors. By grounding LLM reasoning in a deterministic symbolic engine, SymCode represents a key step towards more accurate and trustworthy AI in formal domains. Submission history From: Sina Bagheri Nezhad [view email][v1] Wed, 29 Oct 2025 21:17:57 UTC (817 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Machine Translation (MT) is widely employed to address resource scarcity in low-resource languages by generating synthetic data from high-resource counterparts. While sentiment preservation in translation has long been studied, a critical but underexplored factor is the role of cultural alignment between source and target languages. In this paper, we hypothesize that semantic labels are drifted or altered during MT due to cultural divergence. Through a series of experiments across culturally sensitive and neutral domains, we establish three key findings: (1) MT systems, including modern Large Language Models (LLMs), induce label drift during translation, particularly in culturally sensitive domains; (2) unlike earlier statistical MT tools, LLMs encode cultural knowledge, and leveraging this knowledge can amplify label drift; and (3) cultural similarity or dissimilarity between source and target languages is a crucial determinant of label preservation. Our findings highlight that neglecting cultural factors in MT not only undermines label fidelity but also risks misinterpretation and cultural conflict in downstream applications. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The impact of different multilingual data mixtures in pretraining large language models (LLMs) has been a topic of ongoing debate, often raising concerns about potential trade-offs between language coverage and model performance (i.e., the curse of multilinguality). In this work, we investigate these assumptions by training

:If we cannot inspect the training data of a large language model (LLM), how can we ever know what it has seen? We believe the most compelling evidence arises when the model itself freely reproduces the target content. As such, we propose RECAP, an agentic pipeline designed to elicit and verify memorized training data from LLM outputs. At the heart of RECAP is a feedback-driven loop, where an initial extraction attempt is evaluated by a secondary language model, which compares the output against a reference passage and identifies discrepancies. These are then translated into minimal correction hints, which are fed back into the target model to guide subsequent generations. In addition, to address alignment-induced refusals, RECAP includes a jailbreaking module that detects and overcomes such barriers. We evaluate RECAP on EchoTrace, a new benchmark spanning over 30 full books, and the results show that RECAP leads to substantial gains over single-iteration approaches. For instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text extraction improved from 0.38 to 0.47 - a nearly 24% increase. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenisation, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the this http URL employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantisation. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance this http URL providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The use of LLM-based applications as a means to accelerate and/or substitute human labor in the creation of language resources and dataset is a reality. Nonetheless, despite the potential of such tools for linguistic research, comprehensive evaluation of their performance and impact on the creation of annotated datasets, especially under a perspectivized approach to NLP, is still missing. This paper contributes to reduction of this gap by reporting on an extensive evaluation of the (semi-)automatization of FrameNet-like semantic annotation by the use of an LLM-based semantic role labeler. The methodology employed compares annotation time, coverage and diversity in three experimental settings: manual, automatic and semi-automatic annotation. Results show that the hybrid, semi-automatic annotation setting leads to increased frame diversity and similar annotation coverage, when compared to the human-only setting, while the automatic setting performs considerably worse in all metrics, except for annotation time. Submission history From: Tiago Timponi Torrent [view email][v1] Wed, 29 Oct 2025 19:13:48 UTC (10,743 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Aligning LLM-based judges with human preferences is a significant challenge, as they are difficult to calibrate and often suffer from rubric sensitivity, bias, and instability. Overcoming this challenge advances key applications, such as creating reliable reward models for Reinforcement Learning from Human Feedback (RLHF) and building effective routing systems that select the best-suited model for a given user query. In this work, we propose a framework for modeling diverse, persona-based preferences by learning to aggregate outputs from multiple rubric-conditioned judges. We investigate the performance of this approach against naive baselines and assess its robustness through case studies on both human and LLM-judges biases. Our primary contributions include a persona-based method for synthesizing preference labels at scale and two distinct implementations of our aggregator: Generalized Additive Model (GAM) and a Multi-Layer Perceptron (MLP). Current browse context: cs.AI References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) are increasingly used as raters for evaluation tasks. However, their reliability is often limited for subjective tasks, when human judgments involve subtle reasoning beyond annotation labels. Thinking traces, the reasoning behind a judgment, are highly informative but challenging to collect and curate. We present a human-LLM collaborative framework to infer thinking traces from label-only annotations. The proposed framework uses a simple and effective rejection sampling method to reconstruct these traces at scale. These inferred thinking traces are applied to two complementary tasks: (1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation guidelines for proprietary LLM raters. Across multiple datasets, our methods lead to significantly improved LLM-human agreement. Additionally, the refined annotation guidelines increase agreement among different LLM models. These results suggest that LLMs can serve as practical proxies for otherwise unrevealed human thinking traces, enabling label-only corpora to be extended into thinking-trace-augmented resources that enhance the reliability of LLM raters. Current browse context: cs.AI References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes

:Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams. Code: this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Current evaluations of agents remain centered around one-shot task completion, failing to account for the inherently iterative and collaborative nature of many real-world problems, where human goals are often underspecified and evolve. We argue for a shift from building and assessing task completion agents to developing collaborative agents, assessed not only by the quality of their final outputs but by how well they engage with and enhance human effort throughout the problem-solving process. To support this shift, we introduce collaborative effort scaling, a framework that captures how an agent's utility grows with increasing user involvement. Through case studies and simulated evaluations, we show that state-of-the-art agents often underperform in multi-turn, real-world scenarios, revealing a missing ingredient in agent design: the ability to sustain engagement and scaffold user understanding. Collaborative effort scaling offers a lens for diagnosing agent behavior and guiding development toward more effective interactions. Submission history From: Zejiang Shen [view email][v1] Wed, 29 Oct 2025 17:47:18 UTC (3,364 KB) [v2] Thu, 30 Oct 2025 17:54:45 UTC (3,372 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro

:Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) are increasingly explored as flexible alternatives to classical machine learning models for classification tasks through zero-shot prompting. However, their suitability for structured tabular data remains underexplored, especially in high-stakes financial applications such as financial risk assessment. This study conducts a systematic comparison between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art gradient-boosting model, on a real-world loan default prediction task. We evaluate their predictive performance, analyze feature attributions using SHAP, and assess the reliability of LLM-generated self-explanations. While LLMs are able to identify key financial risk indicators, their feature importance rankings diverge notably from LightGBM, and their self-explanations often fail to align with empirical SHAP attributions. These findings highlight the limitations of LLMs as standalone models for structured financial risk prediction and raise concerns about the trustworthiness of their self-generated explanations. Our results underscore the need for explainability audits, baseline comparisons with interpretable models, and human-in-the-loop oversight when deploying LLMs in risk-sensitive financial environments. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: this https URL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents. Current browse context: cs.SE References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Codes are available at this https URL. Submission history From: Kunpeng Qiu [view email][v1] Wed, 29 Oct 2025 16:47:02 UTC (31,755 KB) [v2] Thu, 30 Oct 2025 14:28:46 UTC (31,755 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a large-model encoder for Wi-Fi channel state information (and optionally mmWave radar or RFID) with a policy-grounded decision layer and end-to-end zero-knowledge proofs of inference. The encoder uses masked spectral pretraining with phase-consistency regularization, plus a light cross-modal alignment that ties RF features to compact, human-interpretable policy tokens. To reduce unsafe actions under distribution shift, we add a calibrated selective-abstention head; the chosen risk-coverage operating point is registered and bound into the proof. We implement a four-stage proving pipeline: (C1) feature sanity and commitment, (C2) threshold and version binding, (C3) time-window binding, and (C4) PLONK-style proofs that the quantized network, given the committed window, produced the logged action and confidence. Micro-batched proving amortizes cost across adjacent windows, and a gateway option offloads proofs from low-power devices. The system integrates with differentially private federated learning and on-device personalization without weakening verifiability: model hashes and the registered threshold are part of each public statement. Across activity, presence or intrusion, respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1 and calibration, yields favorable coverage-risk curves under perturbations, and rejects tamper and replay with compact proofs and fast verification. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Electronic Health Records (EHR) store clinical documentation as base64 encoded attachments in FHIR DocumentReference resources, which makes semantic question answering difficult. Traditional vector database methods often miss nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR) method, introduced by Lopez et al. 2025, uses entity aware retrieval and achieved improved performance with an F1 score of 0.90 versus 0.86 for embedding based retrieval, while using over 70 percent fewer tokens. We developed a Clinical Notes QA Evaluation Platform to validate CLEAR against zero shot large context inference and traditional chunk based retrieval augmented generation. The platform was tested on 12 clinical notes ranging from 10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a 58.3 percent win rate, an average semantic similarity of 0.878, and used 78 percent fewer tokens than wide context processing. The largest performance gains occurred on long notes, with a 75 percent win rate for documents exceeding 65,000 tokens. These findings confirm that entity aware retrieval improves both efficiency and accuracy in clinical natural language processing. The evaluation framework provides a reusable and transparent benchmark for assessing clinical question answering systems where semantic precision and computational efficiency are critical. Submission history From: Tarun Kumar Chawdhury [view email][v1] Wed, 29 Oct 2025 16:41:44 UTC (2,611 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of human-like reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains. Submission history From: Mohammad Aghajani Asl [view email][v1] Wed, 29 Oct 2025 15:25:34 UTC (940 KB) Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. this https URL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception. Current browse context: eess.AS References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the entire recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC, residing in an exponentially large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit measurements, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, adopting projective measurements as a limiting case to obtain mid-circuit readouts while maintaining a coherent recurrent quantum memory. We further devise a soft attention mechanism over the mid-circuit readouts in a sequence-to-sequence model and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating an individual's communication style, behavioral tendencies, and personality traits. However, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains considerably below the human baseline. Submission history From: Bangde Du [view email][v1] Wed, 29 Oct 2025 14:00:42 UTC (1,867 KB) [v2] Thu, 30 Oct 2025 11:19:24 UTC (1,867 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:This paper presents a pipeline integrating fine-tuned large language models (LLMs) with named entity recognition (NER) for efficient domain-specific text summarization and tagging. The authors address the challenge posed by rapidly evolving sub-cultural languages and slang, which complicate automated information extraction and law enforcement monitoring. By leveraging the LLaMA Factory framework, the study fine-tunes LLMs on both generalpurpose and custom domain-specific datasets, particularly in the political and security domains. The models are evaluated using BLEU and ROUGE metrics, demonstrating that instruction fine-tuning significantly enhances summarization and tagging accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct model, despite its initial limitations in Chinese comprehension, outperforms its Chinese-trained counterpart after domainspecific fine-tuning, suggesting that underlying reasoning capabilities can transfer across languages. The pipeline enables concise summaries and structured entity tagging, facilitating rapid document categorization and distribution. This approach proves scalable and adaptable for real-time applications, supporting efficient information management and the ongoing need to capture emerging language trends. The integration of LLMs and NER offers a robust solution for transforming unstructured text into actionable insights, crucial for modern knowledge management and security operations. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Audio Descriptions (ADs) convey essential on-screen information, allowing visually impaired audiences to follow videos. To be effective, ADs must form a coherent sequence that helps listeners to visualise the unfolding scene, rather than describing isolated moments. However, most automatic methods generate each AD independently, often resulting in repetitive, incoherent descriptions. To address this, we propose a training-free method, CoherentAD, that first generates multiple candidate descriptions for each AD time interval, and then performs auto-regressive selection across the sequence to form a coherent and informative narrative. To evaluate AD sequences holistically, we introduce a sequence-level metric, StoryRecall, which measures how well the predicted ADs convey the ground truth narrative, alongside repetition metrics that capture the redundancy across consecutive AD outputs. Our method produces coherent AD sequences with enhanced narrative understanding, outperforming prior approaches that rely on independent generations. Submission history From: Eshika Khandelwal [view email][v1] Wed, 29 Oct 2025 12:06:42 UTC (15,998 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) are increasingly utilized by researchers across a wide range of domains, and qualitative social science is no exception; however, this adoption faces persistent challenges, including interpretive bias, low reliability, and weak auditability. We introduce a framework that situates LLM usage along two dimensions, interpretive depth and autonomy, thereby offering a straightforward way to classify LLM applications in qualitative research and to derive practical design recommendations. We present the state of the literature with respect to these two dimensions, based on all published social science papers available on Web of Science that use LLMs as a tool and not strictly as the subject of study. Rather than granting models expansive freedom, our approach encourages researchers to decompose tasks into manageable segments, much as they would when delegating work to capable undergraduate research assistants. By maintaining low levels of autonomy and selectively increasing interpretive depth only where warranted and under supervision, one can plausibly reap the benefits of LLMs while preserving transparency and reliability. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The rapid advancement of Large Language Models (LLMs) is positioning language at the core of human-computer interaction (HCI). We argue that advancing HCI requires attention to the linguistic foundations of interaction, particularly implicature (meaning conveyed beyond explicit statements through shared context) which is essential for human-AI (HAI) alignment. This study examines LLMs' ability to infer user intent embedded in context-driven prompts and whether understanding implicature improves response generation. Results show that larger models approximate human interpretations more closely, while smaller models struggle with implicature inference. Furthermore, implicature-based prompts significantly enhance the perceived relevance and quality of responses across models, with notable gains in smaller models. Overall, 67.6% of participants preferred responses with implicature-embedded prompts to literal ones, highlighting a clear preference for contextually nuanced communication. Our work contributes to understanding how linguistic theory can be used to address the alignment problem by making HAI interaction more natural and contextually grounded. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated annotation and filtering framework that utilizes VLMs to reduce reliance on manual effort while preserving data quality. Our method is applied to TikTok videos across eight sign languages and to the already curated YouTube-SL-25 dataset in German Sign Language for the purpose of additional evaluation. Our VLM-based pipeline includes a face visibility detection, a sign activity recognition, a text extraction from video content, and a judgment step to validate alignment between video and text, implementing generic filtering, annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we assess the performance of two off-the-shelf SLT models on our filtered dataset for German and American Sign Languages, with the goal of establishing baselines and evaluating the robustness of recent models on automatically extracted, slightly noisy data. Our work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research. Submission history From: Kundeshwar Pundalik Mr [view email][v1] Wed, 29 Oct 2025 11:27:08 UTC (3,886 KB) [v2] Thu, 30 Oct 2025 10:48:05 UTC (3,886 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The development of AI for mental health is hindered by a lack of authentic therapy dialogues, due to strict privacy regulations and the fact that clinical sessions were historically rarely recorded. We present an LLM-driven pipeline that generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires. Grounded on the principles of Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic conversations for clinical disorders such as anxiety and depression. Our framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts structured psychological input into natural language dialogues through therapist-client simulations. Due to data governance policies and privacy restrictions prohibiting the transmission of clinical questionnaire data to third-party services, previous methodologies relying on proprietary models are infeasible in our setting. We address this limitation by generating a high-quality corpus using open-weight LLMs, validated through human expert evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on SQPsychConv achieve strong performance on counseling benchmarks, surpassing baselines in key therapeutic skills. Our findings highlight the potential of synthetic data to enable scalable, data-secure, and clinically informed AI for mental health support. We will release our code, models, and corpus at this https URL Submission history From: Doan Nam Long Vu [view email][v1] Wed, 29 Oct 2025 10:55:52 UTC (1,293 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in bibliographic recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic information depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record is repeatedly represented in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 bibliographic records across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts into memorization. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Legal interpretation frequently involves assessing how a legal text, as understood by an 'ordinary' speaker of the language, applies to the set of facts characterizing a legal dispute in the U.S. judicial system. Recent scholarship has proposed that legal practitioners add large language models (LLMs) to their interpretive toolkit. This work offers an empirical argument against LLM interpretation as recently practiced by legal scholars and federal judges. Our investigation in English shows that models do not provide stable interpretive judgments: varying the question format can lead the model to wildly different conclusions. Moreover, the models show weak to moderate correlation with human judgment, with large variance across model and question variant, suggesting that it is dangerous to give much credence to the conclusions produced by generative AI. Submission history From: Abhishek Purushothama [view email][v1] Wed, 29 Oct 2025 10:21:25 UTC (1,956 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent years have witnessed the rapid development of LLM-based agents, which shed light on using language agents to solve complex real-world problems. A prominent application lies in business agents, which interact with databases and internal knowledge bases via tool calls to fulfill diverse user requirements. However, this domain is characterized by intricate data relationships and a wide range of heterogeneous tasks, from statistical data queries to knowledge-based question-answering. To address these challenges, we propose CRMWeaver, a novel approach that enhances business agents in such complex settings. To acclimate the agentic model to intricate business environments, we employ a synthesis data generation and RL-based paradigm during training, which significantly improves the model's ability to handle complex data and varied tasks. During inference, a shared memories mechanism is introduced, prompting the agent to learn from task guidelines in similar problems, thereby further boosting its effectiveness and generalization, especially in unseen scenarios. We validate the efficacy of our approach on the CRMArena-Pro dataset, where our lightweight model achieves competitive results in both B2B and B2C business scenarios, underscoring its practical value for real-world applications. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Natural language chain-of-thought (N-CoT) and Program chain-of-thought (P-CoT) have emerged as two primary paradigms for large language models (LLMs) to solve mathematical reasoning problems. Current research typically endeavors to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for mutual enhancement and ultimately achieve simultaneous improvements. We conduct a detailed analysis of the error types across two paradigms, based on which we propose Parrot, a novel training pipeline for mathematical problems: 1) Three target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A subtask hybrid training strategy to facilitate natural language semantic transferability. 3) The converted N-CoT auxiliary reward is designed to alleviate the sparse rewards in P-CoT optimization. Extensive experiments demonstrate that Parrot significantly enhances both the performance of N-CoT and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of LLaMA2 and CodeLLaMA achieve gains of +2

:Multimodal sarcasm detection is challenging, especially in low-resource settings where subtle image-text contradictions are hard to learn due to scarce annotated data, which hinders the model's performance. Parameter-efficient fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce overfitting but struggle to reach optimal performance due to limited supervision from few-shot data. We propose PEKD, a unified framework that enhances PEFT methods via distillation from an expert model trained on large-scale sarcasm data, which acts as the teacher. To mitigate unreliable signals from the teacher, we introduce an entropy-aware gating mechanism that dynamically adjusts the distillation strength based on teacher confidence. Experiments on two public datasets demonstrate that our PEKD framework enables PEFT methods to outperform both prior parameter-efficient approaches and large multimodal models, achieving strong results in the few-shot scenario. The framework is modular and adaptable to a wide range of multimodal models and tasks. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Domain-specific question answering in low-resource languages faces two key challenges: scarcity of annotated datasets and limited domain knowledge in general-purpose language models. In this work, we present a multi-stage finetuning strategy to adapt lightweight language models to the Hindi tourism domain by leveraging both original and synthetic training data. Synthetic question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and used to augment the limited original dataset. We explore several training methodologies and analyse their impact on domain generalisation. Our results demonstrate that large models can efficiently generate synthetic data, while small models can effectively adapt to it, offering a scalable pathway for low-resource, domain-specific QA. Submission history From: Paheli Bhattacharya [view email][v1] Wed, 29 Oct 2025 08:32:22 UTC (302 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While Large Language Models (LLMs) are increasingly used in agentic frameworks to assist individual users, there is a growing need for agents that can proactively manage complex, multi-party collaboration. Systematic evaluation methods for such proactive agents remain scarce, limiting progress in developing AI that can effectively support multiple people together. Negotiation offers a demanding testbed for this challenge, requiring socio-cognitive intelligence to navigate conflicting interests between multiple participants and multiple topics and build consensus. Here, we present ProMediate, the first framework for evaluating proactive AI mediator agents in complex, multi-topic, multi-party negotiations. ProMediate consists of two core components: (i) a simulation testbed based on realistic negotiation cases and theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in socio-cognitive mediation theories, capable of flexibly deciding when and how to intervene; and (ii) a socio-cognitive evaluation framework with a new suite of metrics to measure consensus changes, intervention latency, mediator effectiveness, and intelligence. Together, these components establish a systematic framework for assessing the socio-cognitive intelligence of proactive AI agents in multi-party settings. Our results show that a socially intelligent mediator agent outperforms a generic baseline, via faster, better-targeted interventions. In the ProMediate-Hard setting, our social mediator increases consensus change by 3.6 percentage points compared to the generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response (15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous, theory-grounded testbed to advance the development of proactive, socially intelligent agents. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) are increasingly used in content moderation systems, where ensuring fairness and neutrality is essential. In this study, we examine how persona adoption influences the consistency and fairness of harmful content classification across different LLM architectures, model sizes, and content modalities (language vs. vision). At first glance, headline performance metrics suggest that personas have little impact on overall classification accuracy. However, a closer analysis reveals important behavioral shifts. Personas with different ideological leanings display distinct propensities to label content as harmful, showing that the lens through which a model "views" input can subtly shape its judgments. Further agreement analyses highlight that models, particularly larger ones, tend to align more closely with personas from the same political ideology, strengthening within-ideology consistency while widening divergence across ideological groups. To show this effect more directly, we conducted an additional study on a politically targeted task, which confirmed that personas not only behave more coherently within their own ideology but also exhibit a tendency to defend their perspective while downplaying harmfulness in opposing views. Together, these findings highlight how persona conditioning can introduce subtle ideological biases into LLM outputs, raising concerns about the use of AI systems that may reinforce partisan perspectives under the guise of neutrality. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Long-context language models unlock advanced capabilities in reasoning, code generation, and document summarization by leveraging dependencies across extended spans of text. However, a significant portion of readily available long-text data lacks meaningful long-distance dependencies; most spans can be predicted using only local context. Training on such data is inefficient, making careful data selection crucial. Therefore, we introduce LongFilter, a framework for curating training data tailored to long-context pretraining. LongFilter measures the information gain provided by extended context by contrasting model predictions under long-context versus short-context settings, thereby identifying samples where long-range dependencies are essential. Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show that LongFilter efficiently selects high-quality data and yields substantial improvements on benchmarks such as HELMET, LongBench, and RULER. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reinforcement learning (RL) can refine the reasoning abilities of large language models (LLMs), but critically depends on a key prerequisite: the LLM can already generate high-utility reasoning paths with non-negligible probability. For tasks beyond the LLM's current competence, such reasoning path can be hard to sample, and learning risks reinforcing familiar but suboptimal reasoning. We are motivated by the insight from cognitive science that Why is this the answer is often an easier question than What is the answer, as it avoids the heavy cognitive load of open-ended exploration, opting instead for explanatory reconstruction-systematically retracing the reasoning that links a question to its answer. We show that LLMs can similarly leverage answers to derive high-quality reasoning paths. We formalize this phenomenon and prove that conditioning on answer provably increases the expected utility of sampled reasoning paths, thereby transforming intractable problems into learnable ones. Building on this insight, we introduce RAVR (Reference-Answer-guided Variational Reasoning), an end-to-end framework that uses answer-conditioned reasoning as a variational surrogate for question-only reasoning. Experiments in both general and math domains demonstrate consistent improvements over strong baselines. We further analyze the reasoning behavior and find that RAVR reduces hesitation, strengthens conclusion consolidation, and promotes problem-specific strategies in reasoning. Current browse context: cs.AI References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While large language models are trained on massive datasets, this data is heavily skewed towards English. Does their impressive performance reflect genuine ability or just this data advantage? To find out, we tested them in a setting where they could not rely on data abundance: low-resource languages. Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction (NSP) as a test, we created a large-scale benchmark with 10,000 questions each for English (a high-resource language), Swahili (medium-resource), and Hausa (low-resource). We then tested several top models, including GPT-4 Turbo, Gemini

:AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents. We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning. As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation. Submission history From: Hongjin Qian [view email][v1] Wed, 29 Oct 2025 04:29:17 UTC (517 KB) [v2] Thu, 30 Oct 2025 08:52:17 UTC (517 KB) Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Discrete audio representations are gaining traction in speech modeling due to their interpretability and compatibility with large language models, but are not always optimized for noisy or real-world environments. Building on existing works that quantize Whisper embeddings for speech-to-unit modeling, we propose disentangling semantic speech content from background noise in the latent space. Our end-to-end model separates clean speech in the form of codebook tokens, while extracting interpretable noise vectors as quantization residue which are supervised via a lightweight classifier. We show that our approach improves alignment between clean/noisy speech and text, producing speech tokens that display a high degree of noiseinvariance, and improves ASR performance. Keeping Whisper frozen, we show an 82% reduction in error rate compared to Whisper, and 35% improvement over baseline methods on the VBDemand test set. Further analyses show that the learned token space generalizes well to both seen and unseen acoustic conditions. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Human experts often struggle to select the best option from a large set of items with multiple competing objectives, a process bottlenecked by the difficulty of formalizing complex, implicit preferences. To address this, we introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a zero-shot preference oracle, guided only by an expert's high-level priorities in natural language. To operate within LLM constraints like context windows and inference costs, we propose two iterative algorithms: LISTEN-U, which uses the LLM to refine a parametric utility function, and LISTEN-T, a non-parametric method that performs tournament-style selections over small batches of solutions. Evaluated on diverse tasks including flight booking, shopping, and exam scheduling, our results show LISTEN-U excels when preferences are parametrically aligned (a property we measure with a novel concordance metric), while LISTEN-T offers more robust performance. This work explores a promising direction for steering complex multi-objective decisions directly with natural language, reducing the cognitive burden of traditional preference elicitation. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The advancement of Large Language Models (LLMs) has revolutionized natural language processing, yet their training on massive corpora poses significant risks, including the memorization of sensitive personal data, copyrighted material, and knowledge that could facilitate malicious activities. To mitigate these issues and align with legal and ethical standards such as the "right to be forgotten", machine unlearning has emerged as a critical technique to selectively erase specific knowledge from LLMs without compromising their overall performance. This survey provides a systematic review of over 180 papers on LLM unlearning published since 2021, focusing exclusively on large-scale generative models. Distinct from prior surveys, we introduce novel taxonomies for both unlearning methods and evaluations. We clearly categorize methods into training-time, post-training, and inference-time based on the training stage at which unlearning is applied. For evaluations, we not only systematically compile existing datasets and metrics but also critically analyze their advantages, disadvantages, and applicability, providing practical guidance to the research community. In addition, we discuss key challenges and promising future research directions. Our comprehensive overview aims to inform and guide the ongoing development of secure and reliable LLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:This research article examines the effectiveness of various pretraining strategies for developing machine translation models tailored to low-resource languages. Although this work considers several low-resource languages, including Afrikaans, Swahili, and Zulu, the translation model is specifically developed for Lingala, an under-resourced African language, building upon the pretraining approach introduced by Reid and Artetxe (2021), originally designed for high-resource languages. Through a series of comprehensive experiments, we explore different pretraining methodologies, including the integration of multiple languages and the use of both monolingual and parallel data during the pretraining phase. Our findings indicate that pretraining on multiple languages and leveraging both monolingual and parallel data significantly enhance translation quality. This study offers valuable insights into effective pretraining strategies for low-resource machine translation, helping to bridge the performance gap between high-resource and low-resource languages. The results contribute to the broader goal of developing more inclusive and accurate NLP models for marginalized communities and underrepresented populations. The code and datasets used in this study are publicly available to facilitate further research and ensure reproducibility, with the exception of certain data that may no longer be accessible due to changes in public availability. Submission history From: Idriss Nguepi Nguefack [view email][v1] Wed, 29 Oct 2025 02:30:18 UTC (118 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics. Submission history From: Yun-Shiuan Chuang [view email][v1] Wed, 29 Oct 2025 02:21:10 UTC (1,167 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 1

:This study examines the effectiveness of spatio-temporal modeling and the integration of spatial attention mechanisms in deep learning models for underwater object detection. Specifically, in the first phase, the performance of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is developed, through the addition of a Convolutional Block Attention Module (CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the research highlights how temporal modeling improves detection accuracy in dynamic marine environments, particularly under conditions of sudden movements, partial occlusions, and gradual motion. The testing results showed that YOLOv5 achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively, highlighting their superior accuracy and generalization in detecting complex objects. The findings demonstrate that T-YOLOv5 significantly enhances detection reliability compared to the standard model, while T-YOLOv5 with CBAM further improves performance in challenging scenarios, although there is a loss of accuracy when it comes to simpler scenarios. Current browse context: cs.CV References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Scientific progress is driven by the deliberate articulation of what remains unknown. This study investigates the ability of large language models (LLMs) to identify research knowledge gaps in the biomedical literature. We define two categories of knowledge gaps: explicit gaps, clear declarations of missing knowledge; and implicit gaps, context-inferred missing knowledge. While prior work has focused mainly on explicit gap detection, we extend this line of research by addressing the novel task of inferring implicit gaps. We conducted two experiments on almost 1500 documents across four datasets, including a manually annotated corpus of biomedical articles. We benchmarked both closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2) under paragraph-level and full-paper settings. To address the reasoning of implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive Bucketed Inference scheme that structures reasoning and buckets inferred conclusion candidates for validation. Our results highlight the robust capability of LLMs in identifying both explicit and implicit knowledge gaps. This is true for both open- and closed-weight models, with larger variants often performing better. This suggests a strong ability of LLMs for systematically identifying candidate knowledge gaps, which can support early-stage research formulation, policymakers, and funding decisions. We also report observed failure modes and outline directions for robust deployment, including domain adaptation, human-in-the-loop verification, and benchmarking across open- and closed-weight models. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community. Submission history From: Paula Costa Prof [view email][v1] Wed, 29 Oct 2025 00:45:36 UTC (133 KB) [v2] Thu, 30 Oct 2025 01:34:58 UTC (133 KB) Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials. Current browse context: cs.DB References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Mechanistic interpretability aims to reverse-engineer large language models (LLMs) into human-understandable computational circuits. However, the complexity of pretrained models often obscures the minimal mechanisms required for specific reasoning tasks. In this work, we train small, attention-only transformers from scratch on a symbolic version of the Indirect Object Identification (IOI) task -- a benchmark for studying coreference -- like reasoning in transformers. Surprisingly, a single-layer model with only two attention heads achieves perfect IOI accuracy, despite lacking MLPs and normalization layers. Through residual stream decomposition, spectral analysis, and embedding interventions, we find that the two heads specialize into additive and contrastive subcircuits that jointly implement IOI resolution. Furthermore, we show that a two-layer, one-head model achieves similar performance by composing information across layers through query-value interactions. These results demonstrate that task-specific training induces highly interpretable, minimal circuits, offering a controlled testbed for probing the computational foundations of transformer reasoning. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:A major problem in the study of large language models is to understand their inherent low-dimensional structure. We introduce an approach to study the low-dimensional structure of language models at a model-agnostic level: as sequential probabilistic models. We first empirically demonstrate that a wide range of modern language models exhibit low-rank structure: in particular, matrices built from the model's logits for varying sets of prompts and responses have low approximate rank. We then show that this low-rank structure can be leveraged for generation -- in particular, we can generate a response to a target prompt using a linear combination of the model's outputs on unrelated, or even nonsensical prompts. On the theoretical front, we observe that studying the approximate rank of language models in the sense discussed above yields a simple universal abstraction whose theoretical predictions parallel our experiments. We then analyze the representation power of the abstraction and give provable learning guarantees. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We show that across architecture (Transformer vs. Mamba vs. RWKV), training dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12 billion parameters), autoregressive language models exhibit highly consistent patterns of change in their behavior over the course of pretraining. Based on our analysis of over 1,400 language model checkpoints on over 110,000 tokens of English, we find that up to 98% of the variance in language model behavior at the word level can be explained by three simple heuristics: the unigram probability (frequency) of a given word, the $n$-gram probability of the word, and the semantic similarity between the word and its context. Furthermore, we see consistent behavioral phases in all language models, with their predicted probabilities for words overfitting to those words' $n$-gram probabilities for increasing $n$ over the course of training. Taken together, these results suggest that learning in neural language models may follow a similar trajectory irrespective of model details. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Despite their impressive performance, vision-language models (VLMs) still struggle on culturally situated inputs. To understand how VLMs process culturally grounded information, we study the presence of culture-sensitive neurons, i.e. neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts. We examine whether such neurons are important for culturally diverse visual question answering and where they are located. Using the CVQA benchmark, we identify neurons of culture selectivity and perform causal tests by deactivating the neurons flagged by different identification methods. Experiments on three VLMs across 25 cultural groups demonstrate the existence of neurons whose ablation disproportionately harms performance on questions about the corresponding cultures, while having minimal effects on others. Moreover, we propose a new margin-based selector - Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods in identifying culture-sensitive neurons. Finally, our layer-wise analyses reveals that such neurons tend to cluster in certain decoder layers. Overall, our findings shed new light on the internal organization of multimodal representations. Current browse context: cs.LG References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) IArxiv Recommender (What is IArxiv?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed SemCoT. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Language models generally produce grammatical text, but they are more likely to make errors in certain contexts. Drawing on paradigms from psycholinguistics, we carry out a fine-grained analysis of those errors in different syntactic contexts. We demonstrate that by disaggregating over the conditions of carefully constructed datasets and comparing model performance on each over the course of training, it is possible to better understand the intermediate stages of grammatical learning in language models. Specifically, we identify distinct phases of training where language model behavior aligns with specific heuristics such as word frequency and local context rather than generalized grammatical rules. We argue that taking this approach to analyzing language model behavior more generally can serve as a powerful tool for understanding the intermediate learning phases, overall training dynamics, and the specific generalizations learned by language models. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models have demonstrated strong performance on many established reasoning benchmarks. However, these benchmarks primarily evaluate structured skills like quantitative problem-solving, leaving a gap in assessing flexible, multifaceted reasoning abilities that are central to human intelligence. These abilities require integrating logical deduction with spatial awareness and constraint satisfaction, which current evaluations do not measure well. To address this, we introduce RiddleBench, a benchmark of 1,737 challenging puzzles in English designed to probe these core reasoning capabilities. Evaluation of state-of-the-art models on RiddleBench shows fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and 63.16%). Analysis further reveals deep failures, including hallucination cascades (accepting flawed reasoning from other models) and poor self-correction due to a strong self-confirmation bias. Their reasoning is also fragile, with performance degrading significantly when constraints are reordered or irrelevant information is introduced. RiddleBench functions as a diagnostic tool for these issues and as a resource for guiding the development of more robust and reliable language models. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) have demonstrated significant potential to accelerate scientific discovery as valuable tools for analyzing data, generating hypotheses, and supporting innovative approaches in various scientific fields. In this work, we investigate how LLMs can handle the transition from conceptual research ideas to well-structured research plans. Effective research planning not only supports scientists in advancing their research but also represents a crucial capability for the development of autonomous research agents. Despite its importance, the field lacks a systematic understanding of LLMs' research planning capability. To rigorously measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a benchmark built from 200 ICML 2025 Spotlight and Oral papers released after major LLM training cutoffs. Each benchmark instance includes a research idea and a grading rubric capturing the key components of valid plans. We further propose Idea2Plan JudgeEval, a complementary benchmark to assess the reliability of LLM-based judges against expert annotations. Experimental results show that GPT-5 and GPT-5-mini achieve the strongest performance on the benchmark, though substantial headroom remains for future improvement. Our study provides new insights into LLMs' capability for research planning and lay the groundwork for future progress. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We introduce MiRAGE, an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. As audiovisual media becomes a prevalent source of information online, it is essential for RAG systems to integrate information from these sources into generation. However, existing evaluations for RAG are text-centric, limiting their applicability to multimodal, reasoning intensive settings because they don't verify information against sources. MiRAGE is a claim-centric approach to multimodal RAG evaluation, consisting of InfoF1, evaluating factuality and information coverage, and CiteF1, measuring citation support and completeness. We show that MiRAGE, when applied by humans, strongly aligns with extrinsic quality judgments. We additionally introduce automatic variants of MiRAGE and three prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the limitations of text-centric work and laying the groundwork for automatic evaluation. We release open-source implementations and outline how to assess multimodal RAG. Submission history From: Alexander Martin [view email][v1] Tue, 28 Oct 2025 18:21:19 UTC (9,688 KB) Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Grammar refers to the system of rules that governs the structural organization and the semantic relations among linguistic units such as sentences, phrases, and words within a given language. In natural language processing, there remains a notable scarcity of grammar focused evaluation protocols, a gap that is even more pronounced for low-resource languages. Moreover, the extent to which large language models genuinely comprehend grammatical structure, especially the mapping between syntactic structures and meanings, remains under debate. To investigate this issue, we propose a Grammar Book Guided evaluation pipeline intended to provide a systematic and generalizable framework for grammar evaluation consisting of four key stages, and in this work we take Luxembourgish as a case study. The results show a weak positive correlation between translation performance and grammatical understanding, indicating that strong translations do not necessarily imply deep grammatical competence. Larger models perform well overall due to their semantic strength but remain weak in morphology and syntax, struggling particularly with Minimal Pair tasks, while strong reasoning ability offers a promising way to enhance their grammatical understanding. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:In this paper, we present our submissions to the unified WMT25 Translation Evaluation Shared Task. For the Quality Score Prediction subtask, we create a new generation of MetricX with improvements in the input format and the training protocol, while for the Error Span Detection subtask we develop a new model, GemSpanEval, trained to predict error spans along with their severities and categories. Both systems are based on the state-of-the-art multilingual open-weights model Gemma 3, fine-tuned on publicly available WMT data. We demonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture with a regression head on top, can be trained to effectively predict both MQM and ESA quality scores, and significantly outperforms its predecessor. Our decoder-only GemSpanEval model, on the other hand, we show to be competitive in error span detection with xCOMET, a strong encoder-only sequence-tagging baseline. With error span detection formulated as a generative task, we instruct the model to also output the context for each predicted error span, thus ensuring that error spans are identified unambiguously. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Virtual Reality (VR) games require players to translate high-level semantic actions into precise device manipulations using controllers and head-mounted displays (HMDs). While humans intuitively perform this translation based on common sense and embodied understanding, whether Large Language Models (LLMs) can effectively replicate this ability remains underexplored. This paper introduces a benchmark, ComboBench, evaluating LLMs' capability to translate semantic actions into VR device manipulation sequences across 262 scenarios from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o, Gemini-

:Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-3

:Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) have gained significant traction in medical decision support systems, particularly in the context of medical question answering and role-playing simulations. A common practice, Prompt-Based Role Playing (PBRP), instructs models to adopt different clinical roles (e.g., medical students, residents, attending physicians) to simulate varied professional behaviors. However, the impact of such role prompts on model reasoning capabilities remains unclear. This study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to evaluate whether role prompts induce distinct, role-specific cognitive processes in LLMs or merely modify linguistic style. We test this framework on three medical QA datasets, employing neuron ablation and representation analysis techniques to assess changes in reasoning pathways. Our results demonstrate that role prompts do not significantly enhance the medical reasoning abilities of LLMs. Instead, they primarily affect surface-level linguistic features, with no evidence of distinct reasoning pathways or cognitive differentiation across clinical roles. Despite superficial stylistic changes, the core decision-making mechanisms of LLMs remain uniform across roles, indicating that current PBRP methods fail to replicate the cognitive complexity found in real-world medical practice. This highlights the limitations of role-playing in medical AI and emphasizes the need for models that simulate genuine cognitive processes rather than linguistic this http URL have released the related code in the following repository:https: //github.com/IAAR-Shanghai/RolePlay_LLMDoctor References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 7

:Human evaluation of machine translation is in an arms race with translation model quality: as our models get better, our evaluation methods need to be improved to ensure that quality gains are not lost in evaluation noise. To this end, we experiment with a two-stage version of the current state-of-the-art translation evaluation paradigm (MQM), which we call MQM re-annotation. In this setup, an MQM annotator reviews and edits a set of pre-existing MQM annotations, that may have come from themselves, another human annotator, or an automatic MQM annotation system. We demonstrate that rater behavior in re-annotation aligns with our goals, and that re-annotation results in higher-quality annotations, mostly due to finding errors that were missed during the first pass. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:As retrieval-augmented generation (RAG) becomes increasingly widespread, the role of information retrieval (IR) is shifting from retrieving information for human users to retrieving contextual knowledge for artificial intelligence (AI) systems, where relevance becomes difficult to define or annotate beforehand. To address this challenge, we propose R3, a Retrieval framework optimized for RAG through trialand-feedback Reinforced contrastive learning. Unlike prior approaches that rely on annotated or synthetic data for supervised fine-tuning, R3 enables the retriever to dynamically explore and optimize relevance within the RAG environment. During training, the retrieved results interact with the environment to produce contrastive signals that automatically guide the retriever's self-improvement. Extensive experiments across diverse tasks demonstrate that R3 improves RAG performance by 5.2% over the original retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving comparable results to LLM-augmented retrieval and RAG systems built on post-trained or instruction-tuned LLMs. It is both efficient and practical, requiring only 4 GPUs and completing training within a single day. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:We ask where, and under what conditions, dyslexic reading costs arise in a large-scale naturalistic reading dataset. Using eye-tracking aligned to word-level features (word length, frequency, and predictability), we model how each feature influences dyslexic time costs. We find that all three features robustly change reading times in both typical and dyslexic readers, and that dyslexic readers show stronger sensitivities to each, especially predictability. Counterfactual manipulations of these features substantially narrow the dyslexic-control gap by about one third, with predictability showing the strongest effect, followed by length and frequency. These patterns align with dyslexia theories that posit heightened demands on linguistic working memory and phonological encoding, and they motivate further work on lexical complexity and parafoveal preview benefits to explain the remaining gap. In short, we quantify when extra dyslexic costs arise, how large they are, and offer actionable guidance for interventions and computational models for dyslexics. Submission history From: Hugo Rydel - Johnston W [view email][v1] Tue, 28 Oct 2025 17:15:31 UTC (787 KB) Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Reward models (RMs) have become essential for aligning large language models (LLMs), serving as scalable proxies for human evaluation in both training and inference. However, existing RMs struggle on knowledge-intensive and long-form tasks, where evaluating correctness requires grounding beyond the model's internal knowledge. This limitation hinders them from reliably discriminating subtle quality differences, especially when external evidence is necessary. To address this, we introduce OpenRM, a tool-augmented long-form reward model that systematically judges open-ended responses by invoking external tools to gather relevant evidence. We train OpenRM with Group Relative Policy Optimization (GRPO) on over 27K synthesized pairwise examples generated through a controllable data synthesis framework. The training objective jointly supervises intermediate tool usage and final outcome accuracy, incentivizing our reward model to learn effective evidence-based judgment strategies. Extensive experiments on three newly-collected datasets and two widely-used benchmarks demonstrate that OpenRM substantially outperforms existing reward modeling approaches. As a further step, we integrate OpenRM into both inference-time response selection and training-time data selection. This yields consistent gains in downstream LLM alignment tasks, highlighting the potential of tool-augmented reward models for scaling reliable long-form evaluation. Submission history From: Ziyou Hu [view email][v1] Tue, 28 Oct 2025 17:02:46 UTC (5,998 KB) [v2] Wed, 29 Oct 2025 16:06:18 UTC (5,993 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Maintaining mutual understanding is a key component in human-human conversation to avoid conversation breakdowns, in which repair, particularly Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the other to resolve), plays a vital role. However, Conversational Agents (CAs) still fail to recognize user repair initiation, leading to breakdowns or disengagement. This work proposes a multimodal model to automatically detect repair initiation in Dutch dialogues by integrating linguistic and prosodic features grounded in Conversation Analysis. The results show that prosodic cues complement linguistic features and significantly improve the results of pretrained text and audio embeddings, offering insights into how different features interact. Future directions include incorporating visual cues, exploring multilingual and cross-context corpora to assess the robustness and generalizability. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Scaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introduce relative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP) budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure relative alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:With the release of new large language models (LLMs) like Llama and Mistral, zero-shot cross-lingual transfer has become increasingly feasible due to their multilingual pretraining and strong generalization capabilities. However, adapting these decoder-only LLMs to new tasks across languages remains challenging. While parameter-efficient fine-tuning (PeFT) techniques like Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as soft prompt tuning, prefix tuning, and Llama Adapter are less explored, especially for zero-shot transfer in decoder-only models. We present a comprehensive study of three prefix-based methods for zero-shot cross-lingual transfer from English to 35+ high- and low-resource languages. Our analysis further explores transfer across linguistic families and scripts, as well as the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix methods outperform LoRA-baselines by up to 6% on the Belebele benchmark. Similar improvements were observed with Mistral v0.3 7B as well. Despite using only

:The quadratic cost of attention hinders the scalability of long-context LLMs, especially in resource-constrained settings. Existing static sparse methods such as sliding windows or global tokens utilizes the sparsity of attention to reduce the cost of attention, but poorly adapts to the content-dependent variations in attention due to their staticity. While previous work has proposed several dynamic approaches to improve flexibility, they still depend on predefined templates or heuristic mechanisms. Such strategies reduce generality and prune tokens that remain contextually important, limiting their accuracy across diverse tasks. To tackle these bottlenecks of existing methods for long-context modeling, we introduce Dynamic Hierarchical Sparse Attention (DHSA), a data-driven framework that dynamically predicts attention sparsity online without retraining. Our proposed DHSA adaptively segments sequences into variable-length chunks, then computes chunk representations by aggregating the token embeddings within each chunk. To avoid the bias introduced by varying chunk lengths, we apply length-normalized aggregation that scales the averaged embeddings by the square root of the chunk size. Finally, DHSA upsamples the chunk-level similarity scores to token level similarities to calculate importance scores that determine which token-level interactions should be preserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and LongBench show that DHSA matches dense attention in accuracy, while reducing prefill latency by 20-60% and peak memory usage by 35%. Compared to other representative baselines such as block sparse attention, DHSA achieves consistently higher accuracy (6-18% relative gains) with comparable or lower cost, offering an efficient and adaptable solution for long-context on-device LLMs. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Diffusion-based large language models (dLLMs) have exhibited substantial potential for parallel text generation, which may enable more efficient generation compared to autoregressive models. However, current dLLMs suffer from fixed generation lengths, which indicates the generation lengths of dLLMs have to be determined before decoding as a hyper-parameter, leading to issues in efficiency and flexibility. To solve these problems, in this work, we propose to train a diffusion LLM with native variable generation lengths, abbreviated as dLLM-Var. Concretely, we aim to train a model to accurately predict the [EOS] token in the generated text, which makes a dLLM be able to natively infer in a block diffusion manner, while still maintaining the ability of global bi-directional (full) attention and high parallelism. Experiments on standard benchmarks demonstrate that our method achieves a 30.1x speedup over traditional dLLM inference paradigms and a 2.4x speedup relative to autoregressive models such as Qwen and Llama. Our method achieves higher accuracy and faster inference, elevating dLLMs beyond mere academic novelty and supporting their practical use in real-world applications. Codes and models have been released. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 22.6 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases. Submission history From: Guoxin Chen [view email][v1] Tue, 28 Oct 2025 16:22:54 UTC (3,024 KB) [v2] Thu, 30 Oct 2025 11:15:27 UTC (3,024 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research. Current browse context: cs.CL Change to browse by: References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:One of the main challenges in mechanistic interpretability is circuit discovery, determining which parts of a model perform a given task. We build on the Mechanistic Interpretability Benchmark (MIB) and propose three key improvements to circuit discovery. First, we use bootstrapping to identify edges with consistent attribution scores. Second, we introduce a simple ratio-based selection strategy to prioritize strong positive-scoring edges, balancing performance and faithfulness. Third, we replace the standard greedy selection with an integer linear programming formulation. Our methods yield more faithful circuits and outperform prior approaches across multiple MIB tasks and models. Our code is available at: this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:The history of the Korean language is characterized by a discrepancy between its spoken and written forms and a pivotal shift from Chinese characters to the Hangul alphabet. However, this linguistic evolution has remained largely unexplored in NLP due to a lack of accessible historical corpora. To address this gap, we introduce the Open Korean Historical Corpus, a large-scale, openly licensed dataset spanning 1,300 years and 6 languages, as well as under-represented writing systems like Korean-style Sinitic (Idu) and Hanja-Hangul mixed script. This corpus contains 18 million documents and 5 billion tokens from 19 sources, ranging from the 7th century to 2025. We leverage this resource to quantitatively analyze major linguistic shifts: (1) Idu usage peaked in the 1860s before declining sharply; (2) the transition from Hanja to Hangul was a rapid transformation starting around 1890; and (3) North Korea's lexical divergence causes modern tokenizers to produce up to 51 times higher out-of-vocabulary rates. This work provides a foundational resource for quantitative diachronic analysis by capturing the history of the Korean language. Moreover, it can serve as a pre-training corpus for large language models, potentially improving their understanding of Sino-Korean vocabulary in modern Hangul as well as archaic writing systems. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Textual humor is enormously diverse and computational studies need to account for this range, including intentionally bad humor. In this paper, we curate and analyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to better understand "bad" humor in English. Standard humor detection models perform poorly on our corpus, and an analysis of literary devices finds that these sentences combine features common in existing humor datasets (e.g., puns, irony) with metaphor, metafiction and simile. LLMs prompted to synthesize contest-style sentences imitate the form but exaggerate the effect by over-using certain literary devices, and including far more novel adjective-noun bigrams than human writers. Data, code and analysis are available at this https URL Submission history From: Venkata S Govindarajan [view email][v1] Tue, 28 Oct 2025 15:42:03 UTC (1,957 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Many words are ambiguous in terms of their part of speech (POS). However, when a word appears in a text, this ambiguity is generally much reduced. Disambiguating POS involves using context to reduce the number of POS associated with words, and is one of the main challenges of lexical tagging. The problem of labeling words by POS frequently arises in natural language processing, for example for spelling correction, grammar or style checking, expression recognition, text-to-speech conversion, text corpus analysis, etc. Lexical tagging systems are thus useful as an initial component of many natural language processing systems. A number of recent lexical tagging systems produce multiple solutions when the text is lexically ambiguous or the uniquely correct solution cannot be found. These contributions aim to guarantee a zero silence rate: the correct tag(s) for a word must never be discarded. This objective is unrealistic for systems that tag each word uniquely. This article concerns a lexical disambiguation method adapted to the objective of a zero silence rate and implemented in Silberztein's INTEX system (1993). We present here a formal description of this method. We show that to verify a local disambiguation grammar in this framework, it is not sufficient to consider the transducer paths separately: one needs to verify their interactions. Similarly, if a combination of multiple transducers is used, the result cannot be predicted by considering them in isolation. Furthermore, when examining the initial labeling of a text as produced by INTEX, ideas for disambiguation rules come spontaneously, but grammatical intuitions may turn out to be inaccurate, often due to an unforeseen construction or ambiguity. If a zero silence rate is targeted, local grammars must be carefully tested. This is where a detailed specification of what a grammar will do once applied to texts would be necessary. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:As Large language models (LLMs) become increasingly integrated into our lives, their inherent social biases remain a pressing concern. Detecting and evaluating these biases can be challenging because they are often implicit rather than explicit in nature, so developing evaluation methods that assess the implicit knowledge representations of LLMs is essential. We present a novel word association network methodology for evaluating implicit biases in LLMs based on simulating semantic priming within LLM-generated word association networks. Our prompt-based approach taps into the implicit relational structures encoded in LLMs, providing both quantitative and qualitative assessments of bias. Unlike most prompt-based evaluation methods, our method enables direct comparisons between various LLMs and humans, providing a valuable point of reference and offering new insights into the alignment of LLMs with human cognition. To demonstrate the utility of our methodology, we apply it to both humans and several widely used LLMs to investigate social biases related to gender, religion, ethnicity, sexual orientation, and political party. Our results reveal both convergences and divergences between LLM and human biases, providing new perspectives on the potential risks of using LLMs. Our methodology contributes to a systematic, scalable, and generalizable framework for evaluating and comparing biases across multiple LLMs and humans, advancing the goal of transparent and socially responsible language technologies. Submission history From: Katherine Elizabeth Abramski [view email][v1] Tue, 28 Oct 2025 15:03:18 UTC (2,962 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Scientific talks are a growing medium for disseminating research, and automatically identifying relevant literature that grounds or enriches a talk would be highly valuable for researchers and students alike. We introduce Reference Prediction from Talks (RPT), a new task that maps long, and unstructured scientific presentations to relevant papers. To support research on RPT, we present Talk2Ref, the first large-scale dataset of its kind, containing 6,279 talks and 43,429 cited papers (26 per talk on average), where relevance is approximated by the papers cited in the talk's corresponding source publication. We establish strong baselines by evaluating state-of-the-art text embedding models in zero-shot retrieval scenarios, and propose a dual-encoder architecture trained on Talk2Ref. We further explore strategies for handling long transcripts, as well as training for domain adaptation. Our results show that fine-tuning on Talk2Ref significantly improves citation prediction performance, demonstrating both the challenges of the task and the effectiveness of our dataset for learning semantic representations from spoken scientific content. The dataset and trained models are released under an open license to foster future research on integrating spoken scientific communication into citation recommendation systems. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Personalized text generation requires models not only to produce coherent text but also to align with a target user's style, tone, and topical focus. Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich profiles with user and neighbor histories, but they stop at generation and often yield outputs that drift in tone, topic, or style. We present PerFine, a unified, training-free critique-refine framework that enhances personalization through iterative, profile-grounded feedback. In each iteration, an LLM generator produces a draft conditioned on the retrieved profile, and a critic LLM - also conditioned on the same profile - provides structured feedback on tone, vocabulary, sentence structure, and topicality. The generator then revises, while a novel knockout strategy retains the stronger draft across iterations. We further study additional inference-time strategies such as Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp, Goodreads, and Amazon datasets, PerFine consistently improves personalization over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5 refinement iterations, and scalability with increasing critic size. These results highlight that post-hoc, profile-aware feedback offers a powerful paradigm for personalized LLM generation that is both training-free and model-agnostic. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:While new benchmarks for large language models (LLMs) are being developed continuously to catch up with the growing capabilities of new models and AI in general, using and evaluating LLMs in non-English languages remains a little-charted landscape. We give a concise overview of recent developments in LLM benchmarking, and then propose a new taxonomy for the categorization of benchmarks that is tailored to multilingual or non-English use scenarios. We further propose a set of best practices and quality standards that could lead to a more coordinated development of benchmarks for European languages. Among other recommendations, we advocate for a higher language and culture sensitivity of evaluation methods. Submission history From: Taja Kuzman Pungerek [view email][v1] Tue, 28 Oct 2025 14:13:44 UTC (435 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Since real-world legal experiments are often costly or infeasible, simulating legal societies with Artificial Intelligence (AI) systems provides an effective alternative for verifying and developing legal theory, as well as supporting legal administration. Large Language Models (LLMs), with their world knowledge and role-playing capabilities, are strong candidates to serve as the foundation for legal society simulation. However, the application of LLMs to simulate legal systems remains underexplored. In this work, we introduce Law in Silico, an LLM-based agent framework for simulating legal scenarios with individual decision-making and institutional mechanisms of legislation, adjudication, and enforcement. Our experiments, which compare simulated crime rates with real-world data, demonstrate that LLM-based agents can largely reproduce macro-level crime trends and provide insights that align with real-world observations. At the same time, micro-level simulations reveal that a well-functioning, transparent, and adaptive legal system offers better protection of the rights of vulnerable individuals. Current browse context: cs.AI References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models are increasingly used for Islamic guidance, but risk misquoting texts, misapplying jurisprudence, or producing culturally inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar on prompts from authentic Islamic blogs. Our dual-agent framework uses a quantitative agent for citation verification and six-dimensional scoring (e.g., Structure, Islamic Consistency, Citations) and a qualitative agent for five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality). GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI followed (3.68, 3.32), and Fanar lagged (2.76,

:The effectiveness of instruction-tuned Large Language Models (LLMs) is often limited in low-resource linguistic settings due to a lack of high-quality training data. We introduce LuxIT, a novel, monolingual instruction tuning dataset for Luxembourgish developed to mitigate this challenge. We synthesize the dataset from a corpus of native Luxembourgish texts, utilizing DeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following generation, we apply a quality assurance process, employing an LLM-as-a-judge approach. To investigate the practical utility of the dataset, we fine-tune several smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base models on Luxembourgish language proficiency examinations, however, yields mixed results, with performance varying significantly across different models. LuxIT represents a critical contribution to Luxembourgish natural language processing and offers a replicable monolingual methodology, though our findings highlight the need for further research to optimize its application. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Evaluating the reasoning ability of language models (LMs) is complicated by their extensive parametric world knowledge, where benchmark performance often reflects factual recall rather than genuine reasoning. Existing datasets and approaches (e.g., temporal filtering, paraphrasing, adversarial substitution) cannot cleanly separate the two. We present SynthWorlds, a framework that disentangles task reasoning complexity from factual knowledge. In SynthWorlds, we construct parallel corpora representing two worlds with identical interconnected structure: a real-mapped world, where models may exploit parametric knowledge, and a synthetic-mapped world, where such knowledge is meaningless. On top of these corpora, we design two mirrored tasks as case studies: multi-hop question answering and page navigation, which maintain equal reasoning difficulty across worlds. Experiments in parametric-only (e.g., closed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings reveal a persistent knowledge advantage gap, defined as the performance boost models gain from memorized parametric world knowledge. Knowledge acquisition and integration mechanisms reduce but do not eliminate this gap, highlighting opportunities for system improvements. Fully automatic and scalable, SynthWorlds provides a controlled environment for evaluating LMs in ways that were previously challenging, enabling precise and testable comparisons of reasoning and memorization. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent efforts leverage knowledge distillation techniques to develop lightweight and practical sentiment analysis models. These methods are grounded in human-written instructions and large-scale user texts. Despite the promising results, two key challenges remain: (1) manually written instructions are limited in diversity and quantity, making them insufficient to ensure comprehensive coverage of distilled knowledge; (2) large-scale user texts incur high computational cost, hindering the practicality of these methods. To this end, we introduce COMPEFFDIST, a comprehensive and efficient distillation framework for sentiment analysis. Our framework consists of two key modules: attribute-based automatic instruction construction and difficulty-based data filtering, which correspondingly tackle the aforementioned challenges. Applying our method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we enable 3B student models to match the performance of 20x larger teacher models on most tasks. In addition, our approach greatly outperforms baseline methods in data efficiency, attaining the same performance level with only 10% of the data. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Current browse context: cs.AI References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large language models (LLMs) are increasingly deployed with task-specific adapters catering to multiple downstream applications. In such a scenario, the additional compute associated with these apparently insignificant number of adapter parameters (typically less than 1% of the base model) turns out to be disproportionately significant during inference time (upto 2.5x times that of the base model). In this paper, we propose a new zero-latency fused low-rank adapter (zFLoRA) that introduces zero or negligible latency overhead on top of the base model. Experimental results on LLMs of size 1B, 3B and 7B show that zFLoRA compares favorably against the popular supervised fine-tuning benchmarks including low-rank adapters (LoRA) as well as full fine-tuning (FFT). Experiments are conducted on 18 different tasks across three different categories namely commonsense reasoning, math reasoning and summary-dialogue. Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA H100) platforms show that the proposed zFLoRA adapters introduce zero to negligible latency overhead. Current browse context: cs.CL References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Sentence embeddings can be decoded to give approximations of the original texts used to create them. We explore this effect in the context of text simplification, demonstrating that reconstructed text embeddings preserve complexity levels. We experiment with a small feed forward neural network to effectively learn a transformation between sentence embeddings representing high-complexity and low-complexity texts. We provide comparison to a Seq2Seq and LLM-based approach, showing encouraging results in our much smaller learning setting. Finally, we demonstrate the applicability of our transformation to an unseen simplification dataset (MedEASI), as well as datasets from languages outside the training data (ES,DE). We conclude that learning transformations in sentence embedding space is a promising direction for future research and has potential to unlock the ability to develop small, but powerful models for text simplification and other natural language generation tasks. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Recent advances in code agents have enabled automated software development at the project level, supported by large language models (LLMs) and widely adopted tools. However, existing benchmarks for code agent evaluation face two major limitations: high annotation cost and expertise requirements, and rigid evaluation metrics that rely primarily on unit tests. To address these challenges, we propose an agent-driven benchmark construction pipeline that leverages human supervision to efficiently generate diverse and challenging project-level tasks. Based on this approach, we introduce PRDBench, a novel benchmark comprising 50 real-world Python projects across 20 domains, each with structured Product Requirement Document (PRD) requirements, comprehensive evaluation criteria, and reference implementations. PRDBench features rich data sources, high task complexity, and flexible metrics. We further employ an Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of various test types beyond unit tests. Extensive experiments on PRDBench demonstrate its effectiveness in assessing the capabilities of both code agents and evaluation agents, providing a scalable and robust framework for annotation and evaluation. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Generating long, informative, and factual outputs remains a major challenge for Large Language Models (LLMs). Existing benchmarks for long-form generation typically assess real-world queries with hard-to-verify metrics or use synthetic setups that ease evaluation but overlook real-world intricacies. In this paper, we introduce \textbf{LongWeave}, which balances real-world and verifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval constructs tasks by first defining verifiable targets within real-world scenarios, then systematically generating corresponding queries, textual materials, and constraints based on these targets. This ensures that tasks are both realistic and objectively assessable, enabling rigorous assessment of model capabilities in meeting complex real-world constraints. LongWeave supports customizable input/output lengths (up to 64K/8K tokens) across seven distinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models encounter significant challenges in long-form generation as real-world complexity and output length increase. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Large Language Models (LLMs) are increasingly used to answer everyday questions, yet their performance on culturally grounded and dialectal content remains uneven across languages. We propose a comprehensive method that (i) translates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into English and several Arabic dialects, (ii) converts them into open-ended questions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs under both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT) rationales to fine-tune models for step-by-step reasoning. Using this method, we extend an existing dataset in which QAs are parallelly aligned across multiple language varieties, making it, to our knowledge, the first of its kind. We conduct extensive experiments with both open and closed models. Our findings show that (i) models underperform on Arabic dialects, revealing persistent gaps in culturally grounded and dialect-specific knowledge; (ii) Arabic-centric models perform well on MCQs but struggle with OEQs; and (iii) CoT improves judged correctness while yielding mixed n-gram-based metrics. The developed dataset will be publicly released to support further research on culturally and linguistically inclusive evaluation. Submission history From: Hunzalah Hassan Bhatti [view email][v1] Tue, 28 Oct 2025 11:52:51 UTC (815 KB) References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:Stance detection has emerged as an area of research in the field of artificial intelligence. However, most research is currently centered on the target-dependent stance detection task, which is based on a person's stance in favor of or against a specific target. Furthermore, most benchmark datasets are based on English, making it difficult to develop models in low-resource languages such as Korean, especially for an emerging field such as stance detection. This study proposes the LArge-Scale Target-Independent STance (LASTIST) dataset to fill this research gap. Collected from the press releases of both parties on Korean political parties, the LASTIST dataset uses 563,299 labeled Korean sentences. We provide a detailed description of how we collected and constructed the dataset and trained state-of-the-art deep learning and stance detection models. Our LASTIST dataset is designed for various tasks in stance detection, including target-independent stance detection and diachronic evolution stance detection. We deploy our dataset on this https URL. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

:In recent years, many generalization benchmarks have shown language models' lack of robustness in natural language inference (NLI). However, manually creating new benchmarks is costly, while automatically generating high-quality ones, even by modifying existing benchmarks, is extremely difficult. In this paper, we propose a methodology for automatically generating high-quality variants of original NLI problems by replacing open-class words, while crucially preserving their underlying reasoning. We dub our generalization test as MERGE (Minimal Expression-Replacements GEneralization), which evaluates the correctness of models' predictions across reasoning-preserving variants of the original problem. Our results show that NLI models' perform 4-20% worse on variants, suggesting low generalizability even on such minimally altered problems. We also analyse how word class of the replacements, word probability, and plausibility influence NLI models' performance. References & Citations export BibTeX citation Loading... Bibliographic and Citation Tools Bibliographic Explorer (What is the Explorer?) Connected Papers (What is Connected Papers?) Litmaps (What is Litmaps?) scite Smart Citations (What are Smart Citations?) Code, Data and Media Associated with this Article alphaXiv (What is alphaXiv?) CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub (What is DagsHub?) Gotit.pub (What is GotitPub?) Hugging Face (What is Huggingface?) Papers with Code (What is Papers with Code?) ScienceCast (What is ScienceCast?) Demos Recommenders and Search Tools Influence Flower (What are Influence Flowers?) CORE Recommender (What is CORE?) arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

==== Page 1 ==== arX1v:2510.26802v1 [cs.CV] 30 Oct 2025 Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-COF Benchmark Ziyu Guo*'!, Xinyan Chen*?, Renrui Zhang*!?, Ruichuan An**, Yu Qi**, Dongzhi Jiang Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng! CUHK !IMIXR & ?7MMLab- ?Peking University 4Northeastern University *Equal Contribution Project Lead *Corresponding Author Project Page: https: //video-cof.github.io Abstract Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond real- istic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation [70]. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3 [21]. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-COF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. 1 Introduction Video models [21, 63, 55, 81, 11], including text-to-video and video-to-text generation models, have made rapid progress in recent years. Thanks to advances in diffusion [75, 7, 84] and autoregressive [36, 706, 16] architectures, current video models can produce high-fidelity videos maintaining consistent object relations and realistic motion dynamics across frames. This suggests that the models may have internalized substantial visual and structural knowledge about the world. Recent research from Google [70] further hints that, such models are evolving beyond pure content generation: Veo-3 [21] has been shown to perform dozens of distinct vision tasks across perception, modeling, manipulation, and reasoning, without any task-specific training. These emergent capabilities have led researchers to posit that video models could serve as unified, generalist vision models, much like large language models (LLMs) [1, 13, 3, 30] have become foundation models for natural language. Crucially, the sequential nature of video generation provides a new perspective on how such models might reason. Each generated frame builds upon the last, creating a temporal chain of information propagation. This has been dubbed Chain-of-Frame (CoF) reasoning [70], an analogy to the chain- of-thought (CoT) process in LLMs [69, 35, 82, 23, 79] and their multi-modal variants (MLLMs) [12, 4, 40, 31, 10]. In essence, as a video model generates a sequence of frames, it can iteratively refine and update the scene, thereby working through a problem step-by-step in time and space. This CoF concept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may emerge from video generative models. ==== Page 2 ==== Pp 3D Geometry Reasoning Visual Detail SRDS. Reasoning O10, Physics-based Reasoning N "9: bY dy Real-world Spatial one Reasoning al Visual Trace Reasoning iM 2D Geometry Reasoning Zero-shot Reasoning? > veo 9% Sora }1 Seedance O Kling Ql {ite Ba Video Models Table and Chart o00c Object Counting Reasoning Cy GUI Reasoning | Rotation Reasoning Reasoning Medical Reasonin Embodied 8 Reasoning Figure 1: Overview of Our Study on the Reasoning Potential of Video Models. We investigate whether state-of-the-art video models exhibit emergent reasoning potentials beyond content synthesis. The analysis spans 12 reasoning dimensions under a unified perspective, exploring whether large-scale video models can serve as zero-shot visual reasoners via CoF reasoning. However, it remains unclear to what extent current video models truly exhibit reasoning about the content they create. Strong generative performance does not automatically imply robust reasoning potential. Emerging evidence [22, 47, 5, 78] shows that a model may produce coherent videos by learning surface-level patterns in the training data, rather than by internalizing general principles. For instance, a video model can maintain object continuity yet fail to grasp physical plausibility across a long sequence, or it may mimic observed visual sequences without understanding the underlying cause-and-effect relationships. This motivates our central question: Are video models, purely through large-scale visual learning, obtain the zero-shot reasoning potential? To this end, we present the first empirical study to systematically probe the CoF reasoning capabili- ties of modern video models, spanning 12 dimensions such as spatial, geometric, physical, temporal, and embodied logic, as detailed in |. We carry out our analysis on Veo-3, which has been system- atically examined as a zero-shot learner in prior work [70]. Our preliminary observations suggest that current leading video models exhibit comparable reasoning patterns, making Veo-3 a represen- tative choice. Our analysis builds on reasoning scenarios distilled from diverse reasoning-oriented benchmarks [25, 67, 45, 71, 29, 34], as well as those we design ourselves, providing a compact yet expressive foundation. The prompts for video models are meticulously crafted by transforming the underlying, textual reasoning process of problem-solving into a clear, video-presentation format. Each case receives a qualitative assessment across three performance levels, i.e., good, moderate, and bad, complemented by a quantitative success rate to measure robustness. To standardize evaluation, we curate these tasks into the MME-COF benchmark, as illustrated in Figure 2 and Section 3.2. Leveraging this benchmark, we measure several state-of-the-art video models, i.e., Veo-3 [21], Sora-2 [56], Kling [38], and Seedance [19], to obtain directly comparable scores and qualitative behaviors across categories. Our investigation reveals that the models exhibit promising reasoning patterns in short-horizon spatial coherence, fine-grained grounding, and con- ==== Page 3 ==== rat ball nimat Fe r 32 2FCRal inc ae hancbeg show n e Visual Detail Medical Reasoning Visual Trace Reasoning Reasoning SUV). Te Epes block coun aD Embodied Real-world Spatial L nchange. ed di MOO 4 C Ct boxes Reasoning Reasoning i 2 image m a ec Si det Le thrgughout econnecting A purple ight dot ,movement _ ni 4 - Object Counting be 3D Geometry au eft pC OL leone EV L ift Bricks W. Reasoning Reasoning $ inl visible continue oefocus * "colors jong OM. b Ht NS 2 g clearly 3 GUI 2D Geometry 2 E inte  Reasoning Reasoning ce scale teO! eeimins te a arouse oh rete Gas) Table & Chart v Physics-based Vv line graduallySoiue BF eet hold form Reasoning Rotation Reasoning ( je botton yy pixels = changes | Reasoning ae : 3 path 2  Fuld  Hobject sta Pt videolltis he Se iy utd  Kling-v1 +- Veo-3-fast  Sora-2-pro geometry Gre y Cd: Ree end 5 rule i _ Seedance-1.0-pro~*- Veo-3-preview ~~~ Sora-2 Mbunding numbered fcuresimultanedusly Yoapward & (a) Evaluation Radar Map. (b) Word Cloud. Figure 2: Illustration of the MME-COF Benchmark. It showcases that different models specialize in distinct reasoning aspects, but most models exhibit limited reasoning capability across all tasks. sistent local dynamics; however, they struggle with complex reasoning conditions, particularly in long-horizon causal consistency, geometric constraint adherence, and abstract logic. Overall, current video models are not yet ready as standalone zero-shot reasoners. Still, they show encouraging signs of emergent reasoning, suggesting strong potential as complementary reasoning agents alongside specialized models. Our main contributions are summarized as follows: * A Comprehensive Empirical Study. We provide the first investigation of video models (Veo-3) to analyze their visual reasoning potential, detailing representative successes, char- acteristic errors, and the conditions under which CoF reasoning emerges, holds, or breaks.  The MME-COF Benchmark. We curate MME-COF, a compact benchmark providing a standardized taxonomy and an evaluation protocol aligned with CoF reasoning, enabling consistent and category-wise assessment beyond surface-level visual fidelity.  Insights and Directions. We summarize common success patterns (e.g., short-horizon coherence and stable spatial layout) and failure patterns (e.g., long-horizon degradation, violations of basic geometry/physics, and temporal logic), making clear when the behavior reflects genuine reasoning versus pattern replay. 2 Deep-Dive Analysis on Veo-3 2.1 Overview To ensure a rigorous empirical study, we detail our core methodology in this section, including the taxonomy of reasoning tasks, test case curation process, the standardized style for prompt design, and the analysis setup. Task Taxonomy. To capture different dimensions of reasoning, our study starts from dozens of reasoning-oriented tasks, which can be organized into the following 12 categories: ==== Page 4 ==== 1) Visual Detail Reasoning 7) Rotation Reasoning 2) Visual Trace Reasoning &) Table and Chart Reasoning 3) Real-world Spatial Reasoning 9) Object Counting Reasoning 4) 3D Geometry Reasoning 10) GUI Reasoning 5) 2D Geometry Reasoning 11) Embodied Reasoning 6) Physics-based Reasoning 12) Medical Reasoning Each category comprises several representative cases selected to test specific aspects of reasoning. Test Case Curation. We recruit five PhD-level experts with deep expertise in text-image reasoning, who are tasked with selecting representative cases from benchmarks [25, 67, 45, 52, 74] corresponding to each task category. For each reasoning case, the experts manually constructed text prompts that explicitly or unambiguously define the target reasoning objective, aiming to evaluate the potential of video models for multi-modal reasoning. Prompt Design Style. To ensure consistency and fairness, all prompts follow a unified style emphasizing explicit visual constraints, controlled motion, and minimal linguistic ambiguity. Prompts are encouraged to be written in imperative form and designed to reduce variance from language interpretation, focusing the models behavior on the intended visual reasoning objective. The overall design principles are as follows: 1) Static camera and fixed viewpoint, unless motion is explicitly required by the task. 2) Stable spatial composition, consistent framing, and unchanging scene layout across frames. 3) Clear specification of allowed and disallowed changes (e.g., no zoom, no pan, no dolly) to constrain camera dynamics. 4) Explicit temporal phrasing to control the pace of motion, using cues such as instantly, smoothly, or step-by-step. 5) Avoidance of direct textual hints toward the answer; instructions are purely visual and task-oriented. 6) Inclusion of realistic phrasing and scene context to align with the models natural video priors while minimizing artifacts. The standardized prompt style ensures that differences in output primarily reflect the models internal reasoning potential rather than prompt variability. Analysis Setup. For every reasoning case, we construct a text prompt that explicitly or implicitly specifies the target reasoning objective. Each prompt produces six video samples at a resolution of 1280x720, 24 FPS, and a duration of 8 seconds. All experiments are conducted in a unified zero-shot setup without fine-tuning, additional supervision, or auxiliary tools. We evaluate model outputs through qualitative judgments along three levels of performance, i.e., Good , Moderate , and Bad, based on the clarity, correctness, and temporal stability of the visual reasoning process. Detailed definitions and examples of these evaluation criteria are provided in the corresponding task subsections. Note that, since we observe that most video models struggle to follow the requirement of static shot reliably, we apply more permissive qualitative criteria for static-shot evaluations. We further define a success rate to measure robustness across generations for each case, computed as the proportion of successful samples among the six generated. For cases categorized as Bad, the success rate is always 0. Non-zero success rates only appear in cases evaluated as Good or Moderate , indicating that Veo-3 exhibits some potential to perform as a visual reasoner. A higher success rate reflects a more stable reasoning capability of the model. ==== Page 5 ==== I. Question: Text-to-Video Prompt: Q: What is the color of the Apple logo? Zoom in on the black bag with the Apple logo to focus on the A: The color of the Apple logo is polychromatic. logo's color. Static shot. Input Image: Reasoning Video: ~ Moderate G) Success Rate: 17% 1* frame a a \ II. Question: Text-to-Video Prompt: Q: What is the color of the handbag? Gradually zoom in on the group of people walking along the path, centering on the person carrying the handbag. Keep the surrounding park and benches A: The color of the handbag is white. softly blurred to emphasize the handbags color. Static shot. Input Image: Reasoning Video: JY Good Success Rate: 33% 1* frame Bia. TIT. Question: Text-to-Video Prompt: Q: Is the motorcycle on the left or right side of Smoothly zoom in on the dog near the lower right corner of the the dog? scene, then highlight the motorcycle parked near it. Keep the . . surrounding jeeps and people slightly blurred to emphasize A: The motorcycle is on the left side of the dog. spatial relation. Static shot. Input Image: Reasoning Video: Y Good Success Rate: 83% om 1* frame . a \ al IV. Question: Text-to-Video Prompt: Q: Is the baby carriage on the left or right side Gradually zoom in on the area near the cone along the pathway, of the cone? centering both the cone and the baby carriage in the frame. Keep A: The baby carriage is on the right side of the the surrounding trees and grass softly blurred to emphasize these cone. two objects. Static shot. Input Image: Reasoning Video: X Bad 1* frame ia a Figure 3: Showcase of Visual Detail Reasoning by Veo-3. It illustrates Veo-3s ability to localize targets and maintain fine-grained visual attributes across frames, together with common failure modes when targets are small, occluded, or embedded in clutter. ==== Page 6 ==== 2.2 Visual Detail Reasoning Task Description and Evaluated Aspects. In the visual detail reasoning category, the objective is to assess a models ability to discern and maintain fine-grained visual attributes and spatial relations within generated video sequences. It covers attribute recognition, e.g., identifying color, texture or material of an object, and spatial relation identification, e.g., recognizing that one object is on the left of or behind another object. The model is evaluated on the capacity both to attend to the correct target region and to maintain visual consistency, across frames, of the attribute or relation in question. Definition of Good / Moderate / Bad. We define the three-level evaluation criteria as follows: V Good: The reasoning video accurately centers on the correct target region, clearly resolves the relevant attribute, such as color, texture or position, and maintains sharp, stable and natural rendering throughout the sequence. There are no visible frame drops, artifacts or unintended motion. ~ Moderate: The region of interest is approximately correct, and the attribute remains inferable, but the sequence suffers from minor blur, incomplete framing, slight instability mild unnatural motion, or sometimes deviates from the textual instruction and produces a plausible but unaligned or self-directed visual interpretation, limiting confident interpretation. X Bad: The target region is incorrect or ambiguous, the attribute cannot be reliably inferred, or the video exhibits severe artifacts: abrupt frame jumps, major jitter, unintended zoom or crop, extraneous objects interfering, or conspicuous quality degradation that obstructs the reasoning task altogether. Data Source. We sample data from the V* Bench [71], which provides a comprehensive set of evaluation dimensions including spatial relationship and color/attribute consistency tasks. Example and Analysis. We illustrate typical behaviors of Veo-3 in visual detail reasoning through four representative cases in Figure 3. In case I, the model performs well in localizing the target: although it does not strictly execute the zoom in instruction, it instead achieves an equivalent visual outcome through a semantically consistent motion with a persons hand. This slight deviation suggests that the model may exhibit certain generation preferences in how it interprets and realizes spatial instructions, possibly reflecting stylistic tendencies learned from training data. In cases II and III, the model achieves better success rates when the targets are visually salient and contextually distinct. For the handbag and dog-motorcycle scenes, Veo-3 attends to the correct regions and maintains smooth temporal coherence. However, when the object (e.g., the motorcycle) is small or surrounded by distracting elements, the model occasionally fails to locate it accurately, indicating limited fine-grained spatial discrimination in cluttered scenes. In case IV, when the target object is tiny and visually indistinct, Veo-3 cannot identify it even with explicit positional hints, highlighting that the models perceptual grounding and reasoning weaken sharply when object size and salience are too low for reliable attention. Takeaway 1 Veo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded targets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic generation biases that lead to plausible yet instruction-divergent outcomes. 2.3 Visual Trace Reasoning Task Description and Evaluated Aspects. The visual trace reasoning category evaluates a models ability to represent and maintain causal continuity across sequential actions. Typical tasks include maze navigation, path following, and multi-step object manipulation, where the video must visually encode a coherent sequence of intermediate decisions that lead to the correct goal. Performance is assessed based on two major aspects: (i) temporal coherence, which is the smoothness and logical ==== Page 7 ==== I. Question: Text-to-Video Prompt: Q: Starting from the red dot, follow the given Starting at the red dot in the middle-right cell, animate step-by- movement instructions and determine the final step moves: go down 1 cell, left 1, left 1, up 1, and up 1, position. Down 1, left 1, left 1, up 1, up 1. drawing arrows for each step and finishing with a glow around ALA the final cell. Static shot. Input Image: Reasoning Video: X Bad ae 1* frame as A B c | De De i |  II. Questiont: Text-to-Video Prompt: Animate the elf moving step by step toward the gift while carefully avoiding the icy frozen lake. Highlight the successful path and end with the elf standing beside the gift. Static shot. Q: The character must avoid falling into the frozen lake and reach the gift pack safely. Input Image: Reasoning Video: v Good Success Rate: 17% ae Se @epee e - ee *e @12@ @| @ = = = =) Q Q eS g S@e se 2@ 82? a@ 28 a a Q a @ a = = Q Q Q II. Questiont: Text-to-Video Prompt: Animate the red triangle moving step by step toward the white printer, picking it up once it reaches it. Then have the triangle carry the printer upward and place it on the brown area representing the table. End with a subtle highlight around the printer to show it is toggled on. Static shot. Q: Move the character (red triangle) to pick up the white printer and place it anywhere on the desk. Input Image: Reasoning Video: X Bad  an ~ psael uaa oul) ansl EustdOudd na TV. Question: Text-to-Video Prompt: Q: The given picture is a maze, and the black lines Animate a bright path tracing from the blue point at the represent walls that cannot be walked. Now you want to top through the mazes open corridors toward the red walk from the blue point to the red point. Is there a point at the bottom, highlighting each green numbered feasible path? If so, which of the green marks numbered mark it passes. Keep the maze and all walls fixed while 1-5 In the picture must be passed in the path? the glowing path moves smoothly through the correct A: Yes, 3. route. Static shot. Input Image: Reasoning Video: X Bad  1st frame =e" bl aT fez lal Ey os Eee fas a =! = aie a! sleet Wek steal feed rl Figure 4: Showcase of Visual Trace Reasoning by Veo-3 (Part I). It shows short-horizon path- following successes, object-grounding failures, and a certain bias that causes step omissions/mistakes in multi-step traces. ' The ground-truth answers of cases II and III are intuitive and non-unique, which are omitted to highlight the key reasoning behaviors. ==== Page 8 ==== V. Question: Q: In the diagram, the red arrow is the initial arrow, and the green arrow is the final arrow. The arrow can move in four directions (forward, backward, left, right), where forward' always refers to the current direction the arrow is pointing. After each movement, the arrow's direction is updated to the direction of movement. Which of the following paths can make the arrow move from the starting position to the ending position? Text-to-Video Prompt: Create a 2D animation based on the provided diagram. The red arrow is the initial arrow, and the green arrow is the final arrow. The arrow can move in four directions (forward, backward, left, right), where 'forward' always refers to the current direction the arrow is pointing. After each movement, the arrow's direction is updated to the direction of movement. Movement commands: - The red arrow moves forward for 1 unit. - The red arrow moves left for 1 unit (relative to its new current direction after step 1). Then turns green. Scene: - No change in scene composition. - No change in the layout of the diagram. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. A: (Forward, 1 unit) - (Left, 1 unit) Input Image: Reasoning Video: X Bad 1s frame sein sista sein saint e ceases! oy is 4 | 4 ' = = t 1 t > Lwl>| Jf] |eie) [ela] ies | Pd | VI. Question: Text-to-Video Prompt: Two small characters start from the same purple origin at the same time, and move along the red and green paths toward another purple destination at the same speed. Static camera, no zoom, no pan. X Bad i ni a WW Figure 5: Showcase of Visual Trace Reasoning (Part II) by Veo-3. The examples highlight long-horizon planning breakdowns, inconsistent arrow/trajectory rendering, and failures to preserve comparative or sequential information across frames. Q: What are the advantages of the green route and the red route respectively? Input Image: Reasoning Video: 1s frame progression between consecutive steps; and (ii) goal consistency, which means whether the full sequence visually completes the intended reasoning trajectory without deviation or contradiction. Definition of Good / Moderate | Bad. We rate the performance according to the following criteria: Vv Good: Each movement step is depicted continuously and logically toward the correct goal. The motion is smooth, temporally consistent, and follows causal order with no skipping, stuttering, or direction reversal. ~ Moderate: The overall trajectory roughly aligns with the intended sequence, but small discontinuities, timing irregularities, or partial missteps occur. The reasoning path remains interpretable, and the goal can still be inferred. X Bad: Key steps are missing, reversed, or illogical. The sequence shows abrupt jumps, inconsistent object trajectories, or goal confusion, breaking the temporal and causal coherence of the reasoning process. Data Source. We select samples from MVoT [41], FrozenLake [8, 72], MiniBehavior [32], RBench- V [25], SpatialViz-Bench [66], and OmniSpatial [29], which provide controlled multi-step envi- ronments for evaluating temporal reasoning, sequential planning, and causal continuity in visual simulations. ==== Page 9 ==== Example and Analysis. In Figure 4 and Figure 5, we showcase six representative visual-trace examples. In case I, the model repeatedly fails to execute the exact step sequence and instead drifts toward a visually salient central cell. However, case II is one of the few successes: the model can produce a coherent step-by-step path in simple, low-branching settings, but this behavior is not robust across trials. Case III largely fails, where the model often does not ground the specified object (printer), sometimes hallucinating its appearance or placement rather than performing a consistent pickup-and-place. Case IV shows near-uniform failure on long-horizon, highly branched navigation: outputs contain wrong turns, discontinuities, and no faithful global plan. Case V reveals difficulty grounding abstract movement rules, producing inconsistent arrow trajectories. Case VI produces visually plausible motions along individual paths but fails to preserve or present the comparative information required for contrastive reasoning. Taken together, these examples indicate that the model can simulate locally coherent short traces but systematically fails at long-horizon planning, rule-grounded execution, and object-persistent manipulations. Takeaway 2 Veo-3 can produce locally coherent, short-horizon trace animations in simple, low-branching scenarios, but it does not reliably execute long-horizon plans or rule-grounded sequences. 2.4 Real-World Spatial Reasoning Task Description and Evaluated Aspects. This task investigates Veo-3 [21]s ability to perceive and maintain spatial relations within natural scenes, with a focus on reasoning about viewpoint change, orientation consistency, and reference-frame alignment. We assess whether the model preserves a stable global coordinate frame and coherent scene orientation under varying viewpoints, and whether objects retain correct relative positions and orientations with respect to each other across different views. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Vv Good: Scene orientation, reference frame, and viewpoint are consistent and correctly represent spatial relations. The camera remains steady and the motion is natural. ~ Moderate: Scene roughly matches the instruction but contains small perspective errors, unnatural transitions, or partial mirroring. Motion remains interpretable but not physically coherent. X Bad: Reference frame or direction is wrong; viewpoint shifts abruptly or inconsistently. Video suffers from strong camera drift, disorienting motion, or spatial chaos. Data Source. To evaluate on orientation and layout reasoning, we specifically sample data from MMSI-Bench [74]. Also, the tasks of perspective taking and spatial interaction are selected from the OmniSpatial dataset [29]. Example and Analysis. As shown in Figure 6, Veo-3 can correctly handle basic spatial layouts in case I, but struggles with complex viewpoints or orientation changes in case II. The perspective transformations are sometimes inaccurate or even incorrect, suggesting that the model tends to prioritize visual plausibility over precise spatial reasoning, which hinders further reasoning in case IV. Moreover, case III demonstrates that Veo-3 has difficulty understanding depth, further limiting its spatial reasoning capability. Takeaway 3 While Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability remains insufficient for handling more complex spatial understanding tasks. ==== Page 10 ==== I. Question: Question: The balcony is north relative to the door, in which direction on the balcony is the chair? A: Southwest. Input Image: 1s frame II. Question: Question: From the perspective of the player wearing jersey number 10 in purple, where is the basketball? A: Left front. Input Image: TIT. Question: Q: From the dunker's viewpoint, which white- uniformed player is the farthest from them? A: Five. Input Image: 1s frame ~~ TV. Question: Q: If you are facing the washing machine, how should you walk to the stove and face the stove? A: Turn around and go straight, then turn left and go straight, then turn right and go straight, finally turn left to face the stove. Input Image: Text-to-Video Prompt: A red arrow point from the green chair toward the balcony. Another red arrow point from the door to the balcony. Static camera view, no zoom or pan. V Good Success Rate: 33% Reasoning Video: Text-to-Video Prompt: An arrow points from the player wearing jersey number 10 in purpleto the basketball. Static camera view, no zoom or pan. Reasoning Video: ~ Moderate eo) Success Rate: 17% Text-to-Video Prompt: The image transitions to a depth-map of the scene: Darker colors represent pixels further from the camera, lighter colors represent pixels closer to the camera. The exact color map to use is provided on the right side of the image. Static scene, no pan, no zoom, no dolly. Reasoning Video: Text-to-Video Prompt: The camera slowly and smoothly elevates from its current isometric view, gradually rising upwards while maintaining focus on the apartment layout. It continues to ascend until it reaches a complete overhead, bird's-eye perspective, providing a full top-down view of the entire floor plan, displaying all rooms and furniture clearly from above. The movement is fluid and controlled, ending with a static shot from the high vantage point. Reasoning Video: ~ Moderate e) Success Rate: 20% Figure 6: Showcase of Real-World Spatial Reasoning by Veo-3. Although Veo-3 can reason about simple spatial layouts, it still struggles to maintain consistency under complex perspective or orientation changes. 10 ==== Page 11 ==== I. Question: Text-to-Video Prompt: Q: Move the object up. A:D Input Image: Reasoning Video: Vv Good )  Success Rate: 83% 1* frame | Text-to-Video Prompt: Move the object up. Static camera view, no zoom or pan, and the perspective of the object remains unchanged throughout. II. Question: Q: Move the object to the left along the y-axis and up. A hand moves the object to the left along the y-axis and then moves it up. Static camera 9 view, no zoom or pan, and the perspective of the object remains unchanged throughout. A:D. Input Image: Reasoning Video: x Bad S) 1* frame a IIT. Question: Text-to-Video Prompt: Gp Ciek out anciuine The net is folded to form a single cube, with folding edges clearly shown. Static camera faces below: Can the net : ino Ralls tin arama cate perspective, no zoom or pan. yes or no? A: Yes. Input Image: Reasoning Video: X Bad GS) 1* frame & > x h & B IV. Question: Text-to-Video Prompt: Q: The sequence of moves that Smoothly zoom in to the "Initial State" figure. The yellow block, starting at (1,0,0), moves one turns the first cube stack into unit in the positive Y direction to position (1,1,0). Then, move back to (1,0,0). The cyan block, the final one is starting at (2,0,0), moves one unit in the positive Y direction to position (2,1,0), exchange the A: (1,0,0)y+ (1,1,0)y- @,1,0)y+ position with the purple block. Static shot. No pan. No glitches, noise, or artifacts. Input Image: Reasoning Video: X Bad Initial State Rotated intial State f 7 1st frame Initial State, Rotated intial State tit C Rotated intl state Inia State, Rotated Intl state Init State. Rotated inital State Initial State, oe : . . . . rotation clockwise within its own 2D plane. The camera stays static, with no pan. aan - 1 AA Input Image: Reasoning Video: X Bad Original 1 frame Original Original Original Original ae ia Bo Figure 12: Showcase of Rotation Reasoning by Veo-3. Veo-3 struggles in complex scenes. However, its foundational grasp of simple rotations signals its potential to support rotation-based reasoning tasks. 18 ==== Page 19 ==== Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: Vv Good: The rotation is accurate, complete, and strictly confined to the 2D plane, with no extraneous scene motion. The following reasoning tasks are completed correctly. Target objects remain precisely grounded after rotation. ~ Moderate: The rotation is largely correct but may be incomplete or slightly off-angle, though still confined to the 2D plane. The following reasoning tasks are mostly completed. Minor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or object grounding. X Bad: The model fails to perform the correct rotation, extends the transformation into 3D space, or introduces substantial scene distortion. Cannot complete the following reasoning task. The original 2D structure is altered, leading to inaccurate grounding of the target objects. Data Source. To specifically assess the rotation reasoning task, we recruit some PhD-level experts with deep expertise in text-image reasoning to design the evaluation data manually, followed by the necessary review process, as mentioned in Section 3.2. Each question is designed following the principle that it must involve a 2D rotation to reach the correct solution, ensuring the task genuinely probes rotational understanding rather than simple visual matching. Moreover, we sample data from the 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions for the video models. Example and Analysis. The results are shown in Figure 12. In case I, we find that Veo-3 handles small-angle rotations and simple planar scenes reasonably well, demonstrating a basic grasp of rotational motion. However, in more complex scenarios like cases II, III, and IV, the model often ignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect rotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such as OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment. These observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather than principle-driven. However, as it demonstrates a partial understanding of planar rotation, this can to some extent facilitate subsequent reasoning tasks. Takeaway 8 Veo-3 exhibits only a superficial understanding of rotation reasoning. While it can approximate small planar rotations, it fails to preserve geometric consistency under larger or compound transformations. 2.9 Table and Chart Reasoning Task Description and Evaluated Aspects. The table and chart reasoning task requires the model to identify and focus on the key elements within visualizations or tabular data. For evaluation, we further consider how effectively the model identifies the regions relevant to the query and whether it can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and proper scaling. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: Vv Good: Camera precisely focuses on the correct chart or table segment, smoothly high- lighting or zooming into the queried data (e.g., correct year, category, or value). Motion is continuous, the chart and table remain clear, and no distortion or overexposure occurs. ~ Moderate: Camera approximately focuses on the right region but partially misses boundaries, introduces slight blur, or transitions abruptly. Data can still be inferred. X Bad: Video fails to locate the correct region or changes the chart or table geometry unnaturally. Motion jitter, scaling errors, or artifacts make data unreadable or misleading. 19 ==== Page 20 ==== I. Question: Q: What is the sum of footwear manufacturing establishments in Nova Scotia and Mantioba as of December 2020? A:3 Input Image: Ee - 1s frame |  ~s 1 1 I II. Question: Q: In the year 2014, which opinion is dominant? A: Unfavorable. Input Image: 1s frame TIT. Question: Q: What' the color of smallest section in the chart? antity as Mixed Race A: Gray. Input Image: teaty Mice Race 1* frame | TV. Question: Q: What is the end market for the Engineered Systems segment? A: Printing & Identification, Industrials. Input Image: 1st frame Reasoning Video: has mestizo or mulatto? Text-to-Video Prompt: Start with smoothly zooming in to focus on the 'Nova Scotia' row. Then, smoothly zoom out to the full view of the chart. End with smoothly zooming in to focus on the 'Manitoba' row. The chart itself, including all its data, lines, and labels, must remain completely static and unchanged throughout the video. X Bad Text-to-Video Prompt: Start with a static, full view of the chart. Then, smoothly zoom the camera in to focus on the vertical area corresponding to the year 2014. The chart itself, including all its data, lines, and labels, must remain completely static and unchanged throughout the video. Reasoning Video: ~ Moderate eo) Success Rate: 83% Untavorable tinian Opinion of Hamas Declines, 7" ~ Unfavorable Untavorab 48 45 ae 53 8 2c >< Ee a a 53% 1a Bia Bs ake Favorab) 53 48 ou 59% 45 a Text-to-Video Prompt: Zoom in to focus on the smallest section in the chart. The chart itself, including all its data, lines, and labels, must remain completely static and unchanged throughout the video. X Bad Reasoning Video: nestizo or mul "race, that is, belon: re than one racial g 1s mestizo or mulatt 2% Don't [Refussed Fa Don't ne 2 Don't Refi Pew Research Center 21 Text-to-Video Prompt: Draw a bounding box around the end market for the Engineered Systems segment. The table itself, including all its text, lines, and labels, must remain completely static and unchanged throughout the video. X Bad Reasoning Video: Figure 13: Showcase of Table and Chart Reasoning by Veo-3. Veo-3 demonstrates an initial ability to focus on relevant data regions but lacks the precision and consistency required for reliable visual analysis. 20 ==== Page 21 ==== Data Source. We use samples from the ChartQA [52] dataset and TableVQA-Bench [34]. Example and Analysis. For charts, as presented in cases I, II and II in Figure 13, Veo-3 can often zoom into an approximately correct region but lacks the precision needed to accurately locate the queried data. For tables, as shown in case IV, Veo-3 fails to correctly identify the required element and tends to select entries randomly. The model also frequently adds, modifies, or distorts existing chart and table elements, resulting in visual inconsistencies that undermine the accuracy of chart interpretation. Takeaway 9 Veo-3 demonstrates emerging competence and potential in structured visual understanding, but still falls short of functioning as a precise and reliable chart-table reasoner. 2.10 Object Counting Reasoning Task Description and Evaluated Aspects. In this category, we focus on the ability to accurately enumerate objects within a 2D or 3D scene. In each instance, the model is required to identify, ground, and count target objects, typically by highlighting, drawing bounding boxes, applying numerical labels, or panning. The evaluation focuses on the accuracy of the count and the precision of the spatial grounding, performed within a scene that remains static or experiences only minimal motion, ensuring the counting process is not influenced. Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: Vv Good: The model precisely highlights, draws bounding boxes around, or labels the objects with correct numbers, and performs smooth and controlled panning when necessary to cover all targets. Motion is continuous, and the scene remains static or experiences only slight changes that do not influence the counting process. ~ Moderate: The model approximately highlights or draws bounding boxes around the objects, or performs panning with minor instability or incomplete coverage. Objects or the scene may move or change slightly, but this does not strongly affect the counting process. X Bad: The model fails to correctly highlight, label, or draw bounding boxes around the objects, or pans erratically such that parts of the scene are missed or revisited unnecessarily. Objects or the scene move or change substantially, severely affecting the counting process. Data Source. The 2D object counting data are sampled from the counting subset of RBench-V [25]. The 3D object counting data are from the Super-CLEVER dataset [45] and VAT [46]. Example and Analysis. The results are shown in Figures 14 and 15. In the 2D counting tasks from cases I to III, objects frequently move or change during the process, negatively impacting counting stability and accuracy. In the 3D counting tasks, Veo-3 successfully handles simple grounding and counting scenarios, as demonstrated in case V, but struggles with scenes involving complex materials or geometric variations in cases VI and VII, leading to inaccurate counts. Additionally, in the panning process of case VI, the camera fails to precisely move to the regions containing all target objects, further hindering the counting process. Takeaway 10 Veo-3 demonstrates basic counting capability but lacks the spatial control and robustness required for reliable object enumeration in dynamic or complex scenes. 21 ==== Page 22 ==== I. Question: Q: How many unit squares does the line segment pass through in the given grid diagram? A: 16 Input Image: II. Question: Q: How many rectangles are there in the figure? A:8 Input Image: tr cra 1* frame ah; a i=) III. Question: Q: How many rectangles are there in the figure? Text-to-Video Prompt: A scanner dot moves along the black line from bottom-left to top-right. As soon as this dot enters a new grid square, that entire square is instantly filled with yellow color and stays yellow. A square only turns yellow if the scanner dot on the line has entered it. Static camera, no zoom. X Bad Reasoning Video: T Text-to-Video Prompt: Highlight only the rectangles in the figure with a bright yellow color. Not highlight any other shapes like squares, triangles, circles, or irregular polygons. Static camera, no zoom, no pan. X Bad Reasoning Video: Text-to-Video Prompt: Label all the fish with increasing numbers (1, 2, 3, ...). The fish keep static. Static camera, no zoom, no pan. A: 18 Input Image: Reasoning Video: X Bad 1s frame _ 2  2 7 2 T= se 2  2. ae a ao  se = Figure 14: Showcase of 2D Object Counting Reasoning by Veo-3. Veo-3s lack of spatial control often introduces object motion, undermining the stability and accuracy of the counting process. 2.11 GUI Reasoning Task Description and Evaluated Aspects. In the Graphical User Interface (GUI) reasoning task, we focus on the capability to understand and interact with graphical user interfaces across different operating systems, including Android, Linux, and Web environments. In each instance, the model is required to perform actions, such as clicking on specific UI elements. The evaluation focuses on the accuracy of the click and the temporal coherence of the interaction, ensuring the scene and irrelevant UI elements remain consistent. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Vv Good: The click is precise, with no extraneous actions. No superfluous icons appear, and the original data and icons remain unchanged. ~ Moderate: The click is precise but may be accompanied by minor extraneous actions. Superfluous icons might appear but do not obscure the click target, and original data or icons show only slight alterations. 22 ==== Page 23 ==== IV. Question: Text-to-Video Prompt: Q: How many tiny things have the same material as Draw bounding boxes around the tiny things that have the same material as the green the green motorbike? motorbike. Static shot. Ail Input Image: Reasoning Video: X Bad @ 1* frame ~~ V. Question: Text-to-Video Prompt: Q: There is a small yellow object that is . oa: . to the left of the tiny metal motorbike; Draw bounding boxes around the brown metal mountain bikes to the right of how many brown metal mountain bikes the origami crane. Static shot. are to the right of it? A:l Input Image: Reasoning Video: v Good Success Rate: 100% 1% frame ~s ae VI. Question: Text-to-Video Prompt: Q: How many cyan things Draw bounding boxes around any matte tandem bikes and metal cruisers present in the are matte tandem bikes or scene. Static shot. metal cruisers? A:l Input Image: Reasoning Video:  Good  Success Rate: 33% 1* frame -~s VII. Question: Text-to-Video Prompt: Pan smoothly to include both the lidbody interface and the spout or cap in view at a fixed : How many burners are . aa : : Q  scale, keeping exposure steady and avoiding any visual or geometric changes. on the stove? A:4 Input Image: Reasoning Video: ~ Moserate  Success Rate: 17% eo ,@ wa See Figure 15: Showcase of 3D Object Counting Reasoning by Veo-3. Veo-3s basic 3D counting abilities are challenged by complex materials, geometric variations, and imprecise camera control. 23 ==== Page 24 ==== I. Question: Text-to-Video Prompt: Q: Collapse the pkgs folder. . . . A: : Click the pkgs folder to collapse it. Static shot. Reasoning Video: 1 frame II. Question: Text-to-Video Prompt: Q: A calendar icon A: - . . : 5 . located to the right of ns Click the calendar icon located to the right of the flight date options, the flight date options, ea  next to the price display for June 6. Static shot. next to the price with fight + hotel deats! Fe display for June 6. any fight changes Input Image: Reasoning Video: x Bad 240-0 96x86 Bo REDS muses e wesKore Perey B6KOI6 Buc RODS af, on Lf,  Hong Kong = Shanghai  Hong Kong =* Shanghai  Hong Kong = Shans 1 frame III. Question: Text-to-Video Prompt: Q: Anavigation A: axavency eee exe ine Click the navigation arrow located at the right edge of the browse by category carousel. Static shot. right edge of the browse by category carousel. Input Image: Reasoning Video: 1st frame a \ Figure 16: Showcase of GUI Reasoning by Veo-3. Veo-3s attempts at graphical interface interaction exhibit visual inconsistencies and logical inaccuracies, indicating only a shallow grasp of underlying GUI logic. Note that the answer to each question is a bounding box. For visual clarity, screenshots with the ground-truth bounding boxes are shown. X Bad: The click is imprecise or erratic. Original data and icons are significantly altered, hindering judgment and assessment. Data Source. The Linux data are selected from the Common Linux Screenshot subset of ScreenSpot- Pro [42], while the Android and Web data are drawn from the OS Android and OS Web subsections of MMBench-GUI [67], respectively. Example and Analysis. Across the three cases in Figure 16, Veo-3 fails to accurately capture the correct click position and often exhibits inconsistencies between the click location and the resulting on-screen effect. In addition, it occasionally alters or generates new icons and text, which can interfere with judgment. In the Web system in case III, however, the model demonstrates partial GUI responsiveness and provides some degree of visual feedback. 24 ==== Page 25 ==== I. Question: Text-to-Video Prompt: Q: Which point corresponds to the affordance for manipulating the banana? Pan to the banana while keeping tray edges in view. Fix scale (banana ~two-thirds of the frame, axis horizontal). Sweep once along the inner concave edge from stem to tip at constant speed, then stop and hold at its midpoint. Input Image: Reasoning Video: ~ Moderate eo) Success Rate: 33% 1* frame a ] II. Questiont: Text-to-Video Prompt: Q: Which set of 4 A: points is a right trajectory when doing place a cucumber into a pot? Keep the cucumbers start and the pot opening in view. Sweep once from start to pot at fixed scale and speed, briefly dwelling at four evenly spaced waypoints (p1p4) along the path, then hold on both endpoints. Input Image: III. Question: Text-to-Video Prompt: Q: Is the container sealed? Pan smoothly to include both the lidbody interface and the spout or cap in view at a fixed A: No. scale, keeping exposure steady and avoiding any visual or geometric changes. Input Image: Reasoning Video: ~ Moderate eo) Success Rate: 17% Figure 17: Showcase of Embodied Reasoning by Veo-3. It illustrates plausible static affordance detection in simple settings, common workaround/hallucination behaviors for dynamic manipulations, and failures to reliably localize or preserve manipulation-relevant context. ' Green points in the answer image denote ground-truth points or trajectories. Takeaway 11 Veo-3 demonstrates a limited awareness of GUI click actions, imitating interaction behaviors without fully grasping the underlying functional logic. 2.12 Embodied Reasoning Task Description and Evaluated Aspects. This category evaluates the models potential to perceive and reason about object affordances and manipulation dynamics. It involves recognizing both static and dynamic affordances, as well as identifying manipulation-relevant object and scene attributes. Evaluation focuses on two aspects: (i) the generation of stable and contextually relevant visual sequences, and (ii) the maintenance of reasoning fidelity without resorting to implausible planning shortcuts or hallucinated interactions. 25 ==== Page 26 ==== Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Vv Good: The sweep/framing covers all candidates fairly (equal or near-equal dwell), centers the manipulation-relevant geometry (e.g., handle + frame/gap, lid-body interface, hinge side) with crisp focus and stable scale; no cropping of key context; no content alterations. ~ Moderate: The view roughly includes the right region(s) but with minor bias or coverage issues: slight off-center, brief under-exposure of one candidate, small motion jitter, or shallow context (still enough to infer). X Bad: The camera misses or biases the evidence (e.g., lingers only on one point, crops away the hinge/rail, over-zooms a non-relevant patch), introduces distortion/content edits, or produces footage from which a fair decision cannot be made. Data Source. We select samples from Robobench [51] for the analysis. In addition to a general understanding of static attributes, we also sample data to assess whether Veo-3 can perform direct reasoning on tasks involving the generation of static and dynamic affordances. Example and Analysis. As shown in Figure 17, Veo-3 demonstrates the ability to comprehend objects within real-world scenes. However, its capacity for assisting visual reasoning in embodied scenarios remains constrained by insufficient stability. As illustrated in case I, when provided with a clearly defined object for manipulation, Veo-3 is capable of generating plausible manipulation affor- dances. When it comes to dynamic affordances, Veo-3 tends to employ workarounds to compensate for its planning deficiencies, as evidenced in case II, where it generated a new cucumber instead of the intended object. With respect to static attributes, Veo-3 struggles to accurately differentiate visual prompts and misidentifies the position of containers. As shown in case III, the green box, intended to specify the location of the container, inadvertently led Veo-3 to produce hallucinations. Takeaway 12 Veo-3s capabilities are currently limited to basic object recognition rather than true embodied reasoning. It lacks the necessary planning and stability to reliably interpret and act upon dynamic or spatially constrained instructions, indicating its limitations in understanding and reasoning of real-world interactions. 2.13 Medical Reasoning Task Description and Evaluated Aspects. This category assesses the models ability to localize lesions or structures, identify relevant attributes (e.g., side, lobe), recognize pathological patterns (e.g., jump distribution), and make binary decisions (e.g., presence or absence). The evaluation focuses on both the correctness of object manipulation and the visual stability of the surrounding regions. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Vv Good: The camera cleanly settles on the correct anatomical level/lesion, with clear margins and readable context; motion is reasonable; no geometric distortion or content alteration. ~ Moderate: The view roughly covers the right area but is slightly off (partial coverage, mild blur, small framing mistakes). The general shape of the tissue or organ can still be observed. X Bad: The video misses the target region or introduces distortions/crops that hide key cues. Tissues or organs begin to distort. Misleading results due to confusion of medical terminology. Data Source. We select samples representing different body parts from the ViTAR [9] dataset. Example and Analysis. We showcase the evaluation results in Figure 18. Veo-3 retains the ability to manipulate images when dealing with medical images. However, due to its lack of medical 26 ==== Page 27 ==== I. Question: Text-to-Video Prompt: Q: What is the distribution Show the full sagittal lumbar view, then sweep smoothly from L1 to S1 at constant speed pattern of stenotic segments? without pausing. End on a view showing adjacent disc spaces, including narrow and normal A: Jump distribution. levels. Keep image content and geometry unchanged. Input Image: Reasoning Video: 1* frame a II. Question: Text-to-Video Prompt: : Show the full PA chest view, then adjust framing to include both the heart silhouette and iI d h eP : . ae 5 z N i SEATON PASS) the widest inner thoracic diameter at a fixed scale. Keep contrast and geometry unchanged, holding steady for visual CTR estimation. Input Image: Reasoning Video: X Bad 1* frame f aA be a be  TIT. Question: Text-to-Video Prompt: Q: Which lobe contains . . the pulmonary nodule? Show the full axial CT, then pan and zoom smoothly to the right lung so the nodule and A: Left lobe. nearby fissure appear together. Keep windowing standard and geometry unchanged. Input Image: Reasoning Video: X Bad 1s frame v- ye A S FR y : Fx, Figure 18: Showcase of Medical Reasoning by Veo-3. As shown in cases I and II, Veo-3 fails to maintain the shape of the rest of medical organization. Veo-3 also can not understand and precisely locate the mentioned medical terminology in the prompt, as demonstrated in case II. knowledge, Veo-3 struggles to accurately manipulate the correct objects when instructions include medical terminology. This phenomenon is evident across all cases. Furthermore, Veo-3 cannot model medical organs effectively. When performing operations such as zooming in, the medical images suffer from significant distortion, resulting in a substantial loss of detail. Takeaway 13 Veo-3s failure to handle the reasoning in the medical domain, causing distortion even on simple zoom-ins, highlights its limited grasp of specialized, non-general knowledge. 27 ==== Page 28 ==== is nin Se %5.9 Table 1: Key Statistics of MME-CoF. Vay 0; Vis, Rea, 11.96 @, ran 2 Statistic Number Total entries 59 Total categories 12 Max prompt length 124 Avg prompt length 36.7 Max entries per category 7 Avg entries per category 4.9 Figure 19: Category Distribution. 3 MME-CoF 3.1 Benchmark Overview To standardize the empirical study and systematically evaluate the reasoning potential of state-of-the- art generative video models [21, 55, 56], we introduce MME-COF, which, to our knowledge, is the first benchmark specifically designed to reveal and quantify the reasoning potential of video models. 3.2 Benchmark Composition Data Curation and Distribution. Aligning with the task taxonomy in Section 2.1, the MME-COF benchmark is curated from the cases used in our empirical study. It comprises 59 curated entries and instruction prompts spanning 12 diverse reasoning categories. The key statistics of MME-COF and its overall composition are summarized in Table |, Figure 2b and Figure 19. Review Process. Following the prompt design protocol in Section 2.1, all prompts undergo a two-stage review process. In the cross-validation phase, each prompt was independently reviewed by another expert to ensure semantic clarity, alignment with the intended reasoning task, and the absence of linguistic bias. In the final adjudication phase, discrepancies were jointly discussed and resolved through consensus. This multi-step procedure ensured that every prompt was conceptually precise, visually grounded, and fully aligned with the evaluation objectives of MME-CoF. 3.3. Evaluation Protocol Models and Generation Settings. We evaluate the leading video models in a zero-shot setting, including Kling-v1 [38], Seedance-1.0-pro [19], Veo-3.0-preview [70], Veo-3.0-fast [70], Sora-2 [56], Sora-2-pro [56]. Each model generates six video samples per prompt, and final scores were computed as the mean across all samples. All videos are generated at a 16:9 aspect ratio. We adopt the default 8-second duration for the Sora and Veo series, while retaining the default 5-second length for Kling and Seedance. Note that, since most video models apply automated safety filters and content moderation, which may block sensitive content, we exclude videos that are suppressed by such filters from our evaluation. Evaluation Metrics. We employ Gemini-2.5-Pro [12] as an automatic verifier to evaluate each generated video. Gemini is prompted with the following evaluation criteria and returns structured scores between 0 and 4, where higher values indicate better performance: 1) Instruction Alignment (0-4): Measures how well the video follows the described structure and sequence in the prompt. A high score indicates that the visual steps faithfully reflect the textual instructions. 2) Temporal Consistency (0-4): Evaluates the smoothness and continuity between frames. Disjointed or abrupt transitions will lead to a lower score. 28 ==== Page 29 ==== Table 2: Model-level Overall and Per-dimension Performance on MME-COF. Mean scores and standard deviations are reported on a 0-4 scale, as graded by Gemini-2.5-Pro. Model Overall Instruction Temporal Visual Content Focus Alignment  Consistency Stability Fidelity Relevance Kling-v1 [38] 0.64+0.91 001+0.09 O15+0.75 2.4341.86 0.2140.79 0.43+ 1.07 Seedance-1.0-pro[19] [REDACTED] 0.[REDACTED] 16541.57 2.0041.72 1.[REDACTED] 1.98+ 1.75 Veo-3.0-fast [21] 14441.51 0564+1.09 1.3741.51 1.8841.73 1.10+1.52 2.274 1.69 Veo-3.0-preview [21] [REDACTED]+1.06 14341.53 18941.71 1.[REDACTED] 2.26+ 1.73 Sora-2-pro [56] 1.[REDACTED]+0.96 1.3641.59 2.[REDACTED]+41.72 244+ 1.73 Sora-2 [56] 1.[REDACTED] 0.[REDACTED] 2.[REDACTED] 16241.75 2.52+1.71 Table 3: Per-category Scores on MME-COF. Mean scores and standard deviations are reported on a 0-4 scale, as graded by Gemini-2.5-Pro. . Seedance-1.0 Veo-3.0 Veo-3.0 Sora-2 Category Kling-v1 [58] Pro [19] Fast [21] Preview [21] ?! pro [56] Visual Detail 0.72 + 0.69 1.37 + 1.39 11041.24 159+ 1.68 1.14+1.32 1.08+1.89 Visual Trace 0.49 + 0.65 1.23 + 1.13 14341.26 14841.24 1.5141.37 1.75+1.31 Real-world Spatial 0.77 + 0.76 1.79 + 1.53 2.[REDACTED] 2.[REDACTED] 1.7741.35 3D Geometry 0.61 + 0.58 1.95 + 1.64 1.714 1.54 1.54 + 1.43 1.374 1.49 1.42 + 1.45 2D Geometry 0.49 + 0.67 0.96 + 1.11 1.18 + 1.15 1.2741.20 1.[REDACTED] 1.77+1.21 Physics-based 0.60 + 0.62 1.27 + 1.25 14441.[REDACTED].32 2.10+1.33 Rotation 0.22 + 0.34 2.30 + 1.46 1.[REDACTED] 16041.29 16241.37 1444+ 1.28 Table & Chart 0.87 + 0.72 0.71 + 1.18 0.82+1.30 0.[REDACTED] 148+ 1.59 GUI 1.09 + 0.51 0.70 + 0.[REDACTED] 18841.64 152+ 1.48 Object Counting 0.64 + 0.58 1.15 + 0.97 2.03 + 1.42 1.84 + 1.42 2.06 + 1.48 1.86 + 1.41 Embodied 0.80 + 0.00 1.82 + 1.67 1.3341.57 1.[REDACTED] 140+ 1.42 Medical 1.15+41.17 1.56 + 1.41 0.2740.39 0.[REDACTED] 2.08+1.56 1.81 + 1.42 3) Visual Stability (0-4): Assesses the stability of the video in terms of camera motion, object appearance, and scene composition. Shaky or glitchy outputs are penalized. 4) Content Fidelity (0-4): Determines how accurately the key elements described in the prompt are preserved. Hallucinated or missing objects/events will reduce the score. 5) Focus Relevance (0-4): Examines whether the videos visual attention remains focused on the correct objects or regions throughout. Irrelevant distractions or poorly framed targets are penalized. We adopt a direct prompting strategy, instructing Gemini with the prompt, videos, and evaluation criteria to produce numerical scores in JSON format directly. 3.4 Quantitative Results and Analysis We report the quantitative scores of the five evaluated models across the five reasoning dimensions in Table 2, and provide detailed per-category results in Table 3 and Figure 2a. Overall, most models exhibit limited reasoning capability across all tasks in MME-COF, reflected by generally low scores. Among the five dimensions, Visual Stability achieves the highest average, indicating that current video models can generate smooth and coherent sequences. Yet, their behavior remains largely at the level of pattern replay rather than genuine reasoning. The Sora-2 series [56] shows relative advantages in physics-based, embodied, and medical reason- ing, while the Veo-3.0 series [21] performs comparatively better in real-world spatial reasoning. Seedance-1.0-pro [19] demonstrates relative strength in rotation and 3D geometry reasoning. These trends suggest that different models specialize in distinct reasoning aspects. However, their mean scores remain below 2.0 out of 4, highlighting substantial room for improvement and pointing to opportunities for more targeted enhancement in future development. 29 ==== Page 30 ==== 4 Related Work Video Models. Video models have been progressively evolving both in the fields of video under- standing and generation. For video understanding methods, earlier approaches, such as MViT [14], Video Swin Transformer [48], and VideoMAE [62], aim to learn a robust representation that fosters downstream tasks. With the rise of LLMs, recent approaches encode videos as tokens and exploit the language backbone for captioning [61], event localization [59], and high-level reasoning [28, 83]. Video generation models have also attracted much attention. Closed system, including OpenAIs Sora [55, 56], Runways Gen-3 [58], Pika Labs [57], Luma AI [50], and Google DeepMinds Veo series [20, 21], have exhibited impressive results. However, they remain inaccessible due to their closed-source nature. Open-source alternatives have recently become available: Stable Video Diffusion [6] introduces efficient training strategies, Hunyan- Video [37] proposes systematic scaling, and Wan-2.1 [64] presents an efficient 3D VAE with expanded pipelines. Reasoning with Video. The advent of large reasoning models [24, 60, 27, 69], such as OpenAI ol [54] and DeepSeek-R1 [23], has spurred the development of video reasoning benchmarks. Most current methods [15, 44, 53] employ MLLMs specialized in video reasoning understanding. For example, Video-R1 [15] specifically targets temporal reasoning capabilities by introducing a temporal group relative policy optimization (GRPO) loss. VideoChat-R1 [44] focuses on spatio-temporal reasoning abilities by training with GRPO and rule-based rewards. A two-stage training strategy, combining SFT and RL, is used by VideoRFT [65]. When trained on vast collections of images and videos, this strategy boosts the models ability to handle QA tasks, whether in general con- texts or reasoning-focused ones. These methods primarily focus on enhancing specific types of question-answering or captioning tasks. Concurrently, [70] demonstrates the large potential of video generative models in video reasoning. These models have implicitly acquired world knowledge throughdemonstrates impressive performance on various tasks, includinging and reasoning capability. Yet, this direction has rarely been explored and only experimented with in zero-shot settings. Evaluation of Video Models as Zero Shot Learner. Recently, several works have been exploring the zero-shot capability of video generation models in various domains, including general-purpose vision understanding [70, 17], medical imaging [39], and world models [68]. [70] conducts experi- ments on Veo 3 with a variety of vision tasks that have not been explicitly included during training. The video model showcases surprising performance on multiple tasks like object segmentation, image editing, and even maze solving. [39] later adopts a similar paradigm to medical images understanding tasks and finds video generation models also show powerful capabilities, e.g., delineation of anatomi- cal structures in CT scans, medical image segmentation, and even forecasting of future 3D CT phases. Besides, [68] shows that video generation models could also understand complex temporal causality and world knowledge in the real world, thereby serving as a world model [2, 33]. 5 Conclusions and Insights Video models demonstrate an intuitive understanding of the simple visual world. Recent video models can generate high-fidelity videos with realistic motion dynamics, suggesting that they have internalized substantial visual and structural knowledge about the world. Through qualitative results from our empirical study and quantitative results from the MME-COF benchmark, our work confirms that these models do exhibit intuitive yet local reasoning potential. This emergent behavior, which aligns with the Chain-of-Frame (CoF) mechanism, is revealed across several common success patterns. (i) Fine-grained Grounding. Models demonstrate a capability for fine-grained attribute and spatial grounding, especially when targets are visually distinct, as presented in visual detail reasoning tasks. (ii) Short-horizon Trace Consistency. In Visual Trace Reasoning tasks, models can maintain short-term consistency in visual traces. (iii) Emergent Tool-Use Simulation. An emergent ability to follow CoF instructions that mimic tool-use is presented, such as drawing lines in 2D geometry, highlighting targets in object counting, or controlling the camera in table and chart reasoning. (iv) Foundational Spatial and Geometric Grasp. This includes single-step 3D geometry transformations, understanding basic real-world spatial layouts, finding coherent sequential paths, and handling small-angle Rotations. (v) Preliminary Real-world Interaction. Models display a preliminary comprehension of real-world interaction, generating coherent manipulation paths in embodied reasoning. 30 ==== Page 31 ==== Complex visual reasoning reveals fundamental limitations. However, visual reasoning demands more than these foundational skills. It tests a models ability to maintain long-horizon logical consistency, adhere to abstract constraints, and understand functional principles. In these complex areas, our study reveals fundamental limitations and several common failure patterns. (i) Causal and Physical Logic. This is evident in physics-based reasoning, where the model generates implausible motion that violates basic causal principles, and in visual trace reasoning, where the generated sequences break causal order with illogical steps. (ii) Long-horizon and Rule-grounded Reasoning. In visual trace reasoning, models fail to maintain state and adhere to task-specific rules over extended sequences. (iii) Geometric and Spatial Logic. Models fail at multi-step or complex transformations in 3D/2D geometry and real-world spatial tasks, often breaking constraints or prioritizing visual plausibility over correctness. (iv) Functional and Interaction Logic. They merely imitate GUI actions without grasping their purpose and lack the necessary planning and stability for reliable Embodied tasks, often resorting to workarounds. (v) Perceptual Precision and Specialized Knowledge. This weakness appears when models fail to identify small or indistinct targets in visual detail reasoning, distort data in table and chart tasks, and fail to process specialized medical imagery due to a lack of domain understanding. Current video models are not yet ready as standalone zero-shot reasoners. Overall, our findings show that current video models are not yet reliable as standalone zero-shot reasoners. Strong generative performance does not automatically imply robust reasoning during inference. The models behavior appears to be driven more by learning surface-level patterns and correlations rather than by internalizing general principles. It excels at short-term coherence rather than long-horizon causality. This is evident when the model prioritizes visual plausibility over precise spatial reasoning, or favors visually symmetric patterns over strictly adhering to geometric instructions. This tendency to produce plausible but instructionally flawed outputs reveals a reasoning process that is pattern-driven, not principle-driven, thereby undermining its ability to function as a standalone zero-shot reasoner. The potential in advancing next-generation collaborative visual reasoning. Despite these limitations, the emergent behaviors observed in video models signal strong potential. The CoF concept suggests a novel modality for reasoning through visual problems step by step. While these models are not yet robust standalone reasoners, their foundational capabilities demonstrate that they can be guided through carefully designed prompts. This suggests a path where video models exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [5] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378, 2025. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do- minik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent 31 ==== Page 32 ==== 4  = sy  4 sy 4 =  4 diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22563-22575, 2023. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv: 1606.01540, 2016. Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong Wang, Mu Zhou, and Mianxin Liu. Think twice to see more: Iterative visual reasoning in medical vlms. arXiv preprint arXiv:2510.10052, 2025. Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought reasoning. arXiv preprint arXiv:2506.05331, 2025. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-IIms. arXiv preprint arXiv:2406.07476, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages [REDACTED], 2021. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-rl: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Zhanzhou Feng, Qingpei Guo, Xinyu Xiao, Ruihan Xu, Ming Yang, and Shiliang Zhang. Unified video generation via next-set prediction in continuous domain. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19427-19438, 2025. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal Ilms in video analysis. CVPR 2025 Highlight, 2024. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [20] Google DeepMind. Veo 2, 12 2024. Accessed: 2024. [21] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025. [22] Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and Ruihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation and answering. arXiv preprint arXiv:2503. 16867, 2025. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 32 ==== Page 33 ==== [24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [25] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: A primary assessment for visual reasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025. [26] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, and Shi-Min Hu. Rbench-v: A primary assessment for visual reasoning models with multi-modal outputs. 2025. [27  Ziyu Guo*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. CVPR 2025, 2025. [28 4 Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [29] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. [30] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix, and William El Sayed. Mistral 7b, 2023. [31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Martin-Martin. Mini-behavior: A procedurally generated benchmark for long-horizon decision- making in embodied ai. arXiv preprint arXiv:2310.01824, 2023. [33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Huang Gao, and Jiashi Feng. How far is video generation from world model?  a physical law perspective. arXiv preprint arXiv:2406. 16860, 2024. [34] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. [35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022. [36] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [37  Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/, June 2024. [39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video models emerging as zero-shot learners and reasoners in medical imaging? arXiv preprint arXiv:2510.10254, 2025. 33 ==== Page 34 ==== [40] [41 sy [42  [43 4 [44 sy [45 4 [48] [49] [52] [53 4 [54] [55] [56] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay Krishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations. arXiv preprint arXiv:2506.04633, 2025. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-rl: Enhancing spatio-temporal perception via reinforce- ment fine-tuning. arXiv preprint arXiv:2504.06958, 2025. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14963-14973, 2023. Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual abstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025. Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages [REDACTED], 2022. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. LumaLabs. Dream machine, 06 2024. Accessed: 2024. Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, and Shanghang Zhang. Robobench: A comprehensive evaluation benchmark for multimodal large language models as embodied brain, 2025. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages [REDACTED], Dublin, Ireland, May 2022. Association for Computational Linguistics. Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-03 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. OpenAI. Openai ol = system card. https: //openai.com/index/ openai-ol-system-card/, December 2024. Accessed: 2024-12-05. OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024. OpenAI. Sora 2 system card. Technical report, OpenAI, September 2025. 34 ==== Page 35 ==== [57] PikaLabs. Pika 1.5, 10 2024. Accessed: 2024. [58] Runway. Introducing gen-3 alpha: A new frontier for video generation. https: //runwayml. com/research/introducing- gen-3-alpha/, June 2024. [59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages [REDACTED], 2018. [60] Chengzhuo Tong*, Ziyu Guo*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: A study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. [61] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: A versatile metric for evaluating image and video captions using gpt-40. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages [REDACTED], 2025. [62] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [64] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [65] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025. [66 = Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: An mllm benchmark for spatial visualization. arXiv preprint arXiv:2507.07610, 2025. [67] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025. [68] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and Lei Zhang. Videoverse: How far is your t2v generator from a world model? arXiv preprint arXiv:2510.08398, 2025. [69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [70] Thaddus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [71] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal lms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084-13094, 2024. [72] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. [73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic: A benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504. 15279, 2025. 35 ==== Page 36 ==== [74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: A benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [76] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10459-10469, 2023. (77  Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages [REDACTED], 2024. [78 4 Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. [79  Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal IIm truly see the diagrams in visual math problems? ECCV 2024, 2024. [80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pages arXiv2407, 2024. [81] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024. [82  Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. [83 4 Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Confer- ence, pages [REDACTED], 2025. oo K e Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [85 4 Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive up- ward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 26183-26191, 2025. 36

==== Page 1 ==== 2510.26790v1 [cs.CL] 30 Oct 2025 arXiv ve Gistify! Codebase-Level Understanding via Runtime Execution Hyunji Lee*!, Minseon Kim2, Chinmay Singh?, Matheus Pereira?, Atharv Sonwane, Isadora White, Elias Stengel-Eskin, Mohit Bansal!, Zhengyan Shi2, Alessandro Sordoni?, Marc-Alexandre Ct2, Xingdi Yuan?, Lucas Caccia*? *Equal contribution University of North Carolina at Chapel Hill ?Microsoft Research 3Cornell University University of California San Diego University of Texas at Austin [REDACTED] [REDACTED] https: / /microsoft.github.io/debug-gym As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose GISTIFY, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on GISTIFY requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve GISTIFY tasks, especially ones with long executions traces. 1 Introduction Large language models (LLMs) are increasingly being used in code-related tasks, powering applications in debugging (Yuan et al., 2025) and agentic code generation (Yang et al., 2024; Liang et al., 2025). Thus, the ability to handle isolated snippets and reasoning across entire codebases, including complex file and module relationships, is becoming increasingly essential. Yet, the evaluation toolkit for assessing such capabilities has lagged behind. Recent evidence shows that widely-adopted repository-level benchmarks such as SWE-bench (Jimenez et al., 2024) and RepoBench (Liu et al., 2023b) still do not require full reasoning over the whole execution and could be solved through heuristic shortcuts or retrieval of localized patches (Aleithan et al., 2024; Liang et al., 2025). Moreover, because many of these datasets rely on GitHub issues or pull requests for construction, they are not easily generalizable to arbitrary repositories. At the same time, coding agents are increasingly deployed in large, real-world codebases, highlighting the need for automatically constructed, broadly applicable, and more challenging repository-level evaluation. To fill this gap, we introduce the GISTIFY task, which is deliberately inspired by a common practice of how developers navigate and understand unfamiliar repositories. Rather than reading files in isolation, they start from a concrete execution point such as test command or entry script often mentioned in READMEs. Then, they iteratively reason over the runtime behavior such as identifying dependencies, following control paths to uncover the codebases structure and functionality. GISTIFY formalizes this practice by requiring an (agentic) coding model to extract the gist of a given command, i.e. to generate a single, self-contained, minimal, and executable gistified file that faithfully reproduces the runtime behavior of a given command as when using the original full codebase (Figure 1). In addition to serving as a challenging coding task, such gistified repositories might give human coders a better understanding of a specific functionality of a given codebase, or even a way to export the single functionality of interest without inheriting heavy dependencies. To perform well in GISTIFY, an agent should generate a single gistified file that satisfies four key requirements: it should be self-contained, including all necessary components from the codebase so that it can be executed independently; it should ensure execution fidelity, producing the same outputs as the original codebase under the given command; it should satisfy minimality, retaining only the essential code required for execution without redundant or extraneous lines; and it should guarantee faithful preservation, avoiding hallucinated or fabricated code and relying solely on content from the original codebase. To assess model performance, we introduce evaluation metrics that align with these requirements, providing a systematic ==== Page 2 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution compact.py auth.py from http.cookies import Morsel gistified_file. py def _basic_auth(username): from http.cookies import Morsel > test_requests. py def _basic_auth(username): adapters. py from requests.compact import Morsel from adapters import HTT PAdapter from auth import _basic_auth class BaseAdapter: class TestMorsel: class BaseAdapter: morsel = Morsel() def auth(self): _basic_auth(self.name) class HTT PAdapter(BaseAdapter): def test_cookie(): def __ init__(self): s = TestMorsel() ve s.mount(HTT PAdapter(0, 0)) self.auth() class TestMorsel: morsel = Morsel() MI def test_cookie(): s = TestMorsel() s.mount("http://", HTT PAdapter(0, 0)) Gistify Figure 1: The GISTIFY task: given a codebase and a command of entrypoint, the goal is to generate a minimal, self-contained gistified code file that faithfully reproduces the original runtime behavior using code from the given codebase. way to measure codebase-level understanding. GISTIFY requires agents to follow the execution path through the codebase without bypassing modules, i.e., understanding how relevant objects are modified along the way, and identifying which classes or functions can be simplified or removed. Since even moderately sized codebases exceed the context window of current LLMs, success also requires effective search capabilities. The advantages that GISTIFY brings are multiple: first, it provides direct insight into the ability of models to reason at the codebase level with an understanding of runtime execution, rather than on isolated code snippets. Second, it is lightweight and broadly applicable: it requires only the repository and an entrypoint, without issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary repositories, including private ones. Finally, gistified files themselves are valuable outputs: by compressing a specific feature of a large codebase into a minimal file, they can be applied to various downstream tasks, including automated debugging or error localization. We conduct experiments across a variety of frameworks (mini-SWE-agent, SWE-agent, and Copilot) and models (GPT-5-mini, GPT-5, Claude-3.7-Sonnet, and Claude-Sonnet-4) and uncover several interesting findings. First, even widely used, high-performing frameworks and models struggle to create a successful gistified file, especially when execution traces are long and have high coverage on the repositories. Second, faithfully reproducing the test function in the generated file is a strong indicator of gistified performance, as it serves as the starting step for reasoning about execution traces. Third, enabling execution tools yields small but consistent performance gains, and additionally providing global code context and runtime information further boosts performance. Finally, agentic models benefit from dynamically deciding what to read and refine their reasoning through multi-step trajectories, outperforming static approaches. 2 Gistify 2.1 Task Definition As shown in Figure 1, when given a codebase and a command as input, the coding agent must generate a single gistified file that reproduces the runtime behavior of the original codebase under the given command. Specifically, the gistified file must satisfy the following requirements. Self-Contained: All necessary components from the given codebase must be included so that the gistified file can be executed standalone, i.e. without relying on the codebase. The model must identify all relevant modules and dependencies, demonstrating understanding of inter-file relationships. Execution Fidelity: Executing the gistified file must replicate the original codebases runtime behavior, ensuring the model captures the dynamic execution, not just static code patterns. ==== Page 3 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Minimalism: Only the code essential to reproducing the runtime behavior should be preserved, with unused functions and objects pruned. This requires fine-grained understanding of the code to identify which lines are actually executed and essential for the task. Grounded Preservation: No hallucinated code may be introduced. All content must be derived directly from the original codebase. This ensures the task evaluates the models understanding of the codebase, rather than its ability to generate arbitrary code that happens to satisfy the command. 2.2 Evaluation Protocol There are two inputs to a GISTIFY task: i) a docker image containing the target codebase, for consistent evaluation; ii) an entrypoint, such as a pytest command on one of the tests in the codebase. Test cases are existing entrypoints one can easily leverage, but broadly, any command that the user would want to use to run a functionality of the existing codebase is allowed. All models are prompted to generate a gistified file for the entrypoint. We can programmatically verify whether the expected behavior is preserved when the ground-truth test is run within this setup. Here, we focus on comparing outputs of test commands. Once the model generates the gistified file, to ensure that execution for evaluation is based on the original test, we integrate the test code from the original codebase to the gistified file and execute it. This ensures that the model does not cheat by modifying the test. 2.3. Metrics Once a gistified file is generated, we evaluate it using the given execution command. The evaluation considers three dimensions, aligned with the task requirements, to provide a comprehensive measure of a models ability to reason over an entire codebase and understand its execution behavior. See Appendix A.1 for more details. Execution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same output as the original codebase when executed under the given command; otherwise, it is 0. Failures include cases where the file is not runnable or yields different outputs. The comparison checks for tests pass/fail consistency and stdout/stderr matching. Formally, let c denote the given command, C a given codebase, and G a gistified file. Define runs(c,C) as an indicator of whether c executes without crashing when running over C, and out(c,C) returns the set of outputs and error traces from running c with C. Then, execution fidelity is defined as 1[runs(c, G) A (out(c,G) = out(c,C))], (1) where 1{-] is the indicator function. Line Execution Rate measures minimality by calculating the fraction of lines in the gistified file that are actually executed under the given command. A 100% execution rate means all lines are essential, indicating a focused and concise file. This metric is only computed for files that run successfully, since the execution trace is required to determine which lines are run. Formally, let Lexec(G) be a list of executable lines (i-e., no comments) in G. Then, the Line Execution rate is defined as 1 a 1[ is executed]. " | Lexee(G)| emo Line Existence Rate measures the proportion of code in the gistified file that is directly preserved from the original codebase. Specifically, lines of code are grouped into blocks (classes, functions, or top-level units), and matches are computed block by block while respecting the code hierarchy. This helps avoiding false matches from common lines appearing in unrelated parts of the codebase. To ensure robustness, we normalize across common variations such as indentation, multi-line statements, and imports. A 100% existence rate indicates full fidelity to the original codebase without hallucination. ==== Page 4 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Formally, let Bg and Be be the sets of blocks in the gistified file and the original codebase, respectively. For a block 6, let (b) represent its set of lines. Then, the existence rate is defined as = I pj Qe De Lhe Le(b)}. (3) | Be LEL(b) where 1{  e(b)} = 0, if no matching block exists in Be. 3 Experiments 3.1 Setting We conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent (Yang et al., 2024) and GitHub Copilot (Microsoft, 2025) provide a rich scaffolding to LLM-based agents, enabling them to autonomously perform software engineering tasks. This includes a set of tools for creating and editing code files, navigating repositories, and executing tests. These frameworks also offer the LLM controllable cache management, and LLMs follow the standard tool-calling format. We also experiment with Mini-SWE- Agent (Yang et al., 2024), a lightweight framework where LLMs only have access to a bash terminal to solve the task. Commands are parsed from the agent output and executed directly. As the task objective is for the model to use reasoning over the execution flow rather than the ability of tool usage, for the agentic models, we exclude the execution tools (python, pytest) in the default setting where execution is disabled. Our evaluation spans four leading LLM variants: GPT-5 (OpenAI, 2025a), GPT-5-mini (OpenAI, 2025b), Claude-3.7-Sonnet (Anthropic, 2025a), and Claude-Sonnet-4 (Anthropic, 2025b), offering different cost / performance tradeoffs. For ease or reading, we will refer to the last two models as Claude-3.7 and Claude-4. We use a 128K token limit for all models. All experiments run are capped at 50 steps, after which whatever is generated at this moment in the gistifed file is submitted for evaluation. On the data side, we experiment with widely used GitHub repositories which are present in SWE- Bench (requests, pylint, flask, scikit-learn, seaborn). We also explore an additional repository, debug-gym (Yuan et al., 2025)'. This library is relatively new and importantly does not overlap with SWE-Bench. We extract and filter test sets for each repository. Namely, we remove tests whose execution is dependent on the tests file location. For the main experiment, we evaluate over 25 tests for each of the 5 repositories. More details regarding the evaluation setup and prompt can be found in the Appendix A. 3.2 Results We begin by giving an overview of the main results presented in Table 1. We report results for our main evaluation protocol, where the model does not have access to execution tools (e.g. python and pytest commands), as well as the alternative. Examples of gistified files are in Appendix B.1. Strong models and frameworks still struggle with Gistify task. Across models and execution frameworks, performance remains limited: even the strongest model with strong framework (Copilot with Claude-4) achieves 58.7% average Execution Fidelity, a binary success/fail metric, indicating that reliably producing a correct gistified file is still challenging. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets (Section 4.2), suggesting that the benchmark can scale in difficulty and will remain a meaningful target as future models strengthen and require more challenging evaluations. Different model families exhibit distinct strengths. Claude-4 achieves the highest Line Existence scores, indicating that it most faithfully extracts relevant code from the original codebase. In contrast, GPT-5 produces the most concise outputs, with a substantially higher Line Execution rate than other models. We We provide a link to all the GitHub repositories used in this work in Table 4. ==== Page 5 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Table 1: Average Performance over three agentic frameworks with four models. We evaluated over 25 tests over 5 repositories. Execution Fidelity is shown as w/o exec, and w execution tools. Line Existence and Execution are average across the two settings for clarity. Framework Model Execution Fidelity Line Existence Line Execution (wo exec / w. exec) GPT-5-mini 17.1 / 24.0 44.9 61.2 ws GPT-5 51.0 / 54.0 56.8 83.1 mini-SWE-agent Oy de-3.7 38.7 / 43.3 66.0 69.2 Claude-4 54.0 / 55.3 67.0 75.7 GPT-5-mini 30.9 / 45.3 47.9 74.8 GPT-5 30.7 / 46.0 48.3 81.7 SWE-agent Claude-3.7 40.7 / 46.0 66.8 69.9 Claude-4 56.7 / 57.3 66.3 72.9 GPT-5-mini 58.0 / 55.3 62.4 77.8 Copilot GPT-5 58.7 / 60.7 66.9 81.4 P Claude-3.7 43.3 / 56.0 63.0 74.4 Claude-4 58.7 / 61.3 69.6 80.3 observe a similar trend for GPT-5-mini and Claude-3.7: in general, GPT models achieve higher Line Existence, whereas Claude models achieve higher Line Execution. Small(er) models perform well with scaffolding. We note that GPT-5-minis performance varies significantly across different evaluation settings, from 17% in a bash-only setup to 58% when provided with a large inventory of tools from the Copilot framework (see Appendix B.3 for a full list). We note that this performance increase is also reflected in the quality of the generated gist, where we see a notable increase in line existence and line execution. Frontier models (GPT-5 / Claude-4) are strong bash users. When looking at performance on mini-swe-agent, where the models only have access to a bash terminal to solve the task, both models perform relatively well, solving over half of the tasks. Importantly, this is not the case for smaller and previous-generation models. Execution tools are not a silver bullet. Overall, when comparing performance with and without execution in Table 1, we note that in most cases we observe only a small performance gain. We expected that current coding LLMs could better leverage execution tools: indeed, using tools specifically for runtime execution analysis, such as a debugger, could significantly help solving a gistify task. However, we are not seeing this behavior emerge, even from frontier models. We observed a sharp decrease in performance for the GPT-5 model when evaluated on SWE-Agent without execution tools. We performed a visual inspection and noticed formatting issues when rewriting the input test function. A detailled discussion can be found in Appendix B.2. 3.3. Error Analysis Over Execution Failure We proceed with an analysis of the underlying failure causes, in order to understand which aspect of the GISTIFY task different models struggle with. Table 2 shows that each model tends to fail for different reasons. See Appendix B.4 for detailed examples of each error case. Import Error occurs when the model incorrectly imports the original codebase (e.g., import requests) instead of inlining the required modules into the gistified file. We note that this error occurs even as coding LLMs are explicitly prompted not to import the specific packages in question. Perhaps surprisingly, the best performing model, Claude-4, commits this seemingly innocuous error the most out of all four models. File Creation Failure errors arise when the model fails to generate the gistified file. This can happen in two ways: the model exceeds the maximum step limit, or the model terminates the task without any file being generated. ==== Page 6 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Table 2: Average error rates (%) of different failure reasons when running SWE-agent across models. Error cases are categorized into four groups. The numbers in parentheses indicate the number of errors for each category. Models | Import Error File Creation Failure Missing Test Function Pytest Runtime Error GPT-5-mini 1 (2) 11.3 (11) 76.3 (72) 3 (10) GPT-5 2 (4) 10.4 (8) 7 9 (60) 5 (5) Claude-Sonnet-3.7 20 0 (10) 20.0 (10) 0 (1) 58 0 (29) Claude-Sonnet-4 32.5 (13) 10.0 (4) 5 (3) 50.0 (20) Missing Test Function errors occur when the generated gistified file does not contain the function implemen- tation for the test specified in the given command, or implements the test in a different structure. This can happen when the model strips out the content of the test and executes it outside of the pytest wrapper, under e.g. if __name__ == __main__:. Claude models tend to avoid this mistake, while this is the main source of error for GPT-5 models, specifically under the SWE-agent framework. Importantly, we observe that this error does not happen at random, but rather alongside other execution errors; we attempted to add the missing test function, and it in most cases the test fails to run, i.e. it results in a runtime error. This aligns with the analysis in the next section, showing a strong correlation between the tasks success and the fidelity between the original and the generated tests. Pytest Runtime Error occurs when the execution of the generated file fails, either due to a runtime error or because the gistified output does not match the output from the original codebase. The results indicate this is the most common cause of error for the best performing model, Claude-4. 3.4 Importance of Faithfully Preserving the Test Function We observe that models frequently modify the test function, despite being provided with explicit instructions to copy without modification, except for unavoidable adjustments (e.g., removing imports). Again, to ensure consistent evaluation, we replace the test function in the gistified file with the original version before evaluation. To measure such modifications, we define the Test F, Score as the line-level overlap between the test code of the original file and the gistified version. High Test F, Score indicates that the model has successfully identified and copied the correct test function to the gistified file. We observe a strong correlation between Test F Score and execution fidelity (correlation=0.76, p=0.01); test instances with higher F, scores are substantially more likely to produce a successful gistified file. We hypothesize that this arises because in the GISTIFY task, models often reason backwards from the test file, thereby if the model fails from identifying or copying the test function, the subsequent reasoning process is highly likely to fail. To better understand the impact of the first stepsearching, viewing, and copying the test functionwe conduct an ablation study where we remove potential failure at this stage. Specifically, we explicitly provide the correct test function body and signature in the prompt, so the model no longer needs to locate or copy it. This isolates the effect of errors in identifying the test function. In this setting, we observe that Test F, Score improves highly from the base GISTIFY 68.4 to 85.3, along with execution fidelity (from 42.0% to 60.0%). This suggests that accurately handling the test function is a critical first step to do the Gistify task successfully. Detailed results are in Appendix B.5. 4 Analysis In this section, we analyze how different strategies and tools affect performance on the GISTIFY task, identify factors that contribute to its difficulty, and experiment with the use of a static coding LLM to gain a deeper understanding of the task. For all experiments, we evaluate 50 test instances drawn from the pylint codebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired with Claude-Sonnet-4. ==== Page 7 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Table 3: Analysis of the effect of different strategies and tools (global information, execution) on the GISTIFY task. We evaluate SWE-Agent with Claude 4 using 50 test instances from the pylint codebase. Max Steps Reached (%) indicates the percentage of runs that terminated because the maximum step limit was reached. Ablation Type Execution Fidelity Line Existence Line Execution | Max Steps Reached (%) Base GISTIFY 42.0 65.0 58.3 14.6 Execution (Tool) Edit And Execute 50 m3 2 10.0 4.1 Effect of Various Strategies and Tools In this section, we analyze how different strategies and sources of information affect model performance. We begin with the simplest approach, modifying the prompt to guide the model (Prompt-Based Guidance), and then move to more explicit approaches that rely on additional tools: providing global context (Global Information via Tools) or feedback from code execution (Execution-Based Tools). Detailed descriptions of prompts and tools, along with examples, are provided in the Appendix C.1. Prompt-Based Guidance We first begin with the simplest approach: modifying the prompt to provide explicit task guidance. We experiment in two settings. In the former, we prompt the model to perform step-by-step reasoning, by first predicting the execution traces and then going over them, adding relevant code snippets along the way (tracing). In the latter, a similar approach is used, with explicit instructions on how to recursively determine the execution traces: starting from the test, identify the relevant components and read the files where they are defined, and repeat until the end (reading). As shown in Table 3, we observe that adding such strategies tends to enhance overall metrics, giving both better execution fidelity and more faithful code extractions, as measured by line existence. Global Information via Tools Building on the above observation, we next assess the effect of explicitly providing global context through external tools, rather than predicting it. We examine two tools: (1) RepoGraph (Ouyang et al., 2024), which constructs a graph of the codebase where each node represents a line of code and edges capture connections between lines, enabling graph-based search over the entire codebase; and (2) a Tracing tool that exposes gold execution traces obtained from running the given test command. Results in Table 3 show that both tools improve performance, with the Tracing tool yielding the largest gains. This finding suggests that access to the global context, especially the gold tracing information, substantially strengthens the models ability to perform runtime reasoning, as it can easily identify which file to look at. Execution-Based Tools In Section 3.2, we saw that enabling execution tools resulted in small but consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. For this experiment we compare Bash access with a simple method that executes and prints the output of the gistified file whenever it is edited (Edit And Execute). No other execution tools are available to the agent, including runtime information about the ground truth test. The results are surprising: having access to fewer tools actually increases performance. Indeed, we note that when give access to a full set of bash commands, the coding LLM tends to explore more tools, increasing the overall trajectory length, and potentially reaching the maximum step limit. 4.2 Tests with High Coverage are Harder to Gistify In this section, we investigate what properties makes a given test hard to GISTIFY. We hypothesize that tests generating a longer and more complex execution trace would entail a harder task for the coding LLM. To this end, we investigate how two axes to measure a runtime executions difficulty affect performance: the length of trace, as measured by the number of function calls executed, and the number of unique files touched by ==== Page 8 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Performance according to Exec. Trace Difficulty (@@i staticcoding LLM mlm SWE-Agent 100 Trace Length Mm mini-SWE-Agent M8 Copilot ~ e Number of Files Covered iS 80; 2 80  S 60 60} Le w c Y 2 o 5 40 A 40} U Li) x m 20 20; 90> 10% 0% oo? > () oaaee TT oO: o- oO: oO: oO: Execution Line Line Binned Test Quantiles according to difficulty metric Fidelity Existence Execution (a) Difficulty of the Gistify task is measured as a function (b) Performance of a static coding LLM and various agentic of the execution trace difficulty of the underlying test. coding LLMs (mini-SWE-Agent, SWE-Agnet, Copilot). the tracing procedure. While these metrics correlate with one another, they will differ when, for example, a function is looped over many times or when the location of the relevant functions is in a single file versus across multiple files. For this experiment, we use again the same configuration as prior analysis, namely Claude-4 with 50 tests sampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given GISTIFY task, and how complex the execution traces are, according to both metrics considered. We leverage this insight to create a Gistify-hard subset, where we select the 30 most difficult examples according to each. We end up with 57 unique datapoints (30 from pylint, 28 from sklearn, 6 from seaborn). On this subset, performance drops to 21%, as compared to 48%, the baseline weighted performance average following the same distribution over repositories. Overall, this selection criteria offers a promising direction for designing challenging evaluation scenarios with GISTIFY. 4.3. Static Coding LLM In this section, we experiment over how models perform in a static setup, where they have no access to tools and cannot iterate on the generated solution. As such static coding LLMs do not have tools, they cannot search or view files dynamically. Thereby, to measure a possible upper bound for non-agentic approaches, we provide as input all files that were accessed during the original program execution (gold files). Also, as they cannot iterate over multiple steps, they have to output everything at once and are therefore restricted by the context window of the LLM. Since solving the GISTIFY task involves touching multiple files, we observe in many cases that the inputs exceed the models maximum sequence length. Thus, we sample a subset of test examples where the combined content fits within the 128K token limit of the LLM. As shown in Figure 2b, agentic models outperform static ones even when the latter receive all relevant files. This suggests that selecting files dynamically over multiple iterations is more effective than providing everything at once, which can overwhelm the model. However, interestingly, the static coding LLM setup achieves the highest Line Existence score. This is likely because the model can copy lines directly from input, yet it performs worse on Line Execution and Execution Fidelity, suggesting that models do not have a good understanding of the codebase, often copying lines that are incomplete or incorrect. *See Appendix C.2 for detailed statistics on the usage of various tools. ==== Page 9 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution 5 Related Works 5.1 Codebase-level Understanding Benchmark Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding. These generally fall into three categories: question answering, code synthesis, and mapping natural language specifications to the entire codebase. Several benchmarks introduce codebase-level question-answering (Strich et al., 2024; Li et al., 2024b; Sahu et al., 2024; Chen et al., 2025; Hu et al., 2024; Fu et al., 2025). In these settings, the model must correctly answer questions that require an understanding of the codebase. The questions are drawn from various sources, including real-world GitHub issues and queries resembling those asked of tools like Copilot. Another line of work evaluates whether models can synthesize code by leveraging information distributed across multiple files in the codebase (Zhang et al., 2023; Liu et al., 2023b; Ding et al., 2023; Li et al., 2024a; Yu et al., 2024). These benchmarks include tasks such as retrieval-augmented completion, cross-file refactoring, and more specialized settings such as sketch-based coding or codebase evolution. Moreover, there is a line of benchmark that maps natural language specifications to entire code repositories, leveraging hierarchical or multi-stage representations to capture inter-file relationships and maintain consistency across a codebase (Tang et al., 2023; Zan et al., 2024; Ni et al., 2025). Our work tackles a more complex setting, where models must reason over full execution traces and examine multiple files, making the task challenging, and even widely used agentic models struggle alongside static ones. There are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies while preserving executability (Xie et al., 2025b; Jain et al., 2024). This sandboxing step is similar to GISTIFY in that it tries to construct a simplified file that has the feature extracted. However, the sandboxing step is done programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast, our work focuses on generating a simplified file using an LLM, thereby evaluating the models ability to reason about both code dependencies and runtime behavior. Notably, while prior work acknowledges cases in which static programmatic sandboxing fails (e.g., when functions have large dependency slices) and discards those examples, we consider them informative because they require reasoning about more complex runtime behavior. We further observe that these instances also present challenging examples for the GISTIFY task. 5.2 Methods for Codebase-Level Understanding Recent work on autonomous agents for codebase-level code understanding has focused on improving code navigation, reasoning, and generation through structured representations and planning. Approaches leverage structural information of code for function-call graphs, module-dependency graphs, and hierarchical code structures to provide models with core components of repositories (Wang et al., 2025; Liu et al., 2024). Another line of work integrate multi-step reasoning and state update policies to enable more effective planning over complex tasks (Bairi et al., 2024; Gautam et al., 2025). Additional methods combine various agents with multiple tools to streamline codebase-level exploration and task solving (Luo et al., 2024; Zhang et al., 2023; Shrivastava et al., 2023; Wang et al., 2024; Yang et al., 2024; Tang et al., 2023; aider, 2025; Microsoft, 2025; cursor, 2025). 5.3. Runtime Execution Various works have introduced benchmarks to evaluate LLMs ability to reason over code execution at runtime (Gu et al., 2024; Chen et al., 2024; Xie et al., 2025a; Beger & Dutta, 2025; Hu et al., 2025). These benchmarks typically test whether models can predict execution traces or intermediate states such as variable values, control flow, or data dependenciesgiven code and inputs, or alternatively, infer inputs from code and outputs. Some benchmarks further extend this paradigm by leveraging execution traces to construct new problems through program composition, thereby varying complexity in a principled way. Beyond evaluation, execution traces have also been incorporated into training pipelines to strengthen models runtime reasoning abilities (Liu et al., 2023a; Ding et al., 2024). By augmenting pre-training and fine-tuning with execution states, paths, and coverage signals, these methods help models capture program dynamics and generalize to execution-aware tasks. At inference time, several frameworks leverage runtime feedback to iteratively guide models in debugging or completing partial programs, thereby improving performance on execution-driven ==== Page 10 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution tasks (Zhong et al., 2024; Xue et al., 2024). In this work, we extend prior approaches by going beyond reasoning over execution traces to also reformulate programs; the model not only tracks execution but also identifies how to compress and organize code into a concise, coherent file. We further show that this capability serves as a useful tool at inference time, helping models better structure and complete execution-driven tasks. 6 Discussion and Conclusion In this paper, we introduced the GISTIFY task in which a coding LLM extracts a specific funtionality of a codebase into a single, self-contained file. Beyond serving as a standalone evaluation task that is easily applicable to arbitrary repositories and execution commands, the gistified file itself also opens several promising directions for research and practical applications. Large codebases often overwhelm automated agents due to their complex dependencies, and they especially struggle when tasked with fixing bugs that span multiple files (Ganhotra, 2025). In such scenarios, a gistified file would greatly reduce this challenge, and enable a more efficient reasoning about the codebase without navigating through unrelated code. In other words, this file could be leveraged in other downstream tasks, such as code refactoring or debugging, or even as a way to extract and share a minimal implementation of a specific codebase functionality. In summary, with coding LLMs increasingly being deployed in real-world software development, the need for automatically constructing evaluation setups that require codebase-level understanding of arbitrary repositories is growing. Through extensive experiments across a range of models and frameworks, we found that state-of-the-art LLMs still face challenges on the GISTIFY task, especially when faced with long, complex execution traces. Our analysis shows that incorporating global code context or execution-aware tools improves performance, and agentic coding LLM tend to handle the task more effectively by reasoning about which files to inspect using various tools. Beyond serving as a benchmark, the gistified files themselves are valuable artifacts. They distill the essential functionality of complex systems into a compact, executable form, making them easier to inspect and understand. Such files could support a range of practical applications, including debugging, refactoring, and code review, which we leave for future work. 10 ==== Page 11 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution References aider. Ai pair programming in your terminal. 2025. URL https://github.com/Aider-Al/aider?tab= readme-ov-file. Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for Ilms. arXiv preprint arXiv:2410.06992, 2024. Anthropic. Claude sonnet 3.7. https://www.anthropic.com/news/claude-3-7-sonnet, 2025a. Hybrid reasoning model; accessed: 2025-09-25. Anthropic. Claude sonnet 4. https://www.anthropic.com/claude/sonnet, 2025b. Improved version over Sonnet 3.7; accessed: 2025-09-25. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):[REDACTED], 2024. Claas Beger and Saikat Dutta. Coconut: Structural code understanding does not fall out of a tree. In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. [REDACTED]. IEEE, 2025. Jialiang Chen, Kaifa Zhao, Jie Liu, Chao Peng, Jierui Liu, Hang Zhu, Pengfei Gao, Ping Yang, and Shuiguang Deng. Coreqa: uncovering potentials of language models in code repository question answering. arXiv preprint arXtv:2501.08447, 2025. Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. Reasoning runtime behavior of a program with llm: How far are we? arXiv preprint arXtv:2403.16437, 2024. cursor. cursor. 2025. URL https://cursor.com/. Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36:4670146723, 2023. Yangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. Traced: Execution- aware pre-training for source code. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1-12, 2024. Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, et al. Corecodebench: A configurable multi-scenario repository-level benchmark. arXiv preprint arXtv:2507.05281, 2025. Jatin Ganhotra. Do swe-agents solve multi-file issues like humans? a deep dive into swe- bench verified, January 2025. URL https://jatinganhotra.dev/blog/swe-agents/2025/01/05/ swe-bench-mutliple-files/. Blog post. Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refac- torbench: Evaluating stateful reasoning in language agents through code. arXiv preprint arXiv:2503.07832, 2025. Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: A benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 16568-16621. PMLR, 21-27 Jul 2024. Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and Cuiyun Gao. Coderepoga: A large-scale benchmark for software engineering question answering. arXiv preprint arXtv:2412.14764, 2024. 11 ==== Page 12 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, and Kaidi Xu. Dynacode: A dynamic complexity-aware code benchmark for evaluating large language models in code generation. arXiv preprint arXiv:2503. 10452, 2025. Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: Turning any github repository into a programming agent environment. In ICML, 2024. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXtv:2310.06770, 2023. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024a. Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. Infibench: Evaluating the question-answering capabilities of code large language models. Advances in Neural Information Processing Systems, 37:[REDACTED], 2024b. Shanchao Liang, Spandan Garg, and Roshanak Zilouchian Moghaddam. The swe-bench illusion: When state-of-the-art lms remember instead of reason. arXiv preprint arXiv:2506.12286, 2025. Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, and Nan Duan. Code execution with pre-trained language models. In Anna Rogers, Jordan Boyd- Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. [REDACTED], Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.308. URL https: //aclanthology.org/2023.findings-acl .308/. Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto- completion systems. arXiv preprint arXiv:2306.[REDACTED], 2023b. Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXtv:2408.03910, 2024. Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An Ilm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402. 16667, 2024. Microsoft. Github copilot in vs code. 2025. URL https://code.visualstudio.com/docs/copilot/ overview. Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, et al. Gittaskbench: A benchmark for code agents solving real-world tasks through code repository leveraging. arXiv preprint arXiv:2508. 18993, 2025. OpenAI. Gpt-5 technical overview. https://platform. openai.com/docs, 2025a. Accessed: 2025-09-25. OpenAI. Gpt-5 mini. https: //platform.openai.com/docs/models/gpt-5-mini, 2025b. Compact variant of GPT-5; accessed: 2025-09-25. Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXtv:2410.14684, 2024. Surya Prakash Sahu, Madhurima Mandal, Shikhar Bharadwaj, Aditya Kanade, Petros Maniatis, and Shirish Shevade. Codequeries: A dataset of semantic queries over code. In Proceedings of the 17th Innovations in Software Engineering Conference, pp. 1-11, 2024. 12 ==== Page 13 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Disha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2806.10998, 2023. Jan Strich, Florian Schneider, Irina Nikishina, and Chris Biemann. On improving repository-level code QA for large language models. In Xiyan Fu and Eve Fleisig (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. [REDACTED], Bangkok, Thailand, August 2024. Association for Computational Linguistics. ISBN [REDACTED]. doi: 10.18653/v1/2024.acl-srw.28. Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, et al. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. arXiv preprint arXiv:2311.09885, 2023. Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, et al. Repomaster: Autonomous exploration and understanding of github repositories for complex task solving. arXiv preprint arXiv:2505.21577, 2025. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXtv:2407.16741, 2024. Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, and Xiangyu Zhang. Core: Benchmarking Ilms code reasoning capabilities through static analysis tasks. arXiv preprint arXtv:2507.05269, 2025a. Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. Repost: Scalable repository-level coding environment construction with sandbox testing. Conference on Language Modeling, 2025b. Zhipeng Xue, Zhipeng Gao, Shaohua Wang, Xing Hu, Xin Xia, and Shanping Li. Selfpico: Self-guided partial code execution with llms. In Proceedings of the 83rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. [REDACTED], 2024. John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1-12, 2024. Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, et al. debug-gym: A text-based environment for interactive debugging. arXiv preprint arXiv:2503.21557, 2025. Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, et al. Codes: Natural language to code repository via multi-layer sketch. arXiv preprint arXtvu:2403.16443, 2024. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXtv:2308.12570, 2023. Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXtv:2402. 16906, 2024. 13 ==== Page 14 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution A_ Experimental Setting A.1 Metrics Execution Fidelity Execution fidelity measures whether the generated gistified file reproduces the same functional behavior as the original codebase under the given command. This includes producing the same number of test passes or failures, as well as consistent outputs and error handling. If the files behavior matches the original codebase, it is assigned 100%; otherwise, it receives 0%. Line Execution Rate The line execution rate measures the proportion of lines in the gistified file that are actually executed when running it under the given command. We first analyze the gistified file to identify which lines are executable (e.g., imports, function or class definitions) versus not-executable (e.g., comments). Using a tracing function, we then determine which of the executable lines are touched during execution. The line execution rate is computed as the fraction of executable lines that are executed. A rate of 100% indicates that the gistified file is concise and contains primarily necessary lines that are executed, while 0% indicates that non of the executable lines were touched. When calculating line execution rate, we exclude the tests where the self-containment is 0% as the goal of line execution rate is to evaluate the models ability to construct a concise, executable file, not to penalize failures in generating runnable code. We classify each line of code into three categories: executable, potentially executable, and non-executable. Executable lines include imports and functional code that can be directly run. Potentially executable lines are those that may or may not be executed during a run, such as the except block of a try-except statement or placeholders for classes and function definitions. Non-executable lines, such as comments, are those that have no effect on execution. To calculate the line execution rate, we first classify each line in the gistified file and then consider only the executable lines. Non-executable lines are ignored since their presence or absence does not affect execution outcomes, and potentially executable lines are excluded because they are often ambiguous (e.g., placeholders) and cannot be reliably judged as necessary or removable. Line Existence Rate The line existence rate measures the proportion of lines in the gistified file that are directly preserved from the original codebase. We first parse both the gistified file and the original codebase into blocks, where each block corresponds to a class or function. Within classes, functions are nested under their parent class, forming a hierarchy. Lines outside of any block (e.g., top-level statements) are treated as standalone units. For each block in the gistified file, we locate the corresponding block in the original codebase using its name and hierarchical position. If a matching block exists, we compare the two line by line to determine which lines are preserved; whether the lines in the gistified block appear in the corresponding original block. If no match is found, all lines in that block are treated as non-existent. For lines outside any block, existence is determined by direct comparison with top-level lines in the original codebase. An existence rate of 100% indicates perfect preservation of the original code without hallucinated content. Normalization for Line-wise Code Matching When checking the existence of a code line within a file, as our objective is to determine semantic equivalence rather than strict syntactical identity, we do normalization; code that is functionally identical may differ in formatting, such as multiline statements, indentations, or space, which can hinder direct line-wise comparison. To address this, we normalize each code block before performing line-wise matching. Specifically, we parse the code into an Abstract Syntax Tree (AST) and ignore comments; split combined import statements into individual imports; merge statements that span multiple lines into a single line; remove inline comments (e.g., for i in range(5): # comment); and eliminate indentation and redundant spaces. These normalizations ensure robustness by making the comparison focus on the codes underlying structure and functionality rather than superficial formatting differences. A.2. Framework We evaluate experiments with three agentic frameworks: mini-SWE-Agent (Yang et al., 2024), SWE Agent (Yang et al., 2024), and Copilot (Microsoft, 2025). Unless otherwise noted, all experiments are run in the default GIsTIFY setup, where the model is restricted from executing any commands (e.g., python, pytest). 14 ==== Page 15 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution SWE-Agent and Copilot Agent enable LLMs to interact with a codebase through a suite of tools, including bash commands. These tools support capabilities such as viewing, searching, editing, and creating files or directories. In addition, Copilot Agent extends this functionality with browser integration, explicit reasoning, and API usage. mini-SWE-agent is a simplified variant of SWE-Agent that only supports bash commands. Despite its minimal design, it achieves strong performance on the SWE-Bench Verified benchmark (Jimenez et al., 2023). For both mini-SWE-Agent and SWE-Agent, we set the maximum number of steps to 50 and run them in the same Docker environment, using the current version of the repositories. A.3 Experimental Test Set Construction Table 4: Details of the GitHub repositories used as the test set. Repository URL License flask https: //github.com/pallets/flask BSD 3-Clause requests https://github.com/psf/requests Apache-2.0 pylint https: //github.com/pylint-dev/pylint GPL 2.0 scikit-learn | https://github.com/scikit-learn/scikit-learn BSD 3-Clause seaborn https: //github.com/mwaskom/seaborn BSD 3-Clause debug-gym https://github.com/microsoft/debug- gym MIT Table 4 summarizes the repositories used in our evaluation. For each repository, we begin by extracting all available test cases, including parameterized ones. For experimental test runs, we group tests that share the same base structure but differ only in parameterization, treating them as a single test. During evaluation, however, we execute all parameterized instances and measure how many are passed, thereby assessing execution fidelity. Finally, we filter out environment-dependent tests, such as those requiring relative file paths or fixed module locations. In the main experiments, we used 25 test instances for each of the five codebases, and the analysis was conducted using 50 test instances from the pylint codebase. A.4 Prompt for Gistify Figure 3 shows the prompt used in the main experiments. A.5_ Providing specific parameters to commands tends to make models generate parameter-specific gistified files We observe that when specific command-line parameters are provided, models often adapt the generated gistified file to those parameters rather than producing a fully general solution. Examples of this parameter- specific behavior are shown in Figures 4 and 5. Accordingly, in our experiments, we group test cases based on the parameters provided to the command. B~ Results B.1 Example of gistified file Figure 6 and Figure 7 show two gistified files on the same test case with different models; each model succeed or fail in generating a gistified file with execution fidelity of 100% and 0%, respectively. In the successful case (Figure 6), the generated file handles both parameters correctly, achieving a 100% line existence rate, a 65.5% execution rate, and a test F score of 100. In contrast, the failed case (Figure 7) cannot execute due to a missing import pytest statement. Moreover, the hallucinated test function yields a test F; score of 0, and the file shows a much lower line existence rate of 28%. 3We adopt this grouping design as we observe that models often overfit to specific values when parameters are provided. See Appendix A.5 for more details. 15 ==== Page 16 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Prompt for GISTIFY Ive uploaded a python code repository in the directory {working dir}. There is an original test invocation (the command that reproduces behavior we want to preserve): problem statement Your job: create a single file named concise.py saved at {working dir} that is **self-contained**, **minimal**, and **executable**, and when used in place of the original test run reproduces the same runtime behavior and outputs. Follow the instructions below when creating the file. OUTPUT - Produce one file only: {working dir} /concise.py. - The assistant must return only the contents of concise.py (no extra files, no analysis, no commentary). HIGH-LEVEL RULES for creating concise.py 1. Inline internal dependencies * Copy into concise.py every function, class, or top-level code from the files inside {working dir} that is executed when running {problem statement}. * Do not use import statements for modules defined in {working dir}. 2. Remove unexecuted lines * When copying lines in concise.py, keep only the lines that is actually executed when running {problem statement}. * Delete unused functions, classes, variables, if-else, imports, and unreachable branches. * Ensure the file remains syntactically correct and minimal after removal. 3. Preserve original source lines * Do not rewrite or reformat lines unless necessary to keep the files valid. * Do not arbitrary generate new lines that do not exist in the original {working dir} files. * You may adjust indentation, remove empty else blocks, or adapt try-except structures only when required to preserve correctness. 4. Keep external imports * Leave imports to external libraries, frameworks, or standard runtime libraries unchanged. * Only remove or inline dependencies that come from {working dir}. 5. No shortcuts or cheating * Do not stub, fake, or monkey-patch external modules. * Do not reimplement or newly add third-party libraries. * Do not hard-code outputs * Do not replace test logic with simplified equivalents 6. Preserve test behavior * The test function much remain unchanged, except for import adjustments needed to reference inlined code. * The output, exceptions, or exit codes must match the original run of {problem statement}. 7. Do not execute the code * Do not run or simulate the program (e.g., with pytest, python, or any other tools) Figure 3: Base Prompt Template for GISTIFY Task. B.2 Error analysis over execution failure We categorize errors into four types: Import Error Figure 8 shows an example of Import Error. This occurs when the model incorrectly imports the original repository (e.g., import requests) instead of inlining the required modules into the gistified file. File Creation Failure This error arises when the model fails to generate the gistified file. This can happen in two ways: (1) the model exceeds the maximum step limit or (2) the model completes within the time limit but still fails to generate the new file using the tool. 16 ==== Page 17 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution @pytest.mark.parametrize ( "value ,, expected",  ("application/xml", ("application/xml", {})),  application/json,;,charset=utf-8", ("application/json", {"charset": "utf-8"}), ); ("text/plain", ("text/plain", {})), def test__parse_content_type_header(value, expected): assert _parse_content_type_header(value) == expected (a) Original Test Case def test__parse_content_type_header(): """Test for ,the,_parse_content_type_header,,function,with,application/json,jand,,charset=utf-8""" value = "application/json,;,charset=utf-8" expected = ("application/json", {"charset": "utf-8"}) assert _parse_content_type_header(value) == expected (b) Gistified File Figure 4: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter. @pytest.mark.parametrize ( "url, expected",  ("http ://192.168.0.1:5000/", True), ("http://google.com:5000/v1.0/", False), ); ) def test_should_bypass_proxies_no_proxy(url, expected, monkeypatch): """Tests for function,should_bypass_proxies,to,check,if proxy uuUUCcCanybe,bypassed,or, not using, ,the,,no_proxy, argument wun UuuuuU no_proxy = "192.168.0.0/24,127.0.0.1,localhost.localdomain ,172.16.1.1" # Test no_proxy argument assert should_bypass_proxies(url, no_proxy=no_proxy) == expected (a) Original Test Case def test_should_bypass_proxies_no_proxy(url, expected, monkeypatch): """Tests for, function,should_bypass_proxies,to,check,if proxy uuUUCcCanybe,bypassed,or, not using, ,the,,no_proxy, argument wun UuuuuU no_proxy = "192.168.0.0/24,127.0.0.1,localhost.localdomain ,172.16.1.1" # Test no_proxy argument assert should_bypass_proxies(url, no_proxy=no_proxy) == expected (b) Gistified File Figure 5: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter. Missing Test Function This occurs when the generated gistified file does not contain the modules for specified test in the given command. It typically arises when the model fails to locate or copy the modules necessary for the test into the gistified file. Conceptually, this corresponds to a 0% line existence rate for the test function. Since the presence of the modules for the given test case is essential for validation, we classify this as an error. We also observe an interesting behavior of GPT-5 where it tends to insert __name__ == main__" even though it is not provided in the original codebase and even though it is explicitly mentioned that we will test on the provided command and expect the same output. They often remove the test function but move the lines in the test function under the "__main__" guard (e.g., Figure 10). We hypothesize that this may be because they are more familiar with codebases following this pattern. We also observe cases where the model 17 ==== Page 18 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html # For details: https://github.com/pylint -dev/pylint/blob/main/LICENSE # Copyright (c) https://github.com/pylint -dev/pylint/blob/main/CONTRIBUTORS. txt from __future__ import annotations import os from collections.abc import Sequence from typing import Any import pytest def discover_package_path(modulepath: str, source_roots: Sequence[str]) -> str: """Discover,package,path,from,one,its,modules,and,source,roots.""" dirname = os.path.realpath(os.path. expanduser (modulepath) ) if not os.path.isdir (dirname): dirname = os.path.dirname (dirname) # Look for a source root that contains the module directory for source_root in source_roots: source_root = os.path.realpath(os.path. expanduser(source_root)) if os.path.commonpath([source_root, dirname]) in [dirname, source_root]: return source_root # Fall back to legacy discovery by looking for __init__.py upwards as # its the only way given that source root was not found or was not provided while True: if not os.path.exists(os.path. join(dirname, return dirname old_dirname = dirname dirname = os.path.dirname (dirname) if old_dirname == dirname: return os.getcwd() " __init__.py")): @pytest.mark.parametrize ( "py_mod_base_name", ("__init__", "impl"), ids=("explicit-namespace", implicit-namespace"), def test_discover_package_path_source_root_as_parent ( py_mod_base_name: str, tmp_path: Any, ) -> None: """Testydiscover_package_ path when,source,rootyjis,a,parent,of ,the,module.""" # Create this temporary structure: # /tmp_path/ # project/ # my -package/ # __init__.py project_dir = tmp_path / "project" package_dir = project_dir / "mypackage" package _dir.mkdir(parents=True) (package_dir / f"{py_mod_base_name}.py").touch() # Test with project_dir as source root (parent of package) result = discover_package_path(str(package dir), [str(project_dir)]) assert result == str(project_dir) Figure 6: Example of a successful gistified file: the code correctly handles all parameters, achieving 100% line-existence rate, a test F, score of 100, and an execution rate of 65.5%. attempts to cheat the task by injecting a mock, in-memory version of the original codebase package to satisfy import dependencies, rather than copying the necessary code inline (e.g., Figure 12). Pytest Runtime Error This error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues (e.g., Figure 9). Although the absence of test functions is also one of pytest failures, we explicitly separate those cases by first verifying the presence of the required test functions and running pytest only when they exist. 18 ==== Page 19 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution import os import sys from contextlib import contextmanager from pathlib import Path def discover_package_path(path, _search_paths): nun uuuuMinimal,version:jacceptyja filesystem path, (string,or,Path) ,andjreturn,ja,Path uuuuto that directory. ,This,matches,,the,usage,jin,the,specific,test,whereya uuuUpath,to,the,tests,data,directory,is,passed. mun UuuuuU if isinstance(path, Path): p = path else: p = Path(path) if p.exists() and p.is_dir(): return p raise ValueError ("Cannot determine,package,path") @contextmanager def augmented_sys_path(paths): uuuuMinimal,context manager :,setysys.path,to,the,parent,directories,of the uuuuprovided,paths,(in,jorder) ,,restoring,the,original,sys.path,on,exit. uuu" nu original = list(sys.path) try: new = [] for p in paths: pth = Path(p) # Ensure we use the parent directory (the "source root" in the test) parent = pth.parent new.append(str (parent) ) sys.path[:] = new yield finally: sys.path[:] = original @pytest.mark.parametrize ( "py_mod_base_name", ("_linit__"), ids=("explicit-namespace"), def test_discover_package_path_source_root_as_parent(): # This tests behavior is preserved: it uses discover_package_path on the # tests data directory and ensures augmented_sys_path sets sys.path to its # parent (the project/tests directory). TEST_DATA_DIR = os.path.abspath(os.path. join(os.path.dirname(__file__), "tests", "data")) PROJECT_ROOT_DIR = os.path.abspath(os.path.join(TEST_DATA_DIR, "..")) with augmented_sys_path([discover_package_path(TEST_DATA_DIR, [])]): assert sys.path == [PROJECT_ROOT_DIR] Figure 7: Example of failed gistified file: the code fails to import pytest. The model hallucinates the function test_discover_package_path_source_root_as_parent(), resulting in a test F score of 0 and a low line-existence rate of 28.0% B.3 Tools Available in GitHub Copilot Table 5 shows the list of available tools in Github Copilot. B.4 Change Test even high performing models and frameworks (especially GPT-5 and GPT-5-mini) seems to modify test codes even though explicitly mentioned not to. We observed three common modification: (1) removing the test function but move the lines in the test function under the "__main__" guard (e.g., Figure 10), (2) adding the "__main__" guard even though unnecessary (e.g., Figure 11), and (3) mocking a minimal in-memory package to bypass missing dependencies and force the test to run (e.g., Figure 12). 19 ==== Page 20 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution @click.option("--all-methods", is_flag=True, help="Show,,HEAD,,and,,OPTIONS,,methods.") Qwith_appcontext def routes_command(sort, all_methods): """Show yall registered,routes,with,endpoints,and,methods.""" from flask import current_app rules = list (current_app.url_map.iter_rules()) if not rules: click.echo("No,routes,were,registered.") return Figure 8: Example of an Import Error: the gistified file imports from the original repository (e.g., from flask import current_app). T = t.TypeVar("T") class ConfigAttribute(t.Generic[T]): ""'"Makesyan yattribute,forward,to,the,config'"" def _init__( self, name: str, get_converter: t.Callable[[t.Any], T] | None = None ) -> None: self. __name__ = name self.get_converter = get_converter (a) Original Test Case class ConfigAttribute: def _init__( self, name: str, get_converter: t.Callable[[t.Any], T] | None = None ) -> None: self. __name__ = name self.get_converter = get_converter (b) Gistified File Figure 9: Example of an Pytest Runtime Error: gistified file fails with error message E TypeError: type >ConfigAttribute is not subscriptable B.5 Additional Metrics Table 6 shows the result of additional evaluation metrics, including the Average Pytest Pass Rate, which is defined as the average test pass rate over cases with at least one successful run, and the Test F, Score, which quantifies the line-wise F, existence between the test functions in the original codebase and those in the gistified fie. GPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully generate, they tend to pass all pytest. For the Test fF, Score, Claude-4 shows the highest performance, aliging with the trend discussed in Section 3.4. C_ Analysis C.1 Effect of various strategies and tools Prompt-Based Guidance We experiment with two variants of the prompt, Reading and Tracing, where, on top of the base prompt (Figure 3), we add specific instructions of How to Operate to encourage reasoning using a particular strategy. The addition prompt detail of Reading is in Figure 14, and for Tracing is in Figure 15. Global Information via Tools We experiment with two tools that provide global information: RepoGraph and Tracing. Details of the information provided to the model about each tool are shown in Figure 16. RepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level structure. It parses code at the line level, extracts relationships, and constructs a graph where each node represents a line of code and each edge encodes dependencies between code definitions and their references. 20 ==== Page 21 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Tool Description copilot_ getNotebookSummary Returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. Useful for getting cell IDs, execution order, and outputs. edit. notebook _ file Edit an existing Notebook file in the workspace. Supports inserting, deleting, or editing cells while preserving whitespace and indentation. apply_patch Edit text files using a special diff/patch format. notebooks. Do not use for Jupyter semantic search Run a natural language search for relevant code or documentation comments in the workspace. create_ directory Create a new directory structure in the workspace (like mkdir -p). create__ file Create a new file with specified content. Automatically creates directories if they do not exist. file search Search for files in the workspace by glob pattern (e.g., **/*.js). Returns matching paths only. test__search For a source file, find the corresponding test file, and vice versa. grep_search Fast text or regex search in the workspace. Useful for exact string or regex queries. run_notebook_ cell Run a code cell in a notebook file and return the output. Avoid running Markdown cells. read_notebook_cell_ output Retrieve the latest output for a notebook cell, even if not run in the current session. get__search_view_ results Returns results from the search view. github_ repo Search a GitHub repository for relevant code snippets. Use only for external repos, not local workspaces. insert. edit into file Insert or edit code in an existing file using minimal hints, avoiding duplication of unchanged code. install extension Install an extension in VS Code. Used only during workspace creation. list__dir List the contents of a directory (folders and files). create_new_jupyter_notebook Generate a new Jupyter Notebook (.ipynb) in VS Code. create_new_ workspace Set up a complete new project (scaffolding, dependencies, config, boilerplate). get__project_setup_ info Provides project setup information for a VS Code workspace after workspace creation. read_ file Read the contents of a file. Supports offsets and limits for large files. open_simple_ browser Preview or open a URL in VS Codes Simple Browser. test_ failure Include test failure information in the prompt. think Think deeply about a request and log structured reasoning (no execution). Useful for planning, debugging, and brainstorming. get__vscode_api Retrieve comprehensive VS Code API documentation and references for exten- sion development. run_vscode_ command Run a VS Code command by ID with arguments. Used mainly in workspace creation. fetch_webpage Fetch main content from a webpage for summarization or analysis. Table 5: Available tools and their descriptions. We note that many tools available to the agent are never used. Table 6: Average Pytest Pass Rate and Test F) Score of different models using SWE-Agent on the main table (Table 1) test dataset. Models | Execution Fidelity | Average Pytest Pass Rate Test Fy, Score GPT-5-mini GPT-5 Claude-3.7 Claude-4 30.9 49.2 47.9 30.7 88.8 45.0 40.7 61.9 55.9 56.7 72.2 60.0 21 ==== Page 22 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution class TestGetNetrcAuth: def test_works(self, tmp_path, monkeypatch): netrc_path = tmp_path / ".netrc" monkeypatch.setenv("NETRC", str(netrc_path) ) with open(netrc_path, "w") as f: f.write("machine,example.com,login,aaaa,password,,bbbb\n") auth = get_netrc_auth("http://example.com/thing") assert auth == ("aaaa", "bbbb") (a) Original Test Case if _ name _ == "__main__": # Reproduce tests/test_utils.py::TestGetNetrcAuth::test_works with tempfile.TemporaryDirectory() as tmpdir: netrc_path = os.path.join(tmpdir, ".netrc") os.environ["NETRC"] = netrc_path with open(netrc_path, "w") as f: f.write("machine,example.com,login,aaaa,password,,bbbb\n") auth = get_netrc_auth("http://example.com/thing") assert auth == ("aaaa", "bbbb") (b) Gistified File Figure 10: Test Modification Case 1: The test TestGetNetrcAuth.test_works is converted from a pytest unit test into a standalone script. # Test class and method - preserved unchanged class TestArgparseOptionsProviderMixin: """Tests for the,argparse,implementation,of,OptionsProviderMixIn. uuuuThe,logger,,checker,is,used,as,an,example,checker,for,this,implementation. wun UuuuuU @staticmethod def test_logger_without_options() -> None: """Check,thatwe,raise,messages,when,we,do,not supply ,any,options.""" with pytest.raises(SystemExit) as ex: Run ((LOGGING_TEST]) assert ex.value.code == 2 # Main execution for pytest if __name == " main__": test = TestArgparseOptionsProviderMixin() test.test_logger_without_options () Figure 11: Test Modification Case 2: Adding unnecessary "__main__" guard Thereby, when given a specific module, it returns the relationship with other modules as represented within the constructed graph. Tracing is a tool that uses the tracer provided from the sys module to execute a command and track which components of the codebase are accessed. When the model uses the tool with a specific command, the tool provides the model with the files and functions touched when running the command, in the order in which they are encountered. Execution-Based Tools We experiment with two execution-based tools: the Bash tool and the Edit and Execute tool. The Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. In contrast, the Edit and Execute tool is designed specifically for working with the gistified file: it enables the model to create or modify the gistified file and optionally execute it to verify changes. The primary difference between the two tools is their scope of execution. The Bash tool can run commands on both the original codebase and the gistified file, whereas the Edit and Execute tool is restricted to executing only the gistified file. We include an example of the behavior observed when adding the execution tool in Figure 17. Common patterns we observe are: (1) the model first runs the provided command to identify which files are accessed 22 ==== Page 23 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution # Create a minimal in-memory requests package with required submodules. requests_mod = types.ModuleType( requests) requests_mod.__path__ = [] compat_mod = types.ModuleType(requests.compat) structures_mod = types.ModuleType( requests. structures) # Populate compat with only whats needed by this test suite import paths. compat_mod.Mapping = Mapping compat_mod.MutableMapping = MutableMapping compat_mod.urljoin = urljoin # Populate structures with the classes. structures_mod.CaseInsensitiveDict = CaseInsensitiveDict structures_mod.LookupDict = LookupDict # Wire the package hierarchy and register in sys.modules. requests_mod.compat = compat_mod requests_mod.structures = structures_mod sys.modules[requests] = requests_mod sys.modules[requests.compat] = compat_mod sys.modules[requests. structures ] = structures_mod if _ name__ == __main__?: import pytest raise SystemExit (pytest.main([-q, tests/test_structures.py::TestCaseInsensitiveDict::test_list])) Figure 12: Test Modification Case 3: Manually mocking a minimal in-memory package to bypass missing dependencies and force the test to run. @pytest.mark.parametrize ( "value ,, expected",  (?foo="isya fish", bar="asywell", {"foo": isya fish", "bar": as ,well"}), ("key_without_value", {"key_without_value": None}), ); ) def test_parse_dict_header(value, expected): assert parse_dict_header(value) == expected (a) Original Test Case assert parse_dict_header(foo="isya,fish",,bar="as,well") == {"foo": "isya fish", "bar": "as ,well"} assert parse_dict_header("key_without_value") == {"key_without_value": None} (b) Gistified File Figure 13: The test function test_parse_dict_header is simplified: in the original, it used @pytest.mark.parametrize to feed multiple input/expected pairs into one function; in the gistified version, this is replaced with two direct assert statements, one per case. Table 7: Analysis of tool usage during the GISTIFY task Models Avg. tool usage | view search execute other GPT-5-mini 10.8 71.9 9.8 1.7 16.6 GPT-5 18.5 72.4 8.3 3.3 16.1 Claude-Sonnet-3.7 17.3 67.5 10.1 4.5 17.9 Claude-Sonnet-4 19.3 74.6 2.1 11.8 11.5 and to gather execution feedback; (2) after creating a file, it iteratively executes it to verify that the generated gistified file behaves as expected; and (3) it repeatedly compares the outputs of the gistified file and the original codebase under the given command. We also observe that, due to this iterative checking process, enabling the execution tool often leads the model to terminate because it reaches the maximum step limit. 23 ==== Page 24 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution Behavior Reading How to Operate: . Examine the test file and the test function used for {problem statement } . Identify which module used by these functions are defined in {working dir} . Copy and inline the code from those modules into concise.py . Check these modules for any internal functions or classes and inline them as needed. . Repeat this process recursively until all internal dependencies are inlined. . Do not forget to copy and paste external imports. Figure 14: Prompt for Reading strategy. Trace Reasoning How to Operate: 1. Predict the execution traces. 2. Follow the traces and inline (copy) only the necessary executed lines into concise.py 3. Repeat until all traces are fully handled. Figure 15: Prompt for Tracing strategy. C.2 Tool Usage Rates Table 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four categories: view, search, execute, and other, which includes all remaining tools. For all models, we compute usage rates both with and without execution enabled, and then average across the two settings. Among all models, Claude-4 exhibits the highest average tool usage for each test cases, followed by GPT-5, Claude-3.7, and GPT-5-mini. In terms of specific functionality, Claude-4 shows the highest rate of both view and execute tool usage, while Claude-3.7 shows the highest usage of the search tool. To generate a high-quality gistified file, a model must effectively view relevant files and copy only the necessary content. The strong performance of Claude-4 on line existence may be related to its high usage of the view tool. Also, the execution tool tends to support correctness verification of the generated file, which would lead to high execution fidelity. Tool Information * search repo (repograph): searches in the mentioned repository with a specific function or class, and returns the def and ref relations for the search term. * trace info (tracing): returns the trace log when running the pytest command for the test currently being debugged in the given repo Figure 16: Details of the information provided to the model about each tool. 24 ==== Page 25 ==== GIsTIFY! Codebase-Level Understanding via Runtime Execution "role": "assistant", "tool_calls": { "arguments": "{\"command\":\"cd./appy&& upython,-mypytesty, concise.py::TestDeprecationOptions::test_new_names,-v\"}", "name": "bash" } "role": "tool", "content": [ { "type": "text", "text": "OBSERVATION: ,test,session,starts,,...AssertionError\n...pylint: error: unrecognized, arguments:,,/app/regrtest_data/empty.py\n,,FAILED,, concise.py::TestDeprecationOptions::test_new_names,..." I, (fix concise.py based on observation) "role": "assistant", "tool_calls": { "arguments": "{\"command\":\"cd./appy&& upython,-mypytesty, tests/config/test_argparse_config.py::TestDeprecation0ptions::test_new_namesyj-vy-s\"}", "name": "bash" + }, { "role": "assistant", "content": "Excellent!,The,messages,are,identical.,Let,me,verify,my,implementation,removes,ally, unused,imports,and,jonly,includes, what s,needed:" 3 Figure 17: Trajectory when including execution tool. Models often iteratively execute the given codebase or generated gistified file to ensure that it operates expectedly. 25

==== Page 1 ==== 2510.26788v1 [cs.LG] 30 Oct 2025 arXiv Defeating the Training-Inference Mismatch via FP16 Penghui Qi''':?, Zichen Liu"'?, Xiangxin Zhou"', Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin 1Sea AI Lab ?National University of Singapore  https://github.com/sail-sg/Precision-RL Abstract Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dy- namic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to FP16 effectively eliminates this mismatch. The change is simple, fully supported by mod- ern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger per- formance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning. (a) Sanity GRPO (b) Sanity GRPO-Token-TIS (c) Sanity GRPO-Seq-MIS (d) Sanity GSPO 1. 1.0 1.0 1.0 08 oe 08 08 8 os os os o7 OF or OF  er| ,.  BF16 | os  BF16  BF16 os  FP16  FP16  pie | **  P16 oO [REDACTED] [REDACTED] oO [REDACTED] 2500  oO [REDACTED] 2500 oO 500 Loot [REDACTED] Training Steps Training Steps Training Steps Training Steps (e) Sanity PG-Seq-IS (f) Sanity PG-Seq-MIS on (g) OctoThinker GRPO (h) Lora GRPO-Token-TIS 10 1.0 09 06 08 08 oe os o7 08 oe oa 06 o oa os o7 os oa o6  sre | |.  B16 | ,,  BF16 | o3  BF16  FPl6  FPl6  FPl6 | oz  FPl6 os oo 3 sto [REDACTED] 0 soo 1000.~ 1 1+27-1 = 1.[REDACTED]. 1+277  1.[REDACTED] 3.2. Stabilizing FP16 Training with Loss Scaling The primary challenge with FP16s limited range is gradient underflow, which can be effectively solved early in the history of mixed-precision training with a technique called loss scaling [Micikevi- cius et al., 2017]. The procedure is straightforward: 1. The loss is multiplied by a large scaling factor S before backpropagation. 2. This scales up all gradients by S, shifting small gradient values out of the underflow region and into the representable range of FP16, thus preserving them. 3. Before updating the weights, the gradients are scaled back by dividing S. Modern implementations have further improved this with dynamic loss scaling. The scaling factor S is automatically adjusted during training, increased if no overflows (infinity values in gradients) are detected for a number of steps, and decreased immediately if an overflow occurs. ==== Page 6 ==== Table 2: Evaluation scores of DeepSeek-R 1-Distill-Qwen-1.5B using under different precisions (BF16, FP16 and FP32) and token budgets (8K and 32K). dtype AMC23 (8K) AIME24(8K) AMC23 (32K) AIME?24 (32K) BFI16 50.38 22.60 62.35 29.90 FP16 50.60 20.10 63.10 30.94 FP32 51.54 22.30 62.42 28.44 Crucially, these loss scaling techniques are standard, mature components in mainstream training frameworks (e.g., PyTorch [Paszke et al., 2019], Megatron [Shoeybi et al., 2019], DeepSpeed [Rasley et al., 2020]). Enabling them typically requires only a single configuration change or a few lines of code, making the adoption of FP16 training both simple and robust. 3.3. The Rise of BF16 in Modern LLM Training Despite the effectiveness of loss scaling, it complicates the system in distributed settings. Because a global synchronization is needed before the optimizer step to check for overflows and ensure the scaling factor is aligned across all workers. The introduction of BF16 on hardware like Google TPUs and later NVIDIA GPUs (starting with the Ampere architecture) is a game-changer. Having a same dynamic range as FP32, BF 16 offered a drop-in replacement for FP32 that obviates meticulous loss scaling. Its resilience to overflow and underflow made training LLMs significantly more stable and straightforward. Consequently, BF16 quickly became the de-facto standard for modern mixed-precision training. 3.4 Why FP16 is the Key for RL Fine-Tuning While BF16s stability is an advantage for pre-training models, our findings reveal that its low precision is the origin of the training-inference mismatch. Modern RL frameworks often use different engines or optimized kernels for training and inference. Even if both are configured to use BF 16, subtle differences in their implementation (e.g., CUDA kernel optimizations, parallel strategies) can lead to different rounding errors on BF16. When these small discrepancies accumulate over a sequence of tokens during autoregressive sampling, the resulting probability distributions for 7 and ju can diverge significantly. This divergence is the source of the biased gradients and the deployment gap discussed earlier. This is precisely why switching to FP16 provides a fundamental solution. With its 10 mantissa bits, FP16 offers 8 times more precision (21 values vs. 2 values) than BF16. This higher fidelity means that the outputs of the training and inference engines are much more likely to be numerically identical. The increased precision creates a buffer that absorbs the minor implementation differences between the two engines, preventing rounding errors from accumulating and causing a policy divergence. For RL fine-tuning, the dynamic range of the models weights and activations has already been established during pre-training. Therefore, the extreme range of BF16 is less critical, while the precision it sacrifices becomes a dominant drawback. By reverting to FP16, we trade the unnecessary range of BF16 for the critical precision, effectively closing the gap between training and inference without any complex algorithmic or engineering workaround. 3.5 Offline Analysis Results Before proceeding to RL fine-tuning, we first perform an offline analysis to examine performance and traininginference mismatch under different numeric precisions. We begin by sampling 32 responses per question from the AMC and AIME benchmarks [Li et al., 2024] using the DeepSeek-R1-Distill- Qwen-1.5B model [Guo et al., 2025], with a 32K total token budget under both BF16 and FP16 precisions. As shown in Table 2, their performance is largely comparable, suggesting that higher inference precision alone does not necessarily yield improvements. We follow their recommended decoding settings: temperature 0.6 and top-p 0.95. ==== Page 7 ==== Token Probability (BF16) Token Probability (FP16) Seq mismatch v.s. Len (BF16) Seq mismatch v.s. Len (FP16) |----- No mismatch (m= yu) 1. 1.0 E E 08 Pr 0.8 Y Y -10 -10 r-) r-) a8 ace EI 20 EI 20 con) . con) Da Da  04 04 2 x _ 2 x = = sl 1 : Si 0.07  = | ope = -1. 5 e 7 lope = -0.  |  rKUulml=7.64) 491 KLfy|m] = 0.32 Sa No mismatch (17= b 4 2 -! 0.0 (n=H) 0.0 -50 a 50 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 04 0.6 08 1.0 ) 5 10 15 20 25 ) 5 10 15 20 25 Inference policy Inference policy Sequence length (K) Figure 2: FP16 significantly reduces the training-inference mismatch. The left two plots show the token-level probability distribution, and the right two plots present the distribution of sequence-level log probability ratio between the inference policy (.) and the training policy (7). Dashed lines in black denote perfect precision without mismatch. Next, we re-generate 32 responses per question using temperature 1.0 and no top-p sampling (so that ju is directly comparable to 77), and evaluate the token log-probabilities using the same model weights within the DeepSpeed training engine, under both BF16 and FP 16 settings. The left two plots in Figure 2 show the resulting distributions of token probabilities. We find that FP16 notably reduces the mismatch between ,: and 7, with data points more tightly concentrated around the diagonal. Beyond token-level discrepancies, we also analyze sequence-level mismatch, since aus} serves as an unbiased estimator of the importance sampling weight for a full response. The right two plots in Figure 2 depict the distribution of sequence-level log-probability ratios across different generation lengths. The results clearly indicate that BF16 introduces an exponentially larger mismatch, which worsens with longer responses due to cumulative autoregressive errors, whereas FP16 maintains the mismatch at a much milder level (approximately 24x smaller). 4 A Sanity Test for RL Algorithms To rigorously assess the reliability and robustness of RL algorithms, we introduce a novel sanity test. Standard benchmarks often contain a mix of problems with varying difficulty, including questions that are either overly trivial or unsolvable by the initial model. Trivial questions waste computational resources, while unsolvable ones make it difficult to determine whether poor performance stems from a flawed algorithm or the models inherent limitations. Our sanity test is designed to remove this ambiguity with efficiency. By creating a perfectible dataset where every problem is known to be solvable but not trivial, we can cleanly isolate and evaluate an RL algorithms ability to unlock a models latent potential. On this perfectible dataset, a reliable RL algorithm should theoretically be able to achieve 100% training accuracy. We construct this perfectible dataset by filtering out those overly trivial and unsolvable ques- tions for the initial model. Specifically, we unroll 40 responses for each problem in the MATH dataset [Hendrycks et al., 2021], and only keep problems where the initial accuracy is between 20% and 80%. This process yielded a targeted dataset of 1,460 questions for the DeepSeek-R1-Distill- Qwen-1.5B model [Guo et al., 2025]. The smaller size of this dataset makes achieving near- 100% accuracy computationally feasible, allowing for efficient and conclusive testing. We define our sanity test with a clear criterion: an RL algorithm passes if its training accuracy on this perfectible dataset converges above a high threshold (e.g., 95%). An algorithm that fails this test can be considered unreliable or fundamentally flawed, as it is unable to guide the model to solve problems known to be within its reach. While passing is not a guarantee of universal success, failing is a strong indicator of an ill-suited algorithm design, making this test a crucial diagnostic tool. 4.1 Experimental Setup Under this sanity test, we evaluate several representative RL algorithms, particularly those designed to address the training-inference mismatch (see Section 2.1). All experiments use DeepSeek-R1- Distill-Qwen-1.5B as the initial model, with a context length of 8,000. We run each experiment on 8 NVIDIA A100 80G GPUs. For each policy iteration [Schulman et al., 2017], we use a batch size of 64 questions (with 8 rollouts per question) and perform 4 gradient steps. For algorithms in the ==== Page 8 ==== Rewards AIME 2024 Mean[Abs(pi - )] Max&Min of rm - >o7 = BF16 GRPO 0.28 0.03  BF16 GRPO-TokenTIS | 45  BF16 GRPO-Seq-MIS 0.02 06 = BF16 GSPO 028 oor = FP16 PG-Seq-IS 0.20 05 0.00 10 Rewards 0.40 AIME 2024 KL[y|7]  BF16 GRPO  BF16 GRPO-Token-TIS | 455  BF16 GRPO-Seq-MIS 10-4 0.50 == No mismatch (7m = yu)  BF16 GSPO == FP16 PG-Seq-IS 0.20 10% 05s 7 L -1.00 [REDACTED] 1500 ~-2000 [REDACTED]. 1500 ~-2000 [REDACTED] 0 10> -0.25 -0.[REDACTED] 2000 Figure 3: Simply switching from BF16 to FP16 stabilizes and prolongs RL training. The basic importance-weighted policy gradient algorithm in FP16 outperforms all baselines in BF16. Note that the third metric reported in each row slightly differs in implementation due to the use of separate codebases (VeRL and Oat). These metrics are semantically similar, and the minor differences do not affect our conclusions. GRPO family, we set the clip_higher to 0.28 by default [Yu et al., 2025]. The clipping threshold for importance sampling methods (Equation (7) and Equation (10)) is set to C = 3. We evaluate a suite of methods designed to address the training-inference mismatch. This includes:  A vanilla GRPO baseline (specifically, the Dr.GRPO variant from Equation (8)) [Shao et al., 2024, Liu et al., 2025c].  GRPO with a token-level TIS correction (Equation (9)) from Yao et al. [2025].  GRPO with a sequence-level MIS correction (Equation (10)) from Liu et al. [2025a].  The standard policy gradient algorithm with importance sampling (Equation (5)). In addition, we include GSPO [Zheng et al., 2025] in our experiments, although it was primarily designed to address the mismatch introduced by MoE models. 4.2 Comparison with Existing Algorithmic Corrections To ensure robustness and rule out implementation-specific artifacts, we conducted experiments across two different frameworks: VeRL? [Sheng et al., 2024] and Oat [Liu et al., 2025b]. The results, shown in Figure 3, highlight the instability of existing methods when using BF16 precision. The vanilla GRPO baseline collapses early in training, reaching a peak accuracy of only 73% in VeRL and 84% in Oat before its performance degrades. The token-level TIS correction [Yao et al., 2025] prolongs training slightly but ultimately fails, collapsing after reaching 82% (VeRL) and 88% (Oat) accuracy, an observation that aligns with findings from Liu et al. [2025a]. Surprisingly, GSPO demonstrates more stable training for a longer period than GRPO with token-level TIS, achieving higher rewards despite not using the inference policy ju at all.4 Among all the algorithmic corrections in BF16, only GRPO with sequence-level MIS [Liu et al., 2025a] maintains stable training without collapsing. However, this stability is costly. The method suffers from slow convergence due to the high variance of its sequence-level importance ratio (see Figure 2). More importantly, even at its peak, it exhibits a significant deployment gap compared to We identified and corrected an implementation bug in VeRLs Dr.GRPO for our experiments. We optimized the training speed of VeRL based on https: //github.com/sail-sg/odc. In our VeRL experiment, the GSPO gradient norm became NaN after 1200 steps, halting further model updates. ==== Page 9 ==== Rewards Response Length AIME 2024 AIME 2025 0.400 FP16 GRPO FP16 GRPO-TIS FP16 GRPO-Seq-MIS FP16 GSPO FP16 PG-Seq-IS Figure 4: Comparisons between various algorithms based on FP16. our FP16 approach. It achieves a maximum training accuracy of only 95% (vs. 99% in FP16) and a score of 34% (vs. 39% in FP16) on the AIME 2024 benchmark, demonstrating a clear performance ceiling. More evidence on deployment gap can be found in Figures | and 6. The Efficacy of FP16 Precision In contrast to these algorithmic approaches, simply switching both training and inference precision from BF16 to FP16 provides a dramatic improvement. As shown in Figures | and 6, the FP16 training runs are significantly more stable, converge much faster, and achieve substantially higher final rewards and evaluation scores across all tested algorithms. This result demonstrates that addressing the mismatch at the precision level is a more direct and effective solution than applying unstable or inefficient algorithmic corrections. The most surprising finding is that FP16 precision fundamentally improves the behavior of importance sampling. The sequence-level ratio, which is notoriously high-variance, becomes much more concentrated and stable in FP16 (see Figure 2). This stabilization makes it practical to use the classic, unbiased policy gradient estimator without any modifications (Equation (5)). As shown in Figure 3, this simple, unbiased approach, when powered by FP16, dramatically outperforms all existing algorithmic corrections in BF16. Training Dynamics Our experimental results reveal an interesting phenomenon: algorithms that eventually collapse consistently exhibit a growing training-inference mismatch beforehand, making it a potential early-warning signal (see Figure 3). During this period, the policy difference 7(-|0)  11(-|6) also converges to extreme values, where one policys probability approaches | while the others approaches 0, despite using the same copy of weights. We suspect this is driven by a particular optimization bias, though further validation is required. In contrast, stable algorithms maintain a bounded mismatch. Crucially, FP16 training shows a much lower mismatch level than any BF16 method. This inherent stability at the precision level explains why a simple policy gradient with FP16 can outperform all existing, more sophisticated solutions. Framework-Specific Differences While our core conclusions hold across both the VeRL [Sheng et al., 2024] and Oat [Liu et al., 2025b] frameworks, we observed subtle implementation-dependent differences. Initially, the training-inference mismatch is slightly smaller in Oat than in VeRL; for example, the initial policy difference 7(-|0)  ju(-|6) has a minimum near -0.9 in Oat versus -1.0 in VeRL. Even under FP16, where both frameworks exhibit a small mismatch, VeRL was more prone to occasional numerical spikes. These subtle stability differences, which we attribute to their different distributed backends (DeepSpeed ZeRO vs. PyTorch FSDP), likely explain why Oat yields slightly higher training rewards, particularly for the algorithms that eventually collapse. 4.3 Reviewing RL Algorithms under FP16 We then reviewed the performance of various RL algorithms when trained with FP16 precision. As shown in Figure 4, the performance differences between algorithms become almost indistinguishable. We attribute this convergence in performance to the significantly reduced training-inference mismatch in FP16, which effectively transforms the optimization problem into a nearly on-policy setting. In this state, the complex corrections offered by different algorithms provide little to no additional benefit. We did observe a minor exception where the original GRPO scored slightly lower on the AIME 2024 benchmark; however, it also scored slightly higher on AIME 2025, making it difficult to draw a definitive conclusion about its relative performance. ==== Page 10 ==== Rewards AIME 2024 Rollout Time Max&Min of 7 - 0.40 350 1.00 0.38 300 0.75 0.35 0.50 250  fp32vilm-bfl6fsdp | -33 0.25  fpl6vilm-bfl6fsdp 0.30 200 === fp16vilm-fp16fsdp 0.28  bflvilm-bfl6fsdp 0.25 0.25 . 7 100 tm Vv 0.23 Merete sicmunnnnpadevetag lt 0.50 == No mismatch (1 = py) T 50 = u 0.20 0.75 lt 0.5 0 -1.[REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] [REDACTED] Figure 5: Ablation on the precision combinations. 4.4 Ablation on the Precision To isolate the effects of training and inference precision, we conducted an ablation study on the VeRL framework, using VLLM [Kwon et al., 2023] for inference and PyTorch FSDP [Zhao et al., 2023] for training. The results are presented in Figure 5. When training with BF16 precision, we found that increasing the inference precision consistently prolonged training stability and improved performance. Notably, when paired with FP32 inference, the training run became fully stable with no signs of collapse. However, this stability came at an immense cost: FP32 inference was nearly three times slower than FP16 or BF16 inference, making this combination impractical for large-scale experiments. In contrast, using FP 16 for both training and inference yielded the best results. This combination not only produced the lowest training-inference mismatch but also resulted in the most stable training dynamics. It successfully reached nearly 100% training accuracy on the perfectible dataset without any loss of speed, demonstrating a clear superiority in both stability and efficiency. 5 Generalization Across Models, Data, and Training Regimes In Section 4, we scrutinized various algorithmic fixes under the sanity-check setting and found that simply switching from BF16 to FP16 can substantially improve training stability (Section 4.2), with its effect often overshadowing algorithmic tweaks (Section 4.3). In this section, we move beyond the sanity-check setting and validate our findings across more diverse scenarios, including Mixture-of-Experts (MoE) RL, Low-Rank Adaptation (LoRA) RL, and RL on larger prompt sets and alternative model families. 5.1 MoE RL Mixture-of-Experts (MoE) reinforcement learning (RL) training is known for its instability and often requires sophisticated stabilization strategies [Zheng et al., 2025]. Both training and inference of MoE models typically involve distinct parallelization strategies and precision-sensitive operations such as top-k expert selection, which further complicate the situation and usually lead to a larger traininginference mismatch compared to dense models. Given the widespread adoption of MoE architectures in modern LLMs, we conduct RL experiments on MoE models using Qwen3-30B-A3B- Base. We evaluate three different algorithms: GRPO-Seq-MIS, GRPO-Token-TIS, and PG-Seq-TIS, with detailed experimental settings provided in Section A.1. Experiments using FP16 show greater stability and consistently higher training accuracies (see (i), Gj), and (k) in Figure 1) as well as higher validation rewards (see (i), (j), and (k) in Figure 6). The improvement is consistent across all three algorithms, indicating that adopting FP16 effectively mitigates the traininginference mismatch and enhances overall performance. 5.2 LoRA RL LoRA [Hu et al., 2022] has recently regained popularity in LLM RL [Wang et al., 2025a, Schulman and Lab, 2025] due to its efficiency and performance comparable to full fine-tuning. To examine how LoRA-based RL is affected by numeric precision, we train Qwen2.5-Math-1.5B models on the standard MATH dataset using GRPO-Token-TIS (Equation (9)). LoRA is applied to all layers with a 10 ==== Page 11 ==== rank of 32 and scaling factor a = 64. Following Schulman and Lab [2025], we adopt a slightly larger learning rate (4 x 10~) than that used in full fine-tuning. As shown in Figure 1 (h), BF16-based LoRA training collapses after roughly 600 steps, whereas FP16 maintains stable training throughout. 5.3. RL on Large Dense Models Large-scale parameters are typically required in modern LLMs, yielding significantly better perfor- mance compared to smaller models. This motivates us to conduct RL experiments on large dense models. Specifically, we experiment with Qwen3-14B-Base and follow the algorithm of DAPO [Yu et al., 2025]. Refer to Section A.1 for details of experimental settings. As shown in Figure | (1), the training rewards with FP16 increase much faster than those with BF16. Figure 6 (1) demonstrates that FP16 achieves higher validation accuracy on AIME 2024. These results suggest that using FP16 instead of BF16 effectively mitigates the traininginference mismatch in large models, highlighting the potential of this approach for scaling RL training on large models. 5.4 RL on Other Model Families The base models, which serve as the initial policies for RL, can substantially influence the learning dynamics, as they determine not only the scope of exploration but also the numerical range and sensitivity of network parameters and activations. To strengthen our experimental conclusions, we extend our study beyond Qwen-based models and train OctoThinker-3B [Wang et al., 2025b], a model mid-trained from Llama3.2-3B [Grattafiori et al., 2024] on reasoning-intensive data using GRPO. As shown in Figure | (g), BF16 training destabilizes after around 150 steps due to numerical mismatch, while FP16 continues to train smoothly without collapse. 6 Discussions Rethinking the Precision Tradeoff in RL Fine-Tuning Numerical precision is a foundational choice in the LLM training stack, yet this choice has long been dominated by BF16 for both pre- training and post-training, prized for its wide dynamic range and ease of use. Our results, however, suggest this default deserves careful rethinking for RL fine-tuning. In this phase, the training-inference mismatch becomes a critical source of instability, and BF16s low precision exacerbates this problem. We demonstrate that by simply trading BF16s wide dynamic range for FP16s higher precision, one can achieve significantly more stable RL training, faster convergence, and superior final performance. It is important to note that we are not claiming FP16 is a universally optimal choice. The pursuit of efficiency may lead developer to even lower precisions like FP8. Furthermore, using FP16 for extremely large models might present engineering challenges related to its limited range, such as managing potential overflows. However, we believe these are solvable challenges, as evidenced by the recent successes in large-scale FP8 training. Ultimately, we hope this work inspires the community to reconsider FP16 as a powerful and often more suitable alternative for stabilizing RL fine-tuning. The Bias-Variance Tradeoff under BF16 Precision Our results in Section 4.2 reveal a bias- variance trade-off among RL algorithms operating under BF16 precision. Methods with lower variance but higher bias (like GRPO, token-level TIS, and GSPO) initially converge quickly but prove unstable and eventually collapse. Conversely, less biased algorithms that more accurately correct for the policy mismatch (like PG-Seq-IS and GRPO-Seq-MIS) achieve stability but at the cost of high variance, which slows their convergence. This trade-off, however, becomes far less critical under FP16 precision. By fundamentally reducing the training-inference mismatch, FP16 naturally lowers both the bias induced by the mismatch and the variance of the importance sampling corrections. This enhanced stability allows even the most naive policy gradient estimator to converge efficiently, creating a training dynamic where all tested algorithms perform well and the tension between stability and speed is effectively resolved. 11 ==== Page 12 ==== 7 Conclusion This work demonstrates that the training-inference mismatch, a major source of instability in RL fine-tuning, is fundamentally a problem of numerical precision. While existing algorithmic fixes are often complex and inefficient, we show that simply switching from the standard BF16 format to the higher-precision FP16 format can virtually eliminate the mismatch. This single, efficient change leads to more stable training, faster convergence, and superior performance, proving that addressing the problem at the precision level is a more effective strategy. We conclude that FP16 should be reconsidered as a foundational option for robust RL fine-tuning of LLM. References Arash Ahmadian, Chris Cremer, Matthias Gall, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustiin, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in IIms. arXiv preprint arXiv:2402.14740, 2024. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506. 13585, 2025. Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jian- shu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Revisiting reinforcement learning for llm reasoning from a cross-domain perspective, 2025. URL https: //arxiv.org/abs/2506. 14965. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in neural information processing systems, 25, 2012. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages [REDACTED]. PMLR, 2018. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Horace He. Defeating nondeterminism in Ilm inference. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.[REDACTED]. https://thinkingmachines.ai/blog/defeating-nondeterminism- in-llm-inference/. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner series. https: //capricious-hydrogen-41c. notion. site/Skywork-Open-Reaonser-Series- 1d0bc9ae823a80459b46c 149e451680, 2025. Notion Blog. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. JCLR, 1(2):3, 2022. 12 ==== Page 13 ==== Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv: 1905.12322, 2019. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for free!, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages [REDACTED], 2023. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Jiacai Liu, Yingru Li, Yugian Fu, Jiawei Wang, Qian Liu, and Yu Shen. When speed kills stability: Demystifying rl collapse from the inference-training mismatch, 2025a. https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from- the-Inference-Training-Mismatch-27 121 1a558b7808d8b 12d403fd15edda. Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, and Min Lin. Oat: A research-friendly framework for Ilm online alignment. https: //github.com/sail-sg/oat, 2025b. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783, 2025c. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpass- ing ol-preview with a 1.5b model by scaling rl. https://github.com/agentica-project/ deepscaler, 2025. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv: 1710.03740, 2017. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023. Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438, 2025. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza- tions enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages [REDACTED], 2020. John Schulman and Thinking Machines Lab. Lora without regret. Thinking Machines Lab: Connec- tionism, 2025. doi: 10.64434/tml.[REDACTED]. https://thinkingmachines.ai/blog/lora/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv: 1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 13 ==== Page 14 ==== Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409, 19256, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan- zaro. Megatron-Im: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv: 1909.08053, 2019. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, and Jun Zhou. Every attention matters: An efficient hybrid architecture for long-context reasoning. arXiv preprint arXiv:2510.19338, 2025a. Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, et al. Every step evolves: Scaling reinforcement learning for trillion-scale thinking model. arXiv preprint arXiv:2510.18855, 2025b. Shangshang Wang, Julian Asilis, Omer Faruk Akgiil, Enes Burak Bilgin, Ollie Liu, and Willie Neiswanger. Tina: Tiny reasoning models via lora. arXiv preprint arXiv:2504.15777, 2025a. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025b. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):[REDACTED], 1992. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. https://fengyao.notion.site/off-policy-rl. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source Ilm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl- zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304. 11277, 2023. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 14 ==== Page 15 ==== A Detailed Experimental Settings A.1 MoE RL As for experiments of MoE RL, we use Qwen3-30B-A3B-Base as the base model. The training data comes from DAPO-Math-17k [Yu et al., 2025], and we conduct online evaluation on AIME 2024 using the avg@32 metric. The training is performed with the VeRL framework [Sheng et al., 2024], and the key hyperparameters are summarized in Table 3. Dr.GRPO [Liu et al., 2025c] proposes using a constant normalizer instead of a token-count-based normalizer. Notably, the open-source VeRL implementation does not correctly implement this. We refer to our corrected version as seq-mean-token-sum-norm for actor. loss_agg_mode in VeRL. A.2. RL on Large Dense Models For experiments on large dense models, we use Qwen2.5-14B-Base as our base model. The training data is sourced from the mathematical domain dataset curated by Cheng et al. [2025]. They aggregated recent math reasoning collections including OR1 [He et al., 2025], DAPO [Yu et al., 2025], and DeepScaler [Luo et al., 2025], and then performed deduplication and filtering to derive a final collection of 54.4k math training samples. We conduct online evaluation on AIME 2024 using the avg@8 metric. The training algorithms and hyperparameters follow the setup described in Yu et al. [2025], as summarized in Table 3. Table 3: Hyperparameters used for RL training of MoE models and large dense models. Parameter MoE RL Large dense RL trainer .nnodes 8 8 trainer .n_gpu_per_node 8 8 model.path Qwen3-30B-A3B-Base Qwen3-14B-Base vllm_version 0.10.0 0.10.0 data.train_batch_size [REDACTED] data.gen_batch_size N/A 1536 data.max_prompt_length [REDACTED] data.max_response_length 20480 20480 rollout.n 16 16 rollout.temperature 1.0 1.0 rollout .top_p 1.0 1.0 val_kwargs.temperature 0.6 1.0 val_kwargs.top_p 1.0 0.7 actor .ppo_mini_batch_size 32 32 actor .ppo_max_token_len_per_gpu 22528 22528 optim.1lr le-6 le-6 optim.lr_warmup_steps N/A 10 optim.weight_decay 0.0 0.1 optim.betas [0.9, 0.95] [0.9, 0.999] optim.eps le-15 le-8 algorithm.use_kl_in_reward False False actor.use_kl_loss False False actor.clip_ratio_high 0.28 0.28 actor.clip_ratio_low 0.2 0.2 actor.clip_ratio_c N/A 10.0 C in Equations (6) and (7) 3.0 N/A actor.loss_agg_mode seq-mean-token-sum-norm token-mean overlong_buffer.enable False True overlong_buffer.len N/A 4096 overlong_buffer.penalty_factor N/A 1.0 filter_groups.enable False True filter_groups.metric N/A acc filter_groups.max_num_gen_batches N/A 10 15 ==== Page 16 ==== B_ More Experimental Results (a) Sanity GRPO (b) Sanity GRPO-Token-TIS (c) Sanity GRPO-Seq-MIS (d) Sanity GSPO 0.40 maT) a.40 0.40 a.40  FP16 0.35 0.35 0.35 0.35 0.30 0.30 0.30 0.30  BF16  FP16 os 0.25 os 0.25  BF16  BF16 0.20 0.20  FP16 0.20 3 20 sbo [REDACTED] [REDACTED] 0 sb yoo as00~ROOOS*~ SOC soo 1000 ~~as00~0ODS~SC SO 5 sbo [REDACTED] Training Steps Training Steps Training Steps Training Steps (e) Sanity PG-Seq-IS (f) Sanity PG-Seq-MIS (g) OctoThinker GRPO (h) Lora GRPO-Token-TIS 0.40 BF16 0.40 os os  FP16 o7 0.35 0.35 os 06 0.30 0.30 03 os 0.2 o4 os 0.25  BF16 an  BF16 03  BF16 0.20 0.20  FP16  FP16  FP16 oo 02 3 sbo [REDACTED] soo yoo 1s00~~B000~~=S 0 3 ao ado.~eo~=~=CwOOS~S* OO [REDACTED] @[REDACTED] Training Steps Training Steps Training Steps Training Steps (i) MoE GRPO-Seq-MIS (j) MoE GRPO-Token-TIS (k) MoE PG-Seq-TIS (1) Dense-14B DAPO 0.50 o.s0  BF16 a4o ) FP16 a.40 0.40 0.30 a.40 0.30 0.30 0.20 o20 020 0.30 oo  BF16 | o10  BF16 | o10  BF16 | 4  FP16  FP16  FPl16 0.00 0.00 0.00 3 50 75 100 ads ao 5 30 yo rt 250 3 9 7 ao a5 450 avs 5 Fa ry eo ry Training Steps Training Steps Training Steps Training Steps Figure 6: Evaluation comparisons between BF16 and FP16 across various frameworks, algorithms, datasets and training regimes. While Figure 1 presents the training reward curves under different precisions, Figure 6 shows evaluation results using checkpoints trained with these precisions. The results indicate that FP16- trained models generalize well to unseen benchmarks, further supporting our claim. 16

==== Page 1 ==== arX1v:2510.26787v1 [cs.LG] 30 Oct 2025 Remote Labor Index: Measuring AI Automation of Remote Work Mantas Mazeika*', Alice Gatti*', Cristina Menghini*", Udari Madhushani Sehwag*, Shivam Singhal*', Yury Orlovskiy*! Steven Basart', Manasi Sharma, Denis Peskoff?, Elaine Lau, Jaehyuk Lim, Lachlan Carroll', Alice Blair!, Vinaya Sivakumar', Sumana Basu, Brad Kenstler?, Yuntao Ma, Julian Michael, Xiaoke Li!, Oliver Ingebretsen', Aditya Mehta, Jean Mottola!, John Teichmann, Kevin Yu', Zaina Shaik', Adam Khoja, Richard Ren', Jason Hausenloy', Long Phan', Ye Htet?, Ankit Aich?, Tahseen Rabbani, Vivswan Shah, Andriy Novykov', Felix Binder Kirill Chugunov?, Luis Ramirez, Matias Geralnik?, Hernan Mesura, Dean Lee!, Ed-Yeremai Hernandez Cardona, Annette Diamond! Summer Yue**', Alexandr Wang**", Bing Liu**?, Ernesto Hernandez**?, Dan Hendrycks**! Center for AI Safety Scale AI Abstract Als have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AJ-driven labor automation. 1 Introduction The potential for AI to automate human labor is a subject of profound societal interest and concern. As AI capabilities advance, understanding their impact on the workforce becomes increasingly urgent. However, we lack standardized, empirical methods for monitoring the trajectory of AI automation. Without reliable metrics grounded in real-world economic activity, stakeholders may struggle to build consensus and proactively navigate AI-driven labor automation. While AI systems have demonstrated rapid progress on a variety of benchmarks, it remains unclear how these gains translate into the capacity to perform economically valuable work. Many existing AI agent benchmarks measure performance on specialized skills such as software engineering and basic computer use [32], while some focus on simple tasks shared across several professions [23]. These provide valuable signals of capabilities in isolation, yet they often do not capture the vast diversity and complexity inherent in the broader landscape of remote work. Consequently, performance on these benchmarks offers limited insight into the trajectory of human labor automation. *Equal contribution **Senior authors Work done while at Scale AI_*Work done while at CAIS ==== Page 2 ==== Example Projects from RLI oduct Render Human Deliverable: 5 Human Deliverable: Build an interactive dashboard World Happiness Scores Create 3D animations to for exploring data from the ai i showcase the features of a World Happiness Report. y new earbuds design and case. Features: * Silicone tips * Replaceable battery * Sleek charging case ee = = -   Case Back Front Top Battery Requirements: - Use provided data * Overview map * Detailed score breakdown Human Deliverable: i  Create a 2D animated video Rm). p =) r Develop architectural plans anda advertising the offerings of a Ke Ss 3D model for a container home tree services company. A. a based on an existing PDF design : Ld Requirements:  Use provided voiceover file. + Flat design; no subtitles S WAV j VoiceOver.wav Build a brewing-themed a Format a paper using the provided version of the Watermelon figures and equations for an IEEE Game, where players merge Ea conference. falling objects to reach the : : highest-level item Features:  Physics-based interaction + Use the provided objects + Minimalist Ul * Relaxing background music + ov LN Improve task Spot check to criteria for issues YL Ensure tasks meet our D requirements 240 final  tasks 5B 550 tasks collected from >300 freelancers Long Tail. Other tasks a marquise-cut Compare p acceptable? diamond J Justification Input Fil ke peo @) Human Output Figure 7: Evaluation Pipeline: For each RLI project, AI deliverables are rigorously checked against human gold-standard deliverables and the requirements in the project brief for flaws and to determine whether the AI deliverable would be accepted as work product in a realistic freelance setting. Evaluating AI deliverables is itself a highly agentic task, so automating evaluation with LLMs is not currently feasible. Thus, all evaluations are performed manually by trained workers and subject experts. Inter-annotator agreement is above 94%. natively render dozens of different file formats, facilitating a consistent evaluation experience across varied projects. The code for the evaluation platform is open-sourced. Automation rate evaluation. Our evaluation methodology centers on determining whether an AI deliverable completes the project at least as well as the human gold standardspecifically, whether the deliverable would be accepted by a reasonable client as the commissioned work. In preliminary evaluations, we found granular per-project rubrics were often insufficient for capturing project completion. Particularly for projects with hard-to-specify aspects (e.g., design), a deliverable might technically satisfy rubric elements yet fail professional standards. Consequently, we employ a holistic evaluation approach (visualized in Figure (7), drawing from practices for reviewing complex artifacts like papers or grants. Evaluators digest the project context (brief, input files, human deliverable) and compare the human and AI deliverables, examining specific files until confident in their assessment. Given a fixed time per project, they assess the AI deliverable (the alternative) relative to the human deliverable (the reference) using the following 3-point scale, with a written justification: 1. The alternative deliverable does not satisfy the brief as well as the reference deliverable or is of significantly lower quality, such that it would not be accepted by a reasonable client as the commissioned work. 2. The alternative deliverable satisfies the brief as well as the reference deliverable and would be accepted by a reasonable client as the commissioned work. 3. Same as 2, and the alternative deliverable exceeds the reference deliverable in overall quality. The automation rate is calculated based on the percentage of projects receiving an annotation of 2 or 3. This holistic approach allows for targeted analysis, enabling evaluators to zoom into the deliverable and quickly identify major issues without navigating extensive rubrics. Once trained, human evaluators can complete evaluations relatively quickly using this approach. Elo evaluation. While the automation rate measures absolute project completion against the human baseline, the Elo metric captures the relative performance between different AI agents, combining project completion with overall quality. This allows models to eventually exceed the human Elo score of 1,000. The Elo evaluation involves a pairwise comparison between two AI Deliverables (AD-1 and AD-2). We use a modified version of the evaluation platform that displays both AI deliverables, along with the human deliverable as a reference for successful completion. Evaluators assess the comparison along two dimensions using separate 3-point Likert scales: ==== Page 9 ==== Model Automation Rate Manus 2.5% Grok 4 2.1% Sonnet 4.5 2.1% GPT-5 1.7% ChatGPT agent 1.3% Gemini 2.5 Pro 0.8% Table 1: Current AI agents perform near the floor on RLI, solving less than 3% of tasks in the benchmark. * Project completion: Which deliverable is closer to satisfying the brief (i.e., closer to a state where it would be accepted by a reasonable client)? (AD-1 closer / Equally close / AD-2 closer) * Overall quality: Which deliverable has higher overall quality for the project? (AD-1 higher / Same quality / AD-2 higher) To compute the Elo score, we derive a unified preference from these two dimensions. We prioritize the project completion judgment when at least one of the AI agents has failed to complete the project. If both agents have successfully completed the project, we switch to using the overall quality judgment. Evaluation standards and statistics. In all evaluations, we instruct evaluators to adopt the perspec- tive of a reasonable client to minimize subjectivity. This grounds quality assessments in the likely reception of the work in a professional context, rather than the evaluators personal preferences. We use majority voting across three independent evaluations to determine the final judgment. For Elo evaluations, if the three evaluations are split across the 3-way Likert scale (e.g., one vote for AD-1, one for AD-2, and one for a tie), this is recorded as indifference. The evaluation process demonstrates high reliability, with an inter-annotator agreement of 94.4% for the automation rate metric. For Elo evaluations, ternary inter-annotator agreement is 56.9%, far above random chance of 33.0%. The probability of hard disagreements (one vote for AD-1 and one vote for AD-2) is 5.9%, indicating that evaluators are directionally nearly always in agreement. Evaluation times are shown in Figure{I 1] Evaluators were requested to take a maximum of 20 minutes for Automation Rate evaluations and 30 minutes for Elo evaluations. These times were selected based on preliminary testing and provided ample time for completing most evaluations. Evaluations took 11.4 minutes on average for Automation Rate and 17.4 minutes for Elo. We hypothesize that the automation rate inter-annotator agreement rate will fall as AI deliverables become more complex, which could be countered with more experienced evaluators and longer evaluation time. 4 Experiments We evaluate the performance of several frontier AI agents on the Remote Labor Index (RLI) to assess the current state of AI automation capabilities on diverse economically valuable projects. We detail our experimental setup (Section]4. Tp, present quantitative results measuring both absolute and relative performance (Section|4.2), and provide a qualitative analysis of observed failure modes and agent behaviors Stone 4.1 Experimental Setup Models and Environments. We evaluate six state-of-the-art AI agents: ChatGPT agent [21], GPT-5 [22], Claude Sonnet 4.5 [2], Grok 4 [31], Gemini 2.5 Pro [9], and Manus [4]. For models that support computer-use, we used a computer-use scaffold developed by Scale AI. For models that do not support computer-use, we use the OpenHands scaffold, which we refer to as a command line interface (CLI) environment as opposed to a computer-use agent (CUA) environment. For GPT-5, we evaluated both the CUA and CLI scaffolds and report the CLI scaffold in the main tables, as this outperformed the CUA scaffold for this model. A full comparison of performance across environments is available in Appendix ==== Page 10 ==== Across All Projects, Al Agents Are Steadily Improving 1000 atten ee ee ee ee ee eee ee eee eee en ne =4 Human Baseline w 800-4 } fo) UO 7)  6004 WwW 400 4 @   $  * + G * G GF & Gemini GPT-5 Sonnet 4.5 ChatGPT Grok-4 Manus 2.5 Pro agent Figure 8: Relative performance (Elo) scores show that AI agents are making steady progress on RLI and there are meaningful differences between models, despite all models falling short of the human baseline of 1,000. Compared to the automation rate metric, Elo score provides a better measure of partial progress across all projects, including projects that are not solved yet. Scaffolding and prompting. To ensure a fair assessment of peak capabilities, we tune prompts and provide standardized tooling scaffolds. This includes equipping agents with necessary execution tools and providing clear instructions on interfacing with the evaluation platform. For comprehensive details on the experimental setup, including the full prompts used, see Appendix [B] 4.2 Quantitative Results We analyze the performance of AI agents on RLI using both absolute metrics (measuring success against the human baseline) and relative metrics (measuring progress between models). The main results are summarized in Table[I] Absolute performance is near the floor. The central finding of our evaluation is that current AI agents demonstrate minimal capability to perform the economically valuable projects in RLI. We measure this capacity using the Automation Rate: the percentage of projects completed at a quality level equivalent to or exceeding the human gold standard. Across all models evaluated, absolute performance is near the floor, with the highest Automation Rate achieved being only 2.5% (Manus). Correspondingly, the metrics tracking the economic impact of automation (Dollars Earned and Autoflation) are also close to the floor. These results indicate that contemporary AI systems fail to complete the vast majority of projects at a level that would be accepted as commissioned work in a realistic freelancing environment. Despite rapid progress on other AI benchmarks, current systems remain far from capable of autonomously handling the diverse and complex demands of the remote labor market. Elo score reveals steady improvement. While absolute performance remains low, it is crucial to detect more granular signs of progress. To measure the relative performance between different models, we use pairwise comparisons to compute an Elo score that represents how close models are to completing projects along with the overall quality of their deliverables. This enables tracking improvements between models, even when they fail to fully complete most projects. We find that progress is measurable on RLI. The Elo rankings (Figure [8) indicate that models are steadily improving relative to each other, and the rankings generally reflect that newer frontier models achieve higher performance than older ones. This demonstrates that RLI is sensitive enough to detect ongoing progress in AI capabilities. 10 ==== Page 11 ==== 4.3 Qualitative Findings To understand the limitations of current systems and the reasons for the low automation rates, we conducted a qualitative analysis of agent failures by clustering the written justifications provided by evaluators. This analysis reveals a variety of failure modes, ranging from general quality issues to common systematic errors. Common failure modes. Our qualitative analysis across roughly 400 evaluations shows that rejections predominantly cluster around the following primary categories of failure: 1. Technical and File Integrity Issues: Many failures were due to basic technical problems, such as producing corrupt or empty files, or delivering work in incorrect or unusable formats. 2. Incomplete or Malformed Deliverables: Agents frequently submitted incomplete work, characterized by missing components, truncated videos, or absent source assets. 3. Quality Issues: Even when agents produce a complete deliverable, the quality of the work is frequently poor and does not meet professional standards. 4. Inconsistencies: Especially when using AI generation tools, the AI work often shows inconsistencies between deliverable files. For each AI deliverable we assigned one or more failure categories based on issues observed dur- ing the evaluations. Table [2|reports the propor- Frequency (%) tion of deliverables affected by each category. Corrupted files 17.6 Representative failure modes include: videos Incomplete 35.7 far shorter than requested (e.g., 8 seconds rather Poor quality 45.6 than 8 minutes), child-like drawings using ba- Inconsistencies 14.8 sic geometric shapes, inconsistent visual ap- pearance across renderings (e.g., a houses ap- Table 2: Percentage of AI deliverables exhibiting pearance changing across different 3D views), issues, by category. Categories are not mutually robotic or unnatural voice-overs, digital floor exclusive; a deliverable may be counted in multiple plans that do not match the supplied sketches, categories. and web games that function but whose graphics fall short of professional standards. Successful AI deliverables. Across a small subset of projects, AI deliverables were judged com- parable or better than human output. These were predominantly creative projects, especially audio and image related work, along with writing and data retrieval/web scraping. Specifically, across all models we tested, performance matched or exceeded human baselines on several audio editing, mixing and production tasks (e.g., creating bespoke sounds effects for a retro video game, separating vocals from accompaniment in a single track, merging voice-overs with intro and outro music) and on image-generation tasks (e.g., ad and logo creation). AI also performed well on report writing and on generating code for interactive data visualization. We provide examples of successful and unsuccessful AI deliverables (see Appendix[C.6). Cognitive skills analysis. Hendrycks et al. show that the skills and weaknesses of LLMs can be decomposed into several distinct categories, such as broad world knowledge, memory, and audiovisual abilities. We observe that many of the failures exhibited by AI agents stem from deficits in these skills. For example, many failures stem from AI agents being unable to verify the correctness of their work and fix mistakes, especially in projects requiring complex and interactive audiovisual verification, such as architecture, game development, and web development. Analogously, many of the successes of AI models lie in domains where current AI models skills are more developed, such as projects where the complexity is primarily in text processing or image creation. 5 Discussion Generalization to automating new jobs. Historically, automation technologies have been task- specific: the electronic calculator automated the job of human calculators, but when these workers 11 ==== Page 12 ==== Example of Successful Project Completion Project Brief Create a self-hosted interactive dashboard that maps World Happiness Report scores on a world map with hover/click tooltips (country name and exact value) and a linked companion chart that highlights the selected country. Human Deliverable SAl Deliverable World Happiness Scores Q World Happiness Dashboard Pog 5 Figure 9: Here we show a successful project completion from Sonnet 4.5. Simple web visualizations that only require writing code are well within the capabilities of current AI agents, but this work makes up a small slice of all remote labor. Additional examples of successes and failures are shown in Figures/I6]and re-trained and focused on skills that had not yet been automated, the calculator wasnt able to automate any of these new tasks. This is because humans have general cognitive skills that calculators do not. AI differs qualitatively from other automation technologies; it is not designed merely to automate specific tasks, but is being explicitly developed to automate human intelligence itself. Indeed, current Als are not task-specific, but rather have general cognitive skills and are already capturing a substantial fraction of human-level cognitive generality [12]. An AI that automates all current remote work without overfitting is likely to have many of the same general cognitive skills as humans, allowing it to automate new jobs as they arise [[15]. In this way, Als may prove qualitatively different from prior automation technologies. While RLI does not fully represent every part of the remote labor economy, it is a substantial step towards measuring the ability of AI to automate the remote economy in general, rather than just current tasks. Limitations. RLI excludes some types of work found commonly in the remote labor economy, including projects requiring interaction with the client (e.g. tutoring), jobs that require working on a team (e.g., project management), and other types of work that did not meet our requirements (see Appendix [C.2}for the full list of requirements). While RLI is the broadest benchmark of its kind, it does not represent several types of remote work due to these constraints. Thus, an AI obtaining 100% automation rate on RLI may still underperform humans on types of work that we do not evaluate. The cost of the projects reported by human professionals reflects the cost at the time of project completion and is not adjusted for inflation. In most cases where we know the project completion date, the projects were completed in the past five years; consequently, the reported costs likely underestimate the current economic value of this work when accounting for inflation. 6 Conclusion RLI establishes an economically grounded measure of AI automation capacity, with 240 projects spanning 23 domains of digital freelance work, each anchored in demonstrated market value. Frontier Al agents perform near the floor on RLI, achieving an automation rate of less than 3%, revealing a stark gap between progress on computer use evaluations and the ability to perform real and economically valuable work. RLI aims to establish the empirical foundation stakeholders need to monitor AI capabilities, forecast labor market impacts, and proactively navigate AI-driven automation. 12 ==== Page 13 ==== Acknowledgments We would like to thank Anders Edson, Hale Guyer and Connor Smith for providing helpful feedback throughout the drafting process. We would also like to thank Michael Jae Byun and Brian Jang for helpful discussions. References [1] Daron Acemoglu. The simple macroeconomics of ai. Economic Policy, 40(121):13-58, 2025. [2] Anthropic. Claude sonnet 4.5 system card. System card, Anthropic, September 2025. [3] Erik Brynjolfsson, Bharat Chandar, and Ruyu Chen. Canaries in the coal mine? six facts about the recent employment effects of artificial intelligence. Stanford Digital Economy Lab. Published August, 2025. [4] Butterfly Effect Pte. Ltd. Manus. https: //manus.im/, 2025. [5] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [6] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [7] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [8] Nicholas Edwards, Yukyung Lee, Yujun Audrey Mao, Yulu Qin, Sebastian Schuster, and Najoung Kim. Rexbench: Can coding agents autonomously implement ai research extensions? arXiv preprint arXiv:2506.22598, 2025. [9] Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. 2025. [10] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:241 1.04872, 2024. [11 sy Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [12  Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alex Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, and Yoshua Bengio. A definition of agi, 2025. [13] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [14 sy Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. 13 ==== Page 14 ==== [15] Anton Korinek and Donghyun Suh. Scenarios for the transition to agi. Technical report, National Bureau of Economic Research, 2024. [16] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating Ilms as agents. arXiv preprint arXiv:2308.03688, 2023. [17  Grgoire Mialon, Clmentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [18] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier LLMs earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. [19  National Center for O*NET Development. DWA reference  O*NET 30.0 data dictio- nary. https: //www. onetcenter.org/dictionary/30.0/excel/dwa_reference. html 2025. [20 = National Center for O*NET Development. O*NET 30.0 database. 2025. Licensed CC BY 4.0. [21] OpenAI. Chatgpt agent system card. System card, OpenAI, July 2025. [22] OpenAI. Gpt-5 system card. System card, OpenAL, August 2025. [23] Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Sim6n Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, et al. Gdpval: Evaluating ai model performance on real-world economically valuable tasks. arXiv preprint arXiv:2510.04374, 2025. [24] Penrose. Can LLMs do accounting? https: //accounting.penrose.com/, 2025. Accessed: 2025-10-14. [25] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501,14249, 2025. [26] David Rein, Joel Becker, Amy Deng, Seraphina Nix, Chris Canal, Daniel OConnel, Pip Arnott, Ryan Bloom, Thomas Broadley, Katharyn Garcia, et al. Hcast: Human-calibrated autonomy software tasks. arXiv preprint arXiv:2503.17354, 2025. [27  David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [28] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. [29] Bertie Vidgen, Abby Fennelly, Evan Pinnix, Chirag Mahapatra, Zach Richards, Austin Bridges, Calix Huang, Ben Hunsberger, Fez Zafar, Brendan Foody, et al. The ai productivity index (apex). arXiv preprint arXiv:2509.25721, 2025. [30 = Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents against human experts. arXiv preprint arXiv:2411.15114, 2024. [31] xAI. Grok 4 model card. Model card, xAI, August 2025. [32] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040-52094, 2024. 14 ==== Page 15 ==== [33] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. t-bench: A benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406. 12045, 2024. [34] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 15 ==== Page 16 ==== Model Automation Rate Model Elo Manus 2.5% Manus 509.9 Grok 4 2.1% Grok 4 468.2 Sonnet 4.5 2.1% ChatGPT Agent 454.3 GPT-5 (CLI) 1.7% Sonnet 4.5 441.7 ChatGPT agent 1.3% GPT-5 (CLI) 436.7 GPT-5 (CUA) 0.8% GPT-5 (CUA) 431.6 Gemini 2.5 Pro 0.8% Gemini2.5 Pro 411.8 Table 3: Full automation rate and Elo results. In Appendix [A.3| we describe our comparison of two agent scaffolds for GPT-5, a command-line interface (CLI) scaffold and computer-use (CUA) scaffold. In the main paper, we show GPT-5 with the CLI scaffold. Model Dollars Earned /Max Possible Manus $1,720/$143,991 Sonnet 4.5 $1,280/$143,991 GPT-5 (CLI $1,180/$143,991 Grok 4 $858/$143,991 GPT-5 (CUA) $858/$143,991 ChatGPT agent $520/$143,991 Gemini 2.5 Pro $210/$143,991 Table 4: Current models earn a small fraction of the total cost of projects in the dataset. A Additional Results A.1_ Full Results In Table [3] we show the precise Elo score and automation rate for all models, including the CLI and CUA scaffolds for GPT-5. In Table 4| we show the dollars earned for all evaluated models. Current AI agents earn a small fraction of the total cost of projects in the dataset. A.2. Autoflation In Figure [10] we show the reduction in the cost of completing the projects in RLI. Analogous to indices that track the price of bundles of goods, this lets us track deflation in the effective price of the fixed bundle of projects represented by RLI. We refer to this quantity as autoflation and plot how it changes over time as new models are released. For each project, we measure the cost difference relative to the human-produced deliverable when using the lowest-cost method of achieving an acceptable deliverable. If no AI method completes the project at a lower effective cost than the human baseline, the reduction is zero for that project. Because the metric is sensitive to false positives in annotation, we audit all AI deliverables marked as successful to minimize the false-positive rate. A.3 Effect of Agent Scaffolds Our results suggest that current models are not yet able to take full advantage of computer-use environments. For instance, GPT-5 demonstrated superior performance when using a CLI-based agent compared to the Computer-Use Agent (CUA) setup. This holds for both the Elo scores (CLI: 436.7; CUA: 431.6) and the automation rates (CLI: 1.7%; CUA: 0.8%). We expect more vertical integration of model scaffolds will yield stronger performance. 16 ==== Page 17 ==== Autoflation: Cost Reduction for Completing Bundle of Projects 3.54 3.04 2.54 2.04 1.54 Autoflation (%) 1.0 7 0.5 4 0.0 July 2025 August 2025 September 2025 October 2025 Date Figure 10: Autoflation on RLI: the percentage decrease in the cost of completing the fixed RLI project bundle, using AI agents to complete projects if they successfully complete them at lower cost than humans. As AI systems achieve the same deliverables at lower effective cost, the price of this work declines. B_ Evaluation Details B.1 Model Details The vast majority of Manus deliverables were generated over the course of June, 2025. Some deliverables were generated in September, 2025. Our Gemini evaluations are with Gemini 2.5 Pro, not Gemini 2.5 Computer Use. We found that the latter struggled with our computer-use environment, since it was tuned to work with browser-only environments. B.2. Elo Computation Collecting preference data. Projects and model pairs are randomly sampled for comparison, using random ordering of model pairs to remove order effects. We use stratified sampling across models to ensure each model pair is compared on at least 10 projects (median 25). These are combined with the automation rate evaluations (model vs human) to obtain the final preference data. For each project that a model pair is compared on, we perform majority voting, using two independent evaluations with a third to break ties if needed. In cases where the three evaluations are prefer AD-1, indifferent, and prefer AD-2, we code the preference as indifference on this project. In cases where the majority vote is for indifference, we code the preference as 50/50. We numerically average these preferences across all compared projects to obtain a probabilistic preference for the model pair. These probabilistic preferences make up the preference graph. Fitting Bradley-Terry utilities. Following the Chatbot Arena methodology [6], we use global Bradley-Terry fitting on sampled preference edges to compute utility scores, which we refer to as Elo scores for ease of understanding. We use 100 bootstrap samples to compute 95% confidence intervals in Figure[8] Bootstrap samples are taken over projects, followed by re-averaging preferences on the sampled projects to obtain probabilistic preferences. Normalizing scores. After computing Bradley-Terry utilities, we scale and shift the utilities so that the human baseline obtains a score of 1,000 and a difference in score of 400 corresponds to 10: 1 odds of winning. 17 ==== Page 18 ==== Automation Rate Evaluation Time Elo Score Evaluation Time Min: 0.1 mins Mean: 11.4 mins 350 Mean: 17.4 mins 500 Median: 11.6 mins Median: 16.9 mins Max: 20.4 mins 300 Max: 35.6 mins >, 400 > 250 U U o o S 300 S 200 fox fox v  150  200 uw [REDACTED] 50 0 ie) 00 25 50 7.5 10.0 12.5 15.0 17.5 20.[REDACTED] Evaluation Time (minutes) Figure 11: Each evaluator was given a soft maximum of 20 minutes for model vs human evaluations and 30 minutes for model vs model evaluations (the latter requires inspecting more files and takes more time). In preliminary testing, we found this duration was adequate for nearly all projects. Total evaluation time per project is higher, as 2 to 3 evaluations were performed to obtain a majority vote. B.3 Evaluation and Generation Budgets Evaluation time budget. Evaluators were asked to spend no more than 20 minutes per project for automation rate evaluations and no more than 30 minutes per project for Elo evaluations. These times were selected based on preliminary testing and provided ample time for completing most evaluations. Elo evaluations involve inspecting two AI deliverables, and hence require more time. As shown in Figure[I 1] most evaluations finished in less than this amount of time, with a small number exceeding It. For current AI agents, evaluations are possible to complete relatively quickly, because AI deliverables often have glaring errors that are easy to spot. As AI deliverables become more complex and come closer to solving the projects in RLI, we expect that the time needed for evaluating each project will increase. B.4_ Evaluation Instructions Evaluator training materials. Before beginning evaluations, annotators were required to review detailed instructional videos and documents covering the evaluation workflow and common pitfalls. The training emphasized three core principles for evaluation: * Reasonable Client Perspective: We instructed annotators to judge each deliverable holisti- cally from the perspective of a reasonable client commissioning the project. This approach grounds quality assessments in the likely reception of the work in a professional context, minimizing the evaluators personal subjectivity.  Zone of Acceptable Error: The human reference deliverable establishes the baseline level and quality of work accepted by the original client. Annotators were instructed to view the reference within a zone of acceptable error - if the human deliverable contained minor flaws or was missing non-critical components, the AI deliverable was held to the same standard and not penalized for similar omissions. * Common AI Failure Modes: The training materials highlighted specific, common issues prevalent in AI-generated work. Examples included the use of rasterized image generation for projects explicitly requiring vector graphics, the inclusion of unreadable or nonsensical text in images, and a lack of spatial or visual consistency across different files within the same deliverable. The training detailed the standardized evaluation workflow: Annotators must first gain an understand- ing of the project by reading the brief and reviewing the reference deliverable. With this baseline established, they evaluate the AI deliverable(s) based on the requirements of the brief and the human reference. Annotators were allotted time limits for evaluation (20 minutes for Human vs. Model; 30 18 ==== Page 19 ==== minutes for Model vs. Model). However, they were instructed to stop early and fail the project if they identified a critical flaw that rendered the deliverable unusable. We iterated on these evaluation instructions and audited annotator quality until achieving an inter-annotator agreement of >85% ona random subset of the projects. Our final version of the instructions achieved 94.4% inter-annotator agreement. Automation Rate Evaluation Instructions. In the Automation Rate evaluation, evaluators assess the AI deliverable (alternative deliverable or AD) using the human deliverable as a reference for what successful project completion looks like (reference deliverable or RD). After reviewing the project materials according to the trained workflow, evaluators must provide a classification based on the following 3-point scale, accompanied by a written justification: 1. The alternative deliverable does not satisfy the brief as well as the reference deliverable or is of significantly lower quality, such that it would not be accepted by a reasonable client as the commissioned work. 2. The alternative deliverable satisfies the brief as well as the reference deliverable and would be accepted by a reasonable client as the commissioned work. 3. Same as 2, and the alternative deliverable exceeds the reference deliverable in overall quality. The automation rate is calculated based on the percentage of projects receiving a rating of 2 or 3. The distinction between equal (2) and superior (3) quality is maintained to facilitate Elo computations and may help provide greater clarity into the abilities of models near human parity in the future. Elo Score Evaluation Instructions. The Elo evaluation involves a pairwise comparison between two AI deliverables (AD-1 and AD-2). The evaluation platform displays both AI deliverables, along with the human deliverable as a reference for what successful project completion looks like. Annotators assess the comparison along two dimensions using separate 3-point scales: Project completion: 1. AD-1 is closer to satisfying the brief than AD-2, meaning AD-1 is closer to a state where it would be accepted by a reasonable client as the commissioned work. 2. AD-1 is equally close to satisfying the brief as AD-2, meaning both are equally close to a state where they would be accepted by a reasonable client as the commissioned work. 3. AD-2 is closer to satisfying the brief than AD-1, meaning AD-2 is closer to a state where it would be accepted by a reasonable client as the commissioned work. Overall quality: 1. AD-1 has higher overall quality for the project than AD-2. 2. AD-1 has the same overall quality for the project as AD-2. 3. AD-2 has higher overall quality for the project than AD-1. B.5 Evaluation verification To reduce the rate of false positives, we manually audited all annotation cases where the AI deliverable was labeled as good or better than the human deliverable. We were able to audit all of those cases since there were only a small number of these annotations. To get a false negative rate, two co-authors randomly sampled a Human vs Model pair from 50 random projects and did manual evaluation on those projects. We found no false negatives (cases where a annotators incorrectly labeled the human deliverable to be preferable). This gives us 100 U o @ 80 oO  60 LL 40 20 10-1 10 101 Cost (USD) Figure 12: The average cost of generating AI deliverables was $2.34. In all cases, models stopped generating before exceeding $30 of API costs.  Computer-use environment developed by Scale AI  OpenHands (CLI-based environment) For models that support computer-use (GPT-5, Sonnet 4.5), we default to our computer-use scaffold. For models not supporting computer-use (Grok 4, Gemini 2.5 Pro), we use OpenHands. In Appendix [A.3] we compare GPT-5 on both OpenHands and computer-use scaffolds. We refer to these as GPT-5 (CLI) and GPT-5 (CUA), respectively. In the main results, we report GPT-5S (CLI), as this outperformed GPT-5 (CUA). Available software and tools. To support the diverse multimedia requirements of RLI projects, we augmented the OpenHands environment and gave the models access to the following tools: gpt-image- 1 for image generation and editing, openai/tts-1 for speech generation, and veo-3.0-generate-preview for video generation. The agent scaffolding included standardized instructions for managing input and deliverable directories and explicitly directed agents to prioritize using these specialized tools over general LLM queries when the required functionality was available. Our computer use agent was deployed in remote Ubuntu VM instances via the Scrapybara API and given access to three tool categories through the Model Control Protocol (MCP): computer use tools for mouse, keyboard, and screenshot operations, file editor tools for viewing, creating, and modifying files with string replacement and line insertion capabilities, and bash tools for shell command execution with persistent session state. The agent scaffolding employed a modular action system that converted high-level tool calls into standardized operations executed by interface handlers. To generate artifacts, agents received the project brief and input files, were directed to save deliverables to designated directories, and operated within a session timeout (default: 1 hour), with execution traces capturing all actions and outputs for evaluation. Prompts. For all artifact generation setups with the exception of the computer use agent, we used the following base prompt: Read the brief attached and create only the deliverables described. Do not ask any questions. Complete the task and send a download link to the deliverables. You are done once all the deliverables are ready and the download link is sent. There may be auxiliary information necessary to complete the task that is provided in a zipped inputs folder. If this is provided, unzip the folder first and then proceed with completing the task. Make a zip file with all the deliverables. Additionally, we provided the models with specifications regarding the file types supported by the evaluation platform (see Appendix [B.7), which accommodates a comprehensive range of file formats with fallback rendering capabilities for unsupported types. For Manus and ChatGPT Agent, no further scaffolding instructions were added. 20 ==== Page 21 ==== For OpenHands, we extended the base prompt with instructions directing agents to use input/output directories and provided access to specialized multimedia tools: NOTE: You can explore ./inputs directory for extra information and reference material to execute the task. The folder might be empty, meaning that no further information is provided. IMPORTANT: Always save your final deliverables to the ./output directory. This directory has been created for you. Only put the requested deliverable output files in the ./output folder and no other extraneous files (eg. READMEs, etc.) . Each deliverable file must also have an appropriate extension (eg. .jpg, .png, -pdf, .csv, etc.). You can save your intermediate scripts or files to the ./ auxiliary directory but this is not required. SPECIALIZED TOOLS: The ./tools directory contains specialized tools you can use to complete your tasks. These include: - ?gpt-image-1: Image generation and editing - ?openai/tts-1: Speech generation - ?veo-3.0-generate-preview: Video generation You should absolutely use these tools if their functionality is needed to complete the task (instead of defaulting to general LLM query). Before using any tool, make sure to read its documentation and install any required dependencies. After execution, wait at least 300 seconds before killing the operation. For the computer use agent, we used the following prompt. Read the brief below and create only the deliverable described. Do not ask any questions. Try to work in /opt/workspace/ directory first, but if thats not accessible, work in the current directory. If you cannot find the inputs folder or get permission errors, call the navigate_to_workspace function first, then ensure_workspace_directories if needed. If you get Permission denied errors when saving files, call the fix_workspace_permissions function to resolve them. Complete the task, and make sure to submit all of the deliverables. You are done once all the deliverables are ready, and you have saved all deliverables to the Deliverables folder (either /opt/workspace/Deliverables/ or ./Deliverables/ depending on whats accessible). You are allowed to use temporary or auxiliary files, please save them in the auxiliary folder. Avoid long outputs when using bash, you can control the amount of output by using head or tail when using bash. For Claude Sonnet 4.5, we further extended the computer use agent prompt above with quality verification instructions to leverage the models visual reasoning capabilities. Based on best-use recommendations suggested by early users of Claude Sonnet 4.5, we also implemented context management exceeding 1M tokens and included explicit instructions to verify any output code or files and to avoid excessively writing thinking traces to files. No need to write too many text file notes to the filesystem, try to keep your thoughts / reasoning / insights in your context window. Also verify that any outputs you generate (intermediate or final) are of good quality by taking screenshots of files for visual inspection and checking any code for potential errors. B.7 Evaluation Platform Details The evaluation platform is a web-based multimedia viewer and file explorer. It provides native support for viewing the following file types:  Documents: 21 ==== Page 22 ====  Text: .txt, .json, .yml, .py, .js, .ts, .css, .java, .go, .php, .rb, .swift, .sql, .sh, and other common source code files. Any non-binary file not otherwise supported is displayed as text. Formatted: .md, .html, .pdf, .tex (LaTeX), and .ipynb (Jupyter Notebooks). Spreadsheets: .csv, .xls, .xlsx. Microsoft Office: .ppt, .pptx, .doc, .docx.  Media:  Images: . jpg, . jpeg, .png, .gif, .bmp, .webp, .svg, .ico, .avif, .tif, .tiff.  Video: .mp4, .m4v, .mkv, .webm, .mov, .avi, . wmv.  Audio: .mp3, .wav, .ogg, .aac, .m4a, .midi, .mid. * Design & 3D:  Design: . psd (with limited support for complex layer effects).  3D Models: .obj, .mtl, .stl, .gltf, .g1b.  Autodesk/CAD: .dwg, .dxf, .skp, .stp, .step, .ipt, .3dm, .3ds, .fbx, .rvt, .ifc, and other formats supported by the Autodesk Viewer. * Data & Interactive:  Databases: .sqlite, .db.  Websites/WebGL: Interactive builds with .htm1 entry points and associated . js and .cSs assets.  Anki: .apkg (limited to front and back card formats). The evaluation platform is fully open-source. Project-specific notes for evaluation. For some projects, we display short notes in a popup in the evaluation platform. These evaluator notes contain project-specific details of how the evaluation should be performed. For example, in some projects the human deliverable contains additional features that we exclude from the project brief. In these cases, we instruct the evaluator to ignore those parts of the human deliverable and emphasize that the AI deliverable should not include those features. Less than 20 projects have evaluator notes. C_ Dataset Details C.1 Categorization Upwork taxonomy. We categorize all projects using the Upwork job taxonomy. We used the version current at the time of this papers release, which contains 12 major categories and 64 subcategories of work. This taxonomy is detailed below. * Accounting and Consulting: Accounting & Bookkeeping, Financial Planning, Management Consulting & Analysis, Personal & Professional Coaching, Recruiting & Human Resources, Other - Accounting & Consulting  Admin Support: Data Entry & Transcription Services, Market Research & Product Reviews, Project Management, Virtual Assistance * Customer Service: Community Management & Tagging, Customer Service & Tech Support * Data Science and Analytics: AI & Machine Learning, Data Analysis & Testing, Data Extraction/ETL, Data Mining & Management * Design and Creative: Art & Illustration, Audio & Music Production, Branding & Logo Design, Graphic, Editorial & Presentation Design, NFT, AR/VR & Game Art, Performing Arts, Photography, Product Design, Video & Animation * Engineering and Architecture: 3D Modeling & CAD, Building & Landscape Architecture, Chemical Engineering, Civil & Structural Engineering, Contract Manufacturing, Electrical & Electronic Engineering, Energy & Mechanical Engineering, Interior & Trade Show Design, Physical Sciences 22 ==== Page 23 ==== Task Brief Hide Brief Files taka B brief.md yD inputs a & ringdetails.png Work description Create a modified version of the provided ring design by replacing the emerald-cut diamond with a marquise-cut diamond. Using the reference photo, render images of the ring with this new diamond shape while maintaining all other design elements. Provided material Reference image showing the original ring with emerald-cut diamond, including weight, size, and other specifications ( inputs/ringdetails.png ) Deliverables 1. 3D model file with marquise-cut diamond (.3dm format) 2. JPEG Images (minimum 1280x1280 pixels resolution): * Rose gold version, front view (.jpg format) * Rose gold version, side angle view (.jpg format) * Yellow gold version, front view (.jpg format) * Yellow gold version, side angle view (.jpg format) Reference deliverables Alternative deliverables @) R15 MRQ CUT.3dm @) ring_with_marquise.3dm  R15_a1_rg1.2209.jpg @ rose_gold_front.jpg  R15_a1_rg3.2209.jog @ rose_gold_side.jpg  R15_a1_yg1.2209.jpg @ test.3dm @ R15_a1_yg3.2209.jpg B test.txt @ yellow_gold_front.jpg @ yellow_gold_side.jpg R15 MRQ CUT.3dm B) ting_with_marquise.3dm Figure 13: Evaluation platform view with the ring 3D model project example.  IT and Networking: Database Management & Administration, DevOps & Solution Ar- chitecture, ERP/CRM Software, Information Security & Compliance, Network & System Administration  Legal: Corporate & Contract Law, Finance & Tax Law, International & Immigration Law, Public Law  Sales and Marketing: Digital Marketing, Lead Generation & Telemarketing, Marketing, PR & Brand Strategy  Translation: Language Tutoring & Interpretation, Translation & Localization Services  Web, Mobile, and Software Development: AI Apps & Integration, Blockchain, NFT & Cryptocurrency, Desktop Application Development, Ecommerce Development, Game Design & Development, Mobile Development, Product Management & Scrum, QA Testing, Scripts & Utilities, Web & Mobile Design, Web Development, Other - Software Develop- ment  Writing: Content Writing, Editing & Proofreading Services, Professional & Business Writing, Sales & Marketing Copywriting Our final dataset includes projects from 9 major categories and 23 subcategories. In Figure[3| we show the distribution across subcategories. For brevity, we use the following short-form names in the figure: Video for Video & Animation, CAD for 3D Modeling & CAD, Graphic Design for 23 ==== Page 24 ==== Average Number of Files Total Unique Filetypes wn 4 15> mmm GDPval 5 80+ mmm GDPval oO >  Mas RLI o Mas RLI a i L 60 2 1045 g (") s io) 40 = 5 LL oD) 54 5) [e)) ro f 5 20 Z 5 04 = 0 Inputs Human Inputs Human Deliverable Deliverable Figure 14: RLI projects involve significantly more diverse file types than previous comparable benchmarks. Left: Average number of files per project for inputs and human deliverables across benchmarks. Right: Total unique file types found in inputs and human deliverables across benchmarks.  Graphic, Editorial & Presentation Design, Game Dev for Game Design & Development, Audio for Audio & Music Production, and Architecture for Building & Landscape Architecture. To better reflect the diversity of projects, we separate out music composition projects into their own subcategory for the figure, as music composition differs considerably from other projects in Audio & Music Production. Music composition projects make up roughly 6% of the benchmark. Further subdivisions of this nature are possible, as most subcategories in the Upwork taxonomy consist of multiple distinct types of work, but for consistency we use the unmodified Upwork taxonomy for all other discussion in the paper. Most of our analysis focuses on the subcategories in the Upwork taxonomy, so for brevity, we refer to these as categories in other parts of the paper. O*NET taxonomy. The O*NET database provides a widely used taxonomy of occupational requirements and work activities within the US labor market. While valuable for capturing activities performed in long-term occupations, it is not tailored to end-to-end freelance labor markets like Upwork, making it unsuitable for classifying RLI projects and estimating coverage. This limitation stems from O*NETs structure at both the activity and occupational levels. To categorize a broad range of work, O*NET relies on an abstract hierarchy of Work Activities. Even the most granular taxonomy in O*NET, Detailed Work Activities (DWAs), does not provide meaningful granularity for measuring task breadth. The DWA taxonomy includes many ubiquitous and generic items such as Retrieve information from electronic sources, and Read materials to determine needed actions, [19], and coverage of these DWAs does not indicate meaningful coverage of remote work task types. At the occupational level, O*NET classifications are designed to describe the broad, ongoing responsibilities of long-term workers. This structure does not align with the delivery of specific, self-contained freelance projects. For this reason, we use the Upwork taxonomy of remote freelance labor for coverage analysis, since this taxonomy is designed for categorizing freelance work. C.2 Filtering & Cleaning Criteria Project sourcing criteria. To enable building a high-quality standardized benchmark, we hired freelancers from categories on Upwork that met the following criteria: 1. Remote work: It must be possible to complete projects without any physical labor (e.g., no local photography). 2. No open-ended jobs: Most jobs in the category must be end-to-end projects that can be performed, not open-ended long-term contractor roles. 24 ==== Page 25 ==== 3. Can be completed independently: The work can be completed independently by one freelancer and does not inherently require working on a team. 4. Does not require interaction with client: The work does not inherently require interacting with clients (e.g., no tutoring). 5. Does not require interaction with client services: The work does not require testing or interacting with live services set up by the client (e.g., no QA testing of client websites). 6. No scraping without permission: The work does not involve scraping information from low-traffic websites or websites where bots are expressly forbidden. 7. Can be evaluated on the spot: Some categories of work inherently require time to evaluate work outputs (e.g., SEO). These categories were excluded, ensuring that all projects can be evaluated on the spot. Note: This restriction does not apply to projects where evaluations take a long time but can still be performed on the spot. 8. Excluding certain categories: Many projects in the Content Writing category can already be solved by Als and would not provide much information to include. Thus, this category and related categories were excluded. (Note: These are category-level exclusions; individual projects from other categories were not excluded based on whether current models solved them.) Most legal categories were excluded due to PII concerns. 9. Renderability: Deliverables must be possible to view in a web-based evaluation platform (e.g., no desktop application development). Based on these criteria, we entirely excluded projects from the following categories on Upwork during our initial project collection: Personal & Professional Coaching; Recruiting & Human Resources; Project Management; Com- munity Management & Tagging; Customer Service & Tech Support; Performing Arts; Photography; International & Immigration Law; Public Law; Digital Marketing; Marketing, PR & Brand Strategy; Desktop Application Development; Mobile Development; Product Management & Scrum; QA Testing; Content Writing; Professional & Business Writing; and Sales & Marketing Copywriting. This left us with 45 total Upwork categories to source projects from. These sourcing criteria were also applied during long tail project collection. Data cleaning and filtering. After receiving raw data, we conducted an extensive process of cleaning and filtering to ensure that all projects in the dataset met the following criteria: 1. Completeness: The brief and input files are complete and sufficient, with no additional external information needed to complete the project. 2. Anonymization: The input files and deliverables do not include sensitive personal informa- tion pertaining to the client. Client faces were blurred out, and company names and logos were replaced with fake alternatives that preserve the realism of projects. 3. Human deliverable completes the project: The gold-standard human deliverable success- fully completes the project, such that a reasonable client would accept it as the commissioned work. Note: The majority of projects in RLI were paid for by clients, so this is often guaran- teed by default. 4. File quality: Input files are high-quality. E.g., if the raw data for projects sourced from freelancers includes low-resolution images or screenshots, we request higher-quality replace- ments from freelancers. 5. Faithful to the raw data: For projects sourced from freelancers, we ensure that the cleaned projects are as faithful as possible to the raw data sent by the freelancers, using similar or identical phrasing to original client requests where possible. 6. Standardized structure: All projects are standardized to have briefs with three top-level sections: Work description describing the work to be done, Provided material describing the auxiliary project inputs, and Deliverables describing the expected deliverables. 7. Renderability: We ensure that all inputs and human deliverables are viewable in the evaluation platform. We convert unsupported formats to supported ones (e.g., AI to layered PDF) and exclude projects that cannot be supported. This often required improving the capabilities of the evaluation platform to accommodate projects with new file types. 25 ==== Page 26 ==== After the cleaning and filtering process, the dataset contains 240 projects from the following 23 Upwork subcategories: Video & Animation, 3D Modeling & CAD, Graphic & Editorial Design, Audio & Music Production, Building & Landscape Architecture, Product Design, NFT, AR/VR & Game Art, Art & Illustration, Interior & Trade Show Design, Web Development, Branding & Logo Design, Game Design & Development, Management Consulting & Analysis, Data Entry & Transcription Services, Data Analysis & Testing, Language Tutoring & Interpretation, Data Extraction/ETL, Presentation Design, Web & Mobile Design, Corporate & Contract Law, Translation & Localization Services, Market Research & Product Reviews. C.3 Analysis Details Completion time comparison. In Figure[6] we extracted completion time data from the papers for GDPval and HCAST [26]. To determine the average completion time and cost for Upwork projects, we analyzed 275 completed jobs from a random sample of 60 freelancers, using the hours worked and dollars earned for each job. Project type comparison. In Figure|6| we computed the distribution over project types for RLI and GDPval by using a judge LLM to classify the project briefs using the following instructions. Classify this task into one of three categories: 1. Software engineering / coding 2. Research and writing 3. Other A task should be classified as category 1 or 2 if the actual work primarily involves these skills, such that with sufficient knowledge one could solve the task by just using these skills. Examples of category 1: - Front-end development - Game development - Website creation Examples of category 2: - Reading PDFs and writing a report - Searching for information online and writing a report - Writing a blog post about a historical event Examples of category 3: - Performing research, running simulations, and writing a report - Making an as-built drawing of a building - Creating an educational video - QA testing for a video game and writing a bug report (involves playing the game ) For HCAST, we manually classify the task distribution shown in Table 1 of the HCAST paper [26]. For an estimate of the Upwork distribution, we apply the above prompt to the category names in the Upwork taxonomy. This provides a distribution over the different types of work performed on Upwork. Note: This is not a distribution at the job-level, which is more skewed toward software tasks. C.4 Data Collection Details 1. For projects sourced from freelancers, we only included projects where freelancers explicitly verified that they had the rights to sell us the work. 2. In cases where the work contains PII or copyrighted content (e.g., logos or company names), we anonymized the project by redacting information. In some cases, redacted information was replaced with synthetic details (e.g., fake company names or logos). 26 ==== Page 27 ==== Project Cost vs Completion Time Correlation (log-log): 0.785]  4 10 e 8 e e     ey  e e  @ } _~ 103 oo m6 e ae e A e ee e383 e ~~ 8 og OO a) i  BB 8 08   coo 8 %% Bee o  O e  eo awe eo ggece woo  agece  0 @ %0 102   og eecce cog  e  Ss P98 Pg  oS eo e   @ @ ry ) @ee e808 @ 8 e e ee e e e   101   10 107 10? Completion Time (hours) Figure 15: Project cost and completion time are highly correlated on a log-log scale. 3. For long-tail project collection, we either purchased the work or received permission from the original author of the work to link to it in our study. C.5 Project cost and completion time. Collecting cost and completion time. For the vast majority of projects, the human professionals who created the human deliverable provided the cost and completion time for the project. These metrics were operationalized as follows: * Cost: The amount of money in USD earned by the freelancer for completing the project, or a fair price estimated by the professional for recreating the work from scratch. Human professionals self-reported these values. Since these often represent the actual amount of money paid by a client, they provide an accurate measure of the cost of the project. * Completion time: The amount of time in hours that it took human professionals to complete the projects. These values were also self-reported to ensure economic accuracy. In some cases, human professionals communicated a range of times or costs; in these instances, we took the midpoint value. Costs are available for 95% of projects. Completion times are available for 84% of projects. 5% of projects have neither cost nor completion time data, but were kept in the dataset due to being high-quality. For experiments or metrics using this data, we drop projects for which the required values are not available. Distribution over project cost and completion time. In Figure|4| we show the distributions over project cost and completion time. Both variables are roughly log-normal distributed, with project cost and completion time reaching up to $22,500 and 450 hours. Individual numbers are often rounded by freelancers who self-report the data, and fixed price projects tend to cluster at whole-number values, explaining peaks in the data. In Figure[15] we plot these variables against each other on a log-log scale for projects where both values are available. We observe a Pearson correlation of 0.785. C.6 AI Deliverable Examples 27 ==== Page 28 ==== Example of Successful Project Completion Project Brief Create two fun, Halloween-themed Facebook ads that weave in the provided recipe images and clearly feature the copy: SPOOKTACULAR SALE, 20% off site wide, and Coupon Code: SPOOKY20, using playful seasonal visuals to highlight the dishes and the promotion. Human Deliverable Jf Al Deliverable 330% OFF ay T Ta). 207 4. OFF 20% y SITEWIDE el og 4 . Fa i  fay Coupon Code: x epoOKTACUL Ap Figure 16: AI agents leverage image generation tools to solve some marketing projects in RLI. Here we show a successful project completion from Manus. 28 ==== Page 29 ==== Examples of Unsuccessful Project Completion Project Brief Inputs Produce a ~60-second, 2D flat-design explainer educating viewers on trimming, pruning, stump removal, and tree health. Use bold typography, a natural palette, icon-driven graphics, subtle character i i] | i [ | | it 1] i animation, and smooth modern transitions. Pair with the supplied voiceover. Human Deliverable X Al Deliverable Project Brief Produce five short, high-quality 3D product demo animations that clearly showcase the earbuds silicone tips, swappable battery stem, sleek charging case. The clips should be polished and visually consistent, with smooth camera moves and lighting that emphasizes materials, fit, and the replaceable battery mechanism. Human Deliverable X AI Deliverable R Sleek design * Comfortable silicone tips * Portable case Figure 17: Agents fail to successfully complete the vast majority of RLI projects. Here we show failed projects for Gemini 2.5 Pro (top) and GPT-5 (bottom). 29 ==== Page 30 ==== C.7 Detailed Project Examples i  a  Samed Example 1: Animated 3D Product Demonstration of Earbuds Work description We need high-quality animations to showcase the features of a new earbuds design and the case. Create high-quality 3D product demonstration videos that effectively communicates the key features and benefits of the earbuds. We need 5 short, engaging animations to be used in marketing materials. The key features are: * Silicone, airpod-like tips  Stem of earbud swaps out for a replaceable battery  Sleek charging case  L/R indicator decal Input Files Provided material Earbuds image in inputs/earbuds_back.jpg* Earbuds image in inputs/earbuds_front.jpg* Earbuds image in inputs/earbuds_top.jpg* Image demonstrating replaceable battery functionality in inputs/replaceable_battery.jpg* Image of portable charging case in inputs/charging_case.jpg* Deliverables Deliverables * Five short clips showcasing the different features of the earbuds (MP4 format) * 3D models for the earbuds and case (e.g., .fox format) Example 2: Interior Design and Furniture Layout Work description Please design the following: Bathroom: 3 interior design options for the existing bathroom (wall-hung WC in the indicated location). Apartment: 6 furniture layout options; pick one final option for detailed plans. Cadastral notation is "room no. / gross area (meters squared)". Rooms in cadastral plan: Rooms 27, 28, 29: habitable rooms Room 26: kitchen Room 26a: living room Room 26b: veranda Room 25: bathroom Room 24: hallway There is a door from the living room to the veranda, as shown in inputs/additional measurements.jpg> Dimensions in deliverables are design intent; contractor to verify all on site. Input Files Provided material + Cadastral floor plan (metric): inputs/cadastral floor plan.jpg* + Zoomed bathroom plan: inputs/bathroom.jpg* + Site photos: inputs/bathroom_photos/photo_#_y.jpg~ vee enp sp Furie layout plan Open + Additional measurements of the bathroom, living room, and aie veranda: inputs/additional measurements.jpg Deliverables  Bathroom interior design - 3 options: eo Renders: At least 3 views per option, at least 1200 pixels on long edge. Include one render from the top. (JPG) o Material board: one combined sheet per option showing the renders + finish swatches eo Wall finish images: high-res JPGs of each finish used. o 3D source: supply native file (e.g., .skp/.3ds/.max/.blend) plus an interchange file (.fbx or .obj) with textures.  Furniture layouts - 6 options: o One PDF floor plan per option, imperial dimensions (feet- inches) for key clearances and furniture sizes. eo One consolidated DWG containing all options.  Final chosen furniture option; extra plans: eo RCP & lighting plan: show ceiling levels, fixture symbols, mounting heights, and a legend (PDF) e Toilet installation plan: horizontal dimensions in imperial units and outline the plasterboard boxing; no further details required (PDF) e Electrical equipment layout: outlets, switches, appliance points, mounting heights, legend (circuiting by electrician) (PDF) e Floor finishes plan: hatch/legend showing material zones and transition/threshold locations (PDF) + CAD trace of cadastral plan: oe Provide a clean DWG + PDF. Trace to scale, align walls, doors, windows Deliverables Figure 18: Detailed project examples with extended briefs. 30 ==== Page 31 ==== Example 3: Mega-Merge Web Game Deliverables Work description Create a casual, web-based game called "Mega Merge" where players combine falling objects to reach the highest-level item possible. The game should be inspired by the popular Watermelon Game but incorporate unique mechanics and features. It should be designed for accessibility and smooth play on any device, with a responsive layout suitable for both desktop and mobile play. Objective Players will aim to combine objects and score as many points as possible before the box fills up. By merging identical items, players will create higher-level items and work towards unlocking the ultimate object. The goal is to manage space strategically while maximizing the score. Key Features * Platform: Web-based, compatible with all major browsers (Chrome, Safari, Firefox, Edge). + Cross-Platform Compatibility: Works seamlessly on desktop and mobile (iOS and Android) with responsive layouts. + Controls: Supports both touch gestures (tap, swipe) and mouse clicks for flexible gameplay. + Instant Playability: No downloads required; players can start immediately by opening the game in their browser. Technical Requirements + Physics and Collisions: Objects should obey basic physics. They should fall naturally within a defined "box" and exhibit slight bounce effects when landing or colliding with each other. This behavior can be achieved with a physics engine like matter,js or through Construct 3's physics behavior. * File Size: The total file size should be kept under 5 MB to ensure fast loading. Gameplay Mechanics * Object Merging: Players combine matching items to generate higher-level objects, aiming to reach the ultimate item. Visual & UI Design + Container Box: Objects should fall into a clearly defined "box" area with visible boundaries, guiding the player's actions. + Falling Indicator: The next item should have an indicator at the bottom of the screen to show where it will fall, helping players plan their moves. + Score Display: The score should be displayed prominently at the top of the screen. + Minimalist Ul: Essential elements onlyscore display, "Next Item" preview, and basic pause/reset buttons at the top of the screen. Audio and Sound Design * Background Music: The game should include relaxing background music that plays continuously during gameplay to create a calm, enjoyable atmosphere. + Sound Effects:  A satisfying sound effect should play when objects are dropped into the container.  A distinct, gratifying merging sound should play when two objects combine.  These sounds should enhance the feedback of each action, creating an engaging and satisfying player experience. Interaction and Controls * Touch and Mouse Support: The game should support both touch gestures and mouse input to provide a smooth experience on both mobile and desktop platforms. Your Brewing Results Provided material OebeS None Deliverables Game Files: All files (HTML, CSS, JavaScript, images, and audio files) should be organized in a clear folder structure, with folders for assets, icons, images, scripts, styles, and sounds. Interactive Video Game for the Web; Built with Unity Create a Unity WebGL video game with planets and weapons. Polished UI, weapon glow, audio. Provide commented code, README, tested build, and simple HTML embed. Real Freelancer Deliverable: Digital Assets Unity Build Example 4: Interactive Dashboard for the World Happiness Index Build an intuitive, self-hosted interactive dashboard that lets visitors explore why some countries score higher than others in the World Happiness Report. Input Files Requirements * Overview: The dashboard should include an overview map showing each country's overall happiness score. Data: use the provided data as the sole source for country scores and component metrics. Map: display each country shaded on a gradient that reflects its overall happiness score; add hover and click interactions that surface the country name and exact value. Detailed chart: place a second visual (e.g., stacked bar or spider chart) beside or beneath the map. This chart should be linked to the map, so when the reader interacts with one country on the map, the same country in the second chart is highlighted. Design: intuitive, user-friendly, and align with the theme of happiness. Provided material Happiness data for the dashboard in inputs/DataForFigure2.1WHR2021C2.x\s*. Deliverables Deliverables A complete, self-contained dashboard package (HTML, CSS, JavaScript, and any required libraries). Figure 19: Detailed project examples with extended briefs. 31

==== Page 1 ==== [\ LongCat AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Shengnan An*, Xunliang Cai, Xuezhi Cao*, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang', Ziwen Wang, Shuang Zhou (Alphabetical order by last name) Meituan University of Chinese Academy of Sciences Harbin Institute of Technology ABSTRACT We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO- Bench to facilitate further research into advancing the reasoning abilities of language models. Code, Dataset, and Leaderboard: amo-bench.github.io Models mmm LongCat-Flash-Thinking 6 05 9 98.8.99.2 Mmm GLM-4.5 ME Qwen3-235B-A22B-Thinking-2507 Ml Gemini-2.5-Pro mmm DeepSeek-V3.1-Thinking mmm GPT-5-Thinking (High) 40 20 AMO-Bench(Ours) HMMT25 AIME25 AIME24 MATH500 pan ~ foe) Ne) fo) fo) fo) [o) [o) arXiv:2510.26768v1 [cs.CL] 30 Oct 2025 Accuracy (%) u fo) Figure 1: Performance of top-tier reasoning models on AMO-Bench as well as existing competition-level math benchmarks. Except for the results on AMO-Bench, all other results are sourced from Meituan LongCat Team [2025a]. * Correspondence to: {anshengnan, caoxuezhi }@meituan. com. tT Work done during the internship at Meituan. ==== Page 2 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat 1 Introduction Recent advances in large language models (LLMs) have demonstrated significant improvements in reasoning capabili- ties [OpenAI, 2024, Gemini Team, 2025, OpenAI, 2025, Anthropic, 2025, xAI, 2025, Yang et al., 2025, Guo et al., 2025, DeepSeek-AI, 2025, Meituan LongCat Team, 2025b, GLM-4.5 Team, 2025, ByteDance Seed, 2025, Tencent Hunyuan Team, 2025, Kimi Team, 2025, Meituan LongCat Team, 2025a]. To track this rapid progress, mathematical problem solving has become a critical metric for evaluation, as it inherently demands complex and multi-step reasoning processes to arrive at correct answers. As a result, many current benchmarks utilize problems from high school mathematics competitions (e.g., HMMT and AIME) to assess the reasoning abilities of LLMs [Balunovi et al., 2025, He et al., 2024, Gao et al., 2024, Fang et al., 2025]. Recent results indicate that state-of-the-art models are achieving remarkable performances on these benchmarks, with some even surpassing 90% accuracy on competitions like AIME24/25. However, these impressive results also expose an emerging challenge: many existing mathematics benchmarks are approaching performance saturation and are becoming less effective for assessing further advancements in reasoning capabilities. On the one hand, as LLMs gradually approach or even surpass human-level capabilities in mathematics, some math competitions are becoming less challenging for top-tier models [OpenAI, 2025, DeepSeek-AI, 2025, Yang et al., 2025, Meituan LongCat Team, 2025a]. On the other hand, most current benchmarks are derived from previous competitions, raising concerns about potential data memorization and performance leakage [Sun et al., 2025, Balunovi et al., 2025]. While recent efforts have incorporated problems from more difficult and newly held contests such as the International Mathematical Olympiad (IMO), these questions tend to be proof-based and require manual verification by experts [Balunovi et al., 2025, Petrov et al., 2025]. This reliance on expert review hinders the implementation of automated scoring processes, leading to inefficiency and inconsistency in large-scale evaluations and result reproductions. To address these limitations, we present AMO-Bench, an advanced mathematical reasoning benchmark consisting of 50 novel and extremely challenging problems. The core features of AMO-Bench are as follows: Original problems. To prevent performance leaks from existing resources as much as possible, all problems in AMO-Bench are newly crafted by human experts. Moreover, we conduct a secondary verification to ensure that there are no highly similar problems in existing competitions or online resources. Guaranteed difficulty. Each problem has undergone rigorous cross-validation by multiple experts to ensure it meets at least the difficulty standards of IMO. We also incorporate an LLM-based difficulty filtering stage to exclude questions that do not present sufficient challenge to current reasoning models. Final-answer based grading. Each problem in AMO-Bench requires a final answer rather than a full proof, enabling efficient automatic grading. For each problem, we employ a parser-based or LLM-based grading method according to its answer type, balancing the grading cost and generalizability. Human-annotated reasoning paths. In addition to the final answer, each problem also includes a detailed reasoning path written by human experts. These additional annotations enhance solution transparency and could support further explorations on AMO-Bench, such as prompt engineering and error analysis. Experimental results across various LLMs demonstrate that contemporary LLMs still struggle with the significant challenges presented by AMO-Bench. Among 26 evaluated models, the state-of-the-art accuracy on AMO-Bench is only 52.4%, achieved by GPT-5-Thinking (High), with most models scoring below 40%. Figure | illustrates the performance of several leading models on AMO-Bench as well as the comparison with other mathematical benchmarks. Beyond their limited final performances on AMO-Bench, LLMs consume substantially more output tokens in AMO-Bench compared to existing evaluation datasets. For example, GPT-5-Thinking (High) generates an average of approximately 37K output tokens for AMO-Bench, whereas it produces only about 7K and 6K tokens for AIME25 and AIME?24, respectively. This exceptionally high token consumption further underscores the difficulty of AMO-Benrch for current LLMs. Despite the poor performances of current LLMs, our analysis also reveals considerable potential for further improvements. Notably, top-tier models achieve pass @32 rates exceeding 70%, suggesting they possess the initial capability to solve these challenging problems even if they do not consistently identify the correct reasoning path at present. Furthermore, we show that the model performances exhibit a near-linear growth trend relative to the logarithm of output length, indicating continued benefits from test-time scaling. These analyses suggest substantial opportunities remain to enhance reasoning capabilities in future generations of language models. The data and evaluation code of AMO-Bench are publicly available at amo-bench. github. io. We hope this novel and challenging benchmark will facilitate further research into advancing the reasoning abilities of language models. ==== Page 3 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat Problem: Let x;, x2, ..., X2024 be positive real numbers such that xz +m = km... Solution: From the assumption: for any positive integer 1 => => ce Web Search GS ito df LLM-Based Performance Quality Review Originality Review Difficulty Review Grading Method Problem: Find all positive integers n such that Human Experts for any: dy>n-1>...> a2 2>a,>0... Solution: Forn = 1, it clearly holds. Forn = 2, we have aya; = 1, 50a; -a? = ap... Final Answer: {1, 2, 3} Data Creation Figure 2: The construction and grading pipeline of AMO-Bench. 2 AMO-Bench In this section, we first introduce the construction process of AMO-Bench (Section 2.1) and present the basic statistics of this dataset (Section 2.2). Then, we elaborate on the grading methodology designed for AMO-Bench (Section 2.3). Figure 2 briefly illustrate the construction and grading pipeline of AMO-Bench. 2.1 Construction Pipeline To ensure the high standards of quality, originality, and difficulty level in our dataset, we have built up a comprehensive multi-stage construction pipeline that covers the entire process from question creation to final inclusion. This pipeline comprises four major stages: data creation, quality review, originality review, and difficulty review. Data creation. All problems are independently designed by mathematics experts from top universities and educational institutions. These experts have extensive backgrounds in high school mathematics competitions, either having won MO-level mathematics competition awards or possessing experience in competition problem design. Beyond the final answer, each problem author must provide a detailed step-by-step solution. These annotated solutions will be utilized in the subsequent quality review stage and will also aid in assessing the overall difficulty of AMO-Bench (see Section 2.2 for details). Quality review. Each candidate problem undergoes blind review by at least three experts to assess its quality. This quality review stage focuses primarily on two aspects:  Whether the problem statement and solution are semantically unambiguous and logically correct.  Whether the mathematical knowledge required for the problem is within the scope typically covered in MO-level competitions such as IMO. Originality review. The originality review stage aims to ensure that these newly created problems are not mere rewrites of publicly available materials, but demonstrate genuine originality. To this end, we assess the originality of each problem through the following methods:  Compare it against problems in existing datasets (e.g., AIME24/25) with 10-gram matching. * Conduct web searches to identify any similar online content. Additionally, during the quality review stage, experts are also required to indicate whether they have encountered highly similar questions in past competitions. Difficulty review. To ensure that AMO-Bench presents a sufficient challenge to state-of-the-art LLMs, we implement a difficulty review stage to filter out problems lacking adequate complexity (even if they may be suitable for some MO-level competitions, e.g., the first 10 questions in AIME). Specifically, each selected problem must satisfy the following two criteria:  The problem must meet or exceed the IMO difficulty standards, as verified by the human expert.  We employed multiple advanced reasoning models (such as GPT, DeepSeek, and Gemini series models) for prelimi- nary evaluation, requiring that at least two such models fail to correctly and consistently solve the problem. 3For each model, our preliminary evaluation involves three samples. If all three samples are correct, the model is deemed capable of consistently solving the problem. ==== Page 4 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat 100 5 lm ~MATH500 93% Mmm AIME2024 Geometry Mmm = AMO-Bench 10% ~ 804 S Functions &  Sequences Fo 26% x 604  Number Theory  18% 2 g c 40% 40% o 407 Algebraic Equations 3 26% & Inequalities Combinatorics 20 22% 24% | o 10% 12% 3% 0! 0% 0% poe 0% 0% 2% 4,096 (a) Distribution of problem categories. (b) Comparison of solution lengths. Figure 3: Basic statistics of AMO-Bench. (a) The distribution of problem categories in AMO-Bench. (b) The distribution of human-annotated solutions in AMO-Bench as well as the comparison with MATH500 and AIME24. 2.2 Dataset Statistics Problem categories. Referring several official competition syllabus, we categorize the 50 problems of AMO-Bench into the following five primary categories: Algebraic Equations & Inequalities (11/50), Functions & Sequences (13/50), Geometry (5/50), Number Theory (9/50), and Combinatorics (12/50). Figure 3a show the overall distribution of problem categories in AMO-Bench. Length distribution of human-annotated solutions. Since the problems in our AMO-Bench are equipped with manually annotated solutions, we can preliminarily analyze the reasoning complexity of these problems from the view of solution length. We measure solution length in terms of token count*. Additionally, we compare the distribution of solution lengths with those from AIME24 and MATH500. Figure 3b illustrates the solution length distributions across these benchmarks. It reveals that solutions in AMO-Bench exhibit significantly higher lengths, indicating that problems in this benchmark are inherently more challenging and require more complex reasoning to arrive at the final answer. We conduct a further analysis of the model solution lengths in Section 3.2. 2.3 Grading Method For evaluating answers generated by LLMs, prior work has primarily utilized two approaches: parser-based grading and LLM-based grading. Parser-based grading offers high efficiency and accuracy when the models response can be successfully parsed; however, its applicability is limited to simple answer formats such as numerical values or sets, making it challenging to assess more complex answers. In contrast, LLM-based grading provides greater flexibility across diverse answer types but may be less efficient and does not consistently guarantee accuracy. To fully leverage the strengths of both grading methods, AMO-Bench employs different grading approaches based on the specific answer type for each problem. Specifically, problems in AMO-Bench are divided into four main answer types: numerical answers (e.g., Example 1), set answers (e.g., Example 2), variable-expression answers (e.g., Example 3 which requires providing the general formula for an arithmetic sequence), and descriptive answers (e.g., Example 4 which involves comprehensively considering multiple scenarios). The prompt templates for used for grading are contained in Appendix A. Example 1: Problem with Numerical Answer Question: Let x1, %2,--- , 2024 be positive real numbers such that 7, + 2%, > km for any 1 @n_1 > Gn_2 > ag > a, > 0, satisfying n n n > ay = SS 4, the inequality [] af > 1 holds. k=l k=l" k=1 Answer: | {1, 2,3} Example 3: Problem with Variable-Expression Answer Question: The sequence {a,,}2, consists of positive terms, with a, = 7, a2 = 2, and satisfies the recurrence relation 80,49 =344angi tan (ne N*). Find the general term formula for this sequence. Answer: (2+ V3) "+ (2- V3)?" 2 Example 4: Problem with Descriptive Answer Question: Let n be an integer with n > 2. Real numbers aj, a2,..., Gy, satisfy Say =2n, 52k lag| = dn. k=1 k=1 Find the minimum value of a? + a3 +--+ a2. Answer: For n = 3, the minimum of a? + a3 + a3 is 12. >. 6n? nis . For n > 4, the minimum of a? + a3 bee+ +4 For problems requiring numerical, set, or variable-expression answers (39 out of 50), we employ the parser-based grading. The evaluated LLMs are instructed to format their final responses as \boxed{ }. We then utilize the tools provided by math-verify to parse these answers and verify the equivalence with the ground truth. Moreover, if the model answer containing decimal values, we require an accuracy of at least four decimal places. For variable- expression answers, we assign multiple sets of values to the variables in the expression, then verify whether the values of the generated expression match that of the ground-truth expression. We also manually review the parsing results during the preliminary evaluation and adjust the post-processing algorithms. For problems requiring descriptive answers (11 out of 50), we use LLM-based grading with 04-mini (Low) serving as the grading model. To ensure robust assessment, majority voting is performed across five independent grading samples for each response. Additionally, during preliminary evaluation, we manually verify the correctness of LLM-based grades for all descriptive answers and revise answer descriptions where needed to enhance grading accuracy. Grading accuracy. Prior to conducting the large-scale evaluation, we performed a manual quality check to ensure the reliability of the designed grading method. This assessment included 1,000 responses generated by 10 different LLMs. The results indicate that the grading accuracy reached 99.2%, providing strong validation for the effectiveness of the grading method on AMO-Bench. 3 Experiments In this section, we present the experimental results on AMO-Bench. We first describe the experimental setup (Section 3.1), followed by a discussion of the main results and analysis (Section 3.2). Thttps://github.com/huggingface/Math- Verify. ==== Page 6 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat lm Proprietary Models lm Open Source Models Reasoning Models Non-Reasoning Models 50 4 47.8 47.6 473 40 4 S ~ 304 m  $ a 20 7 18.[REDACTED] Y 14.6 13.1 . Yj Y [REDACTED] gg 104 Yj Yj 75 Y 5.2 41 A\Y LO Dae: 04 lA LL. LAVA era ds A  (Om   9  nm YY PO FE YP EF A So} * By WW om LY Af es Pe cS we we rs s ws oy et cS eS ss 3S & Rg st PS Ry ov a ww oe RS) aw ee aw 3s Ria ~S xX Ss oe o a? of & & e & & es we & Roe is S : SS WS ? EN KS OW AV gd AS g CF OF ES SX oF SW KS LK KS oF S FS & i SE OO WS OS e PF PLP ME LS PS ge 2 x bed ae eo we oe we & & & Pra ww & Cd oe & oY & Pri aS & oe ree  6 Vv os ow & Ry [o> Figure 4: The AVG @32 performance of various LLMs on AMO-Bench. 3.1 Experimental Setup Models. To conduct a comprehensive and representative evaluation on AMO-Bench, we select a diverse set of leading LLMs, encompassing both open-source models and proprietary models. Specifically, the evaluation includes top-tier models provided by OpenAI [OpenAI, 2025], Gemini [Gemini Team, 2025], Anthropic [Anthropic, 2025], DeepSeek [Guo et al., 2025], Qwen [Yang et al., 2025], GLM [GLM-4.5 Team, 2025], Moonshot [Kimi Team, 2025], and LongCat [Meituan LongCat Team, 2025a]. In addition to evaluating reasoning models that have been specifically enhanced for long-term thinking tasks, we also incorporated several powerful non-reasoning models to demonstrate their potential in tackling complex reasoning challenges. Sampling settings. We set the temperature of sampling to 1.0 for reasoning models and 0.7 for non-reasoning models. For all evaluated models, we use top-k=50 and top-p=0.95 during sampling. We configure the maximum context/output length to the highest allowable limit for each model during inference. This avoids underestimating the reasoning capabilities of the model due to restrictions on the token budget. To ensure the stability of the final evaluation results, we sampled the results from each model 32 times and reported the average performance of these 32 results as the final metric (denoted as AVG@32). Appendix B illustrates the fluctuation of the average result across different sampling times. It demonstrates that when sampling 32 times, the average model performance exhibits a relatively small fluctuation and rarely appears to reverse the model ranking order. 3.2. Results and Analysis Main results. Figure 4 presents the AVG@32 performance of various leading LLMs, categorized by proprietary/open- source status and reasoning/non-reasoning properties. Overall, all these models still struggle with the significant challenges presented by AMO-Bench. Even the highest performing model GPT-5-Thinking (High) reaches just 52.4%, while most others score below 40%. This indicates substantial room for improvement in complex reasoning abilities across all current language models. Moreover, both proprietary and open-source reasoning models occupy top ranks in the leaderboard, indicating that recent open-source advancements are closing the gap with leading commercial models. The best-performing open-source model is only about 5% lower than the top proprietary result. Besides reasoning models, some non-reasoning models demonstrate a performance exceeding expectations, such as Qwen3-Max-Instruct 5To facilitate easier reproduction and utilization of AMO-Bench, you can take a fast try on the AMO-Bench-P subset, which includes only the 39 parser-based grading problems from AMO-Bench. Appendix C presents the AVG @32 performance of LLMs on AMO-Bench-P. ==== Page 7 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat GPT-5-Thinking (High) S) 50 4 DeepSeek-V3.1-Thinking Qwen3-235B-Thinking @ @ LongCat-Flash-Thinking 04-mini (High) 40 @ Gemini-2.5-Pro GLM-4.5 Qwen3-Next-Thinking 3-mini (High 8 03-mini (High) eepSeek-R1-0528 ZS 30 Qwen3-Max-Instruct N m  g =z 20 - - ini Qwen3-Next-Instruct Claude-Sonnet-4.5 Gemini-2.5-Flash LongCat-Flash - - DeepSeek-R1 DeepSeek-V3.1 Claude-Opus-4 "oO 10  Kimi-K2 DeepSeek-V3-0324 GPT-4.1 (0) GPT-[REDACTED] () [REDACTED] 40000 50000 Average Output Tokens Figure 5: The AVG @32 performance of LLMs vs. the average model output length. and LongCat-Flash. These non-reasoning models even outperforms several reasoning models such as 03-mini (Medium), indicating their significant potential in tackling complex reasoning tasks. Comparison of reasoning efficiency. Figure 5 shows the average output length and the AVG@32 performance of each model. Overall, it demonstrates a clear trend that higher-performing models tend to require more output tokens. The first-tier models that reach higher than 40% AVG@32 scores utilize more than 35K completion tokens. Even among non-reasoning models, those with superior performance are distinguished by their ability to process more tokens, sometimes reaching levels comparable to reasoning models. Additionally, when examining models within the same series, there are notable improvements in reasoning efficiency over time. For example, 04-mini (High) outperforms 03-mini (High) at similar or slightly increased token counts. Likewise, DeepSeek-V3.1-Thinking shows significant gains compared to DeepSeek-R1-0528 with even significantly less output tokens. Beyond the main results outlined above, we also provide further analysis and insights based on the AMO-Bench experimental findings. The model output length could indicate the reasoning challenge of the benchmark. Section 2.2 provides a pre-analysis of benchmark difficulty based on annotated solution lengths. Here, we offer a post-hoc analysis of benchmark difficulty based on the relationship between model performance and model output length. Figure 6 clearly demonstrates that the average output length of each model increases as the reasoning benchmark becomes more challenging. Specifically, across six models, benchmarks with higher accuracy scores (such as MAH500 and AIME24) correspond to shorter average outputs, while those with lower scores (like AMO-Bench) require significantly longer responses. This suggests that harder benchmarks demand more elaborate reasoning steps or explanations from the models, resulting in increased token usage. These results demonstrate that the model output length could be an indicator of reasoning challenge in the benchmark. Performance on AMO-Bench still benefits from test-time scaling. The reasoning efficiency results discussed above indicate a correlation between model performance and output length. Here, we conduct a more rigorous analysis by directly controlling the reasoning effort for the same model. As shown in the Figure 7, all three models (GPT-5, 04-mini, and 03-mini) exhibit a near-linear growth trend in AVG @32 as the logarithm of average output length increases. Such a trend is highly aligned with earlier experimental observations from existing benchmarks such as MATH500 ==== Page 8 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat MAHS500 [REDACTED] @- LongCat-Flash-Thinking ~@- GLM-4.5 od @- Qwen3-235B-A22B-Thinking-2507 90 IME2. 90 90 80 i roa i B 7 BeyondAIME fod fod BeyondAIME P=) P=) > 70 g g g > > 3 @ @    3 BeyondAIME 3 7 3 BeyohdAIME QO 60 g Q 70 }$ Example: ### The final answer is: $\boxed{123}$ The final answer should be given as precisely as possible (using LaTeX symbols such as \sqrt, \frac, \pi, etc.). If the final answer involves a decimal approximation, it must be accurate to at least four decimal places. Grading prompt template. We employ the LLM-based grading using 04-mini (Low) as the grading model, and use the following grading prompt to verify the equivalence between the LLM output and the reference answer. Example 6: Grading Prompt Template For the following math problem, we have the reference answer and the students answer. Determine whether the students answer is equivalent to the reference answer. If equivalent, output "Correct". If not equivalent, output "Incorrect". ### Problem ### Reference Answer ### Student Answer Now, please provide your judgment. Please strictly follow the format below to summarize your conclusion at the end of your judgment: ### Conclusion: Correct/Incorrect If the answer involves a decimal approximation, it must be accurate to at least four decimal places. B_ Analysis of AVG @k Figure 9 illustrates the fluctuation of the average performance across different sampling times. It shows that as the sampling time grows, the models performance become more stable. When sampling 32 times, it rarely appears the reverse-order phenomenon. C. Performance on AMO-Bench-P Subset To facilitate easier reproduction and use of AMO-Bench, you can utilize the AMO-Bench-P subset, which includes only the 39 parser-based grading problems from AMO-Bench. Table | presents the AVG@32 performance of LLMs on AMO-Bench-P. In general, performance on AMO-Bench-P tends to be slightly higher than on the full AMO-Bench, as problems requiring complex descriptive answers are inherently more challenging than those with simple-format answers. 12 ==== Page 13 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat AVG@K (%) [REDACTED].0 53.7 53.2 52:5~-52:4-23:8 Ba 529 52.6 527 52.[REDACTED] [REDACTED] 52.4 41.0 46.7 46.8 45 53-4 36.8 37.0 36.9 36.8 35,6 36-0 35.8 35.9 35.9 36.0 36.0 1 28.4 28.5 29.7 28.9 29.0 29.0 28.8 29.6 28.8 28.8 28.8 28.[REDACTED] 28.2 576 27.9 24.8 28.[REDACTED] 27.[REDACTED] 26.8 26.7 26.6 27.1 26.0 20.0 7 15,0 15.2 15,0 15.[REDACTED] 15,2 153 15.2 153 15.[REDACTED] [REDACTED] 146 14.[REDACTED] 14.4 148 14 12.0 11.3 lo 114 10:510:4 14593 ido 10-4 10.2 10.0 99 10.3 144 10-4 10.2 10.3 10.3 10.2 10.1 10.1 142 10.2 10.2 10.1 10.1 100 9[9 99 98 6.0 6.0 fi 53 54 55 53 54 53 ao 40 48 50 48 46 44 gh [REDACTED] [REDACTED] [REDACTED] 30 K =O GPT-5-Thinking (High) =O= LongCat-Flash-Thinking =O Qwen3-Max-Instruct =O= DeepSeek-V3.1 =O= DeepSeek-V3.1-Thinking =O GLM-4.5 =O= LongCat-Flash =O GPT-4.1 Figure 9: The AVG@& trend of various LLMs with increasing k. 13 ==== Page 14 ==== AMO-Bench: Large Language Models Still Struggle in High School Math Competitions fA LongCat Table 1: The AVG @32 performance of LLMs on the AMO-Bench and AMO-Bencb-P, the latter of which contains only 39 parser-based grading problems. Model AMO-Bench AMO-Bench-P GPT-5-Thinking (High) 52.4 54.8 Qwen3-235B-A22B-Thinking-2507 47.8 56.2 DeepSeek-V3.1-Thinking 47.6 53.0 LongCat-Flash-Thinking 43.6 45.3 04-mini (High) 40.2 43.8 Gemini-2.5-Pro 38.7 41.7 GLM-4.5 36.8 41.0 Qwen3-Next-80B-Thinking 34.8 37.4 DeepSeek-R1-0528 34.3 37.1 o03-mini (High) 32.3 34.0 Qwen3-Max-Instruct 28.8 30.9 Qwen3-Next-80B-Instruct 18.2 17.8 Gemini-2.5-Flash 18.1 18.0 Claude-Sonnet-4.5 17.6 18.1 LongCat-Flash 14.6 14.9 DeepSeek-R1 10.9 11.7 Claude-Opus-4 10.6 11.4 DeepSeek-V3.1 9.8 9.6 Kimi-K2 75 8.4 DeepSeek-V3-0324 5.2 5.4 GPT-4.1 4.1 4.8 GPT-[REDACTED] 1.5 1.9 14

When I was first learning to meditate, the instruction was to simply pay attention to my

breath and with my mind wandered to bring it back.

Sounded simple enough.

Yet, I'd sit on these silent retreats, sweating through t-shirts in the middle of winter.

I'd take naps every chance I got because it was really hard work.

Actually, it was exhausting.

The instruction was simple enough, but I was missing something really important.

So why is it so hard to pay attention?

Well studies show that even when we're really trying to pay attention to something, like

maybe this talk, at some point about half of us will drift off into a daydream or have

this urge to check our Twitter feed.

So what's going on here?

It turns out that we're fighting one of the most evolutionarily conserved learning processes

currently known in science, one that's conserved back to the most basic nervous systems known

to man.

This reward-based learning process is called positive and negative reinforcement and basically

goes like this.

We see some food that looks good.

Our brain says, calories, survival.

We eat the food, we taste it, it tastes good, and especially with sugar, our bodies send

a signal to our brain that says, remember what you're eating and where you found it.

We lay down this context dependent memory and learn to repeat the process next time.

See food, eat food, feel good, repeat.

Trigger, behavior, reward.

Simple right?

Well after a while our creative brain say, you know what, you can use this for more than

just remembering where food is.

Even or next time you feel bad, why don't you try eating something good so you'll feel

better?

We think our brains are the great idea.

Try this and quickly learn that if we eat chocolate or ice cream when we're mad or sad

we feel better.

Same process, just a different trigger.

Instead of this hunger signal coming from our stomach, this emotional signal feeling

sad triggers that it's to eat.

Maybe in our teenage years we were a nerd at school and we see those rebel kids outside

smoking we think, hey I want to be cool so we start smoking.

The Marlboro man wasn't adoric and that was no accident.

See cool, smoke to be cool, feel good, repeat.

And each time we do this we learn to repeat the process and it becomes a habit.

So later feeling stressed out triggers that urge to smoke a cigarette or to eat something

sweet.

Now with these same brain processes we've gone from learning to survive to literally

killing ourselves with these habits.

Obesity and smoking among the leading preventable causes of morbidity and mortality in the world.

So back to my breath, what if instead of fighting our brains or trying to force ourselves

to pay attention we instead tapped into this natural reward based learning process but

added a twist.

What if instead we just got really curious about what was happening in our momentary experience.

I'll give you an example.

In my lab we studied whether mindfulness training could help people quit smoking.

Now just like trying to force myself to pay attention on my breath they could try to

force themselves to quit smoking and the majority of them had tried this before and failed

on average six times.

Now with mindfulness training we dropped a bit about forcing and instead focused on being

curious.

In fact we even told them to smoke.

What?

We said go ahead and smoke just be really curious about what is like when you do and what

did they notice.

Well here's an example from one of our smokers.

She said mindful smoking smells like stinky cheese and tastes like chemicals.

Yuck!

Now she knew cognitively that smoking was bad for her.

That's why she joined our program.

What she discovered just by being curiously aware when she smoked was that smoking tastes

like shit.

Now she moved from knowledge to wisdom.

She moved from knowing in her head that smoking was bad for her to knowing it in her bones.

And the spell of smoking was broken.

She started to become disenchanted with her behavior.

Now the prefrontal cortex that youngest part of our brain from an evolutionary perspective

it understands on an intellectual level that we shouldn't smoke and it tries its hardest

to help us change our behavior to help us stop smoking to help us stop eating that second

that third that fourth cookie.

We call this cognitive control.

We're using cognition to control our behavior.

Unfortunately this is also the first part of a brain that goes offline when we get stressed

out which isn't that helpful.

Now we can all relate to this in our own experience.

We're much more likely to do things like yell at our spouser kids when we're stressed

out or tired even though we know it's not going to be helpful.

We just can't help ourselves.

Now when the prefrontal cortex goes offline we fall back into our whole habits which is

why this disenchantment is so important.

Seeing what we get from our habits helps us understand them in a deeper level to know

it in our own bones so we don't have to force ourselves to hold back or restrain ourselves

from behavior.

We're just less interested in doing it in the first place.

This is what mindfulness is all about.

Seeing really clearly what we get when we get caught up in our behaviors.

Becoming disenchanted on a visceral level and from this disenchanted stance naturally

letting go.

This isn't to say that poof magically we quit smoking but over time as we learn to see

more and more clearly the results of our actions we let go of old habits and form new ones.

The paradox here is that mindfulness is just about being really interested in getting

close and personal with what's actually happening in our bodies and minds from moment to moment.

This willingness to turn toward our experience rather than trying to make unpleasant cravings

go away as quickly as possible.

And this willingness to turn toward our experience is supported by curiosity which is naturally

rewarding.

What does curiosity feel like?

It feels good.

And what happens when we get curious?

We start to notice that cravings are simply made up of body sensations.

Oh, there's tightness, there's tension, there's restlessness and that these body sensations

come and go.

These are bite-sized pieces of experiences that we can manage for a moment to moment rather

than getting clobbered by this huge scary craving that we choke on.

In other words, when we get curious we step out of our old fear-based reactive habit patterns

and we step into being.

We become this inner scientist where we're eagerly awaiting that next data point.

Now this might sound too simplistic to affect behavior but in one study we found that mindfulness

training was twice as good as gold standard therapy at helping people quit smoking.

So it actually works.

And when we studied the brains of experienced meditators we found that parts of a neural

network of self-referential processing called the default mode network were at play.

Now one current hypothesis is that a region of this network called the posterior cingulate

cortex is activated not necessarily by craving itself but when we get caught up in it, when

we get sucked in and it takes us for a ride.

In contrast when we let go, step out of the process just by being curiously aware of what's

happening, this same green region quiets down.

Now we're testing app and online-based mindfulness training programs that target these core mechanisms

and ironically use the same technology that's driving us to distraction to help us step

out of our unhealthy habit patterns of smoking, of stress eating, and other addictive behaviors.

Now remember that bit about context dependent memory?

We can deliver these tools to people's fingertips in the context that matter most.

So we can help them tap into their inherent capacity to be curiously aware right when

that urge to smoke or stress eat or whatever arises.

So if you don't smoke or stress eat, maybe the next time you feel this urge to check

your email when you're bored or you're trying to distract yourself from work or maybe to

compulsively respond to that text message when you're driving, see if you can tap into

this natural capacity just be curiously aware of what's happening in your body and mind

in that moment.

It will just be another chance to perpetuate one of our endless and exhaustive habit loops

or step out of it.

Instead of seat text message, compulsively text back, feel a little bit better.

Notice the urge, get curious, feel the joy of letting go, and repeat.

Thank you.

So in college, I was a government major, which means I had to write a lot of papers.

Now when a normal student writes a paper, they might spread the work out a little like this.

So you know, you get started maybe a little slowly, but you get enough done in the first

week that with some heavier days later on, everything gets done and things taste civil.

And I would want to do that like that. That would be the plan. I would have it all ready to go,

but then that's actually the paper would come along and then I would kind of do this.

And that would happen every single paper. But then came my 90-page senior thesis.

A paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow

was not an option. It was way too big a project. So I planned things out and I decided I kind of

had to go something like this. This is how the year would go. So I'd start off light and I'd bump

it up in the middle months. And then at the end, I would kick it up into high gears. Just a little

staircase. How hard can I just walk up the stairs? No big deal, right? But then funniest thing happened.

Those first few months that came and went and I couldn't quite do stuff. So we had an awesome

new revised plan. And then those middle months actually went by and I didn't really write words.

And so we were here. And then two months turned into one month. She turned into two weeks. And one

day I woke up with three days until the deadline. Still not having written a word. And so I did the

only thing I could. I wrote 90 pages over 72 hours, pulling not one, but two all-nighters. Humans

are not supposed to pull two all-nighters. Sprinted across campus, dove in slow motion and

got it in just at the deadline. I thought that was the end of everything. But a week later, I get a call.

It's the school. And they say, is this Tim urban? And I say, yeah. And they say, we need to talk about

your thesis. And I say, okay. And they say, that's the one we've ever seen.

That did not happen. It was a very, very bad thesis.

I just wanted to enjoy that one moment when all of you thought, this guy is amazing.

No, no, it was very, very bad. Anyway, today I'm a writer, blogger, guy. I write the blog, wait, but why?

And a couple of years ago, I decided to write about procrastination. My behavior is always

perplexed the non-procrastinators around me. And I wanted to explain to the non-procrastinators

of the world what goes on in the heads of procrastinators and why we are the way we are.

Now, I had a hypothesis that the brains of procrastinators were actually different than the brains

of other people. And to test this, I found an MRI lab that actually let me scan both my brain

and the brain of a proven non-procrastinator. And so I could compare them. And I actually brought

them here to show you today. And I want you to take a look carefully to see if you can notice

a difference. And I know that if you're not a trained brain expert, it's not that obvious,

but just take a look. Okay? So here's the brain of a non-procrastinator.

Now, here's my brain.

There is a difference. Both brains have a rational decision maker, and then

the procrastinators brain also has an instant gratification monkey. Now, what does this mean for

the procrastinator? Well, it means everything's fine until this happens.

So the rational decision maker will make the rational decision to do something productive,

but the monkey doesn't like that plant. So he actually takes the wheel, and he says,

actually, let's read the entire Wikipedia page of the Nancy Kerrigan time you're harding scandal,

because I just remember that that happened. Then we're going to go over to the fridge. We're

going to see if there's anything new in there since 10 minutes ago. After that,

we're going to go on a YouTube spiral that starts with videos of Richard Feynman talking about

magnets and ends much, much later with us watching interviews with Justin Bieber's mom.

All of that's going to take a while, so we're not going to really have room on the schedule for

any work today. Sorry. Now, what is going on here?

The instant gratification monkey does not seem like a guy you want behind the wheel. He lives

entirely in the present moment. He has no memory of the past, no knowledge of the future,

and he only cares about two things. Easy and fun. Now, in the animal world, that works fine.

If you're a dog and you spend your whole life doing nothing other than easy and fun,

things, you're a huge success. And to the monkey, humans are just another animal species.

He has to keep well slept, well fed, and propagating into the next generation.

Which in tribal times might have worked okay. But if you haven't noticed, now we're not in tribal

times. We're in an advanced civilization, and the monkey does not know what that is.

Which is why we have another guy in our brain, the rational decision maker,

who give us the ability to do things no other animal can do. We can visualize the future. We can

see the big picture. We can make long-term plans, and he wants to take all of that into account.

And he wants to just have us do whatever makes sense to be doing right now.

Now, sometimes it makes sense to be doing things that are easy and fun. Like when you're having

dinner or going to bed or enjoying well or in leisure time, that's why there's an overlap.

Sometimes they agree. But other times, it makes much more sense to be doing things that are

harder and less pleasant for the sake of the big picture. And that's when we have a conflict.

And for the procrastinator, that conflict tends to end a certain way every time,

leaving him spending a lot of time in this orange zone, an easy and fun place that's entirely

out of the makes sense circle. I call it the dark playground.

Now, the dark playground is a place that all of you procrastinators out there know very well.

It's where leisure activities happen at times when leisure activities are not supposed to be

happening. The fun you have in the dark playground isn't actually fun because it's completely

unearned and the air is filled with guilt, dread, anxiety, self-hatred, all those good procrastinator

feelings. And the question is, in this situation, with the monkey behind the wheel, how does the

procrastinator ever get himself over here to this blue zone? A less pleasant place, but we're

really important things happen. Well, it turns out that the procrastinator has a guardian angel,

someone who's always looking down on him and watching over him in his darkest moments.

Someone called the panic monster.

Now, the panic monster is dormant most of the time, but it suddenly wakes up. Anytime a deadline

gets too close or there's danger of public embarrassment, a career disaster or some other scary

consequence. And importantly, he's the only thing that the monkey is terrified of. Now,

he became very relevant in my life pretty recently because the people of Ted

reached out to me about six months ago and invited me to do a Ted talk.

Now, of course, I said, yes, it's always been a dream of mine to have done a Ted talk in the past.

But in the middle of all this excitement, the rational decision-maker seemed to have something

else in his mind. He was saying, we clear on what we just accepted. Do we get what's going to be

now happening one day in the future? We need to sit down and work on this right now. And the

monkey said, totally agree, but also let's just open Google Earth and zoom into the bottom of India,

like 200 feet above the ground, and we're going to scroll up two and a half hours until we get to

the top of the country so we can get a better feel for India. So that's what we did that day.

As six months turned into four and then two and then one, the people of Ted decided to release

the speakers. And I opened up the website and there was my face staring right back at me and

guess who woke up. So the panic monster starts losing his mind and a few seconds later,

the whole system's in Mayhem. And the monkey who remember, he's terrified of the panic monster,

boom, he's up the tree. And finally, finally, the rational decision-maker can take the wheel and

I can start working on the talk. Now, the panic monster explains all kinds of pretty insane

procrastinated behavior like how someone like me could spend two weeks unable to start the opening

sentence of a paper and then miraculously find the unbelievable work ethic to stay up all night

and write eight pages. And this entire situation with the three characters, this is the procrastinator's

system. It's not pretty, but in the end, it works. And this is what I decided to write about on the

blog just a couple years ago. Now, when I did, I was amazed by the response. Literally thousands

of emails came in from all different kinds of people from all over the world doing all different

kinds of things. These were people who were nurses and bankers and painters and engineers and

lots and lots of PhD students. And they were all writing saying the same thing, I have this problem

too. But what struck me was the contrast between the light tone of the post and the heaviness of

these emails. These people were writing with intense frustration about what procrastination had

done to their lives, about what this monkey had done to them. And I thought about this and I said,

if the procrastinator system works, then what's going on? Why are all these people in such a dark

place? Well, it turns out that there's two kinds of procrastination. Everything I've talked about

today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of

procrastination are contained to the short term because the panic monster gets involved. But there's

a second kind of procrastination that happens in situations when there is no deadline. So if you

want to have a career, you want to be a self starter, something in the arts, something entrepreneurial,

there's no deadlines on those things at first. Because nothing's happening at first, not until you've

gone out and done the hard work to get some momentum to get things going. There's also all kinds of

important things outside of your career that don't involve any deadlines, like seeing your family,

or exercising and taking care of your health, working on your relationship, or getting out of

a relationship that isn't working. Now, if the procrastinator's only mechanism of doing these

hard things is the panic monster, that's a problem because in all of these non-deadline situations,

the panic monster doesn't show up. He has nothing to wake up for. So the effects of procrastination,

they're not contained. They just extend outward forever. And it's this long term kind of procrastination

that's much less visible and much less talked about than the funnier short term deadline-based kind.

It's usually suffered quietly and privately. And it can be the source of a huge amount of long term

unhappiness and regrets. And I thought, you know, that's why these people are emailing and that's

why they're in such a bad place. It's not that they're cramming for some project. It's that long-term

procrastination has made them feel like a spectator at times in their own lives. You know, the

frustration was not that they couldn't achieve their dreams, is that they weren't even able to start

chasing them. So I read these emails and I had a little bit of an epiphany that I don't think

non-procrastinator is exist. That's right. I think all of you are procrastinators. Now you might

not all be a mess like some of us. And some of you may have a healthy relationship with deadlines.

But remember, the monkey's sneakiest trick is when the deadlines aren't there. Now I want to show you

one last thing. I call this a life calendar. That's one box for every week of a 90-year life.

That's not that many boxes, especially since we've already used a bunch of those. So I think we

need to all take a long hard look at that calendar. And we need to think about what we're really

procrastinating on because everyone is procrastinating on something in life. We need to stay aware of

the instant gratification monkey. That's a job for all of us. And because there's not that many

boxes on there at the job that should probably start today. Well, it may not today, but you know,

sometimes soon. Thank you.

