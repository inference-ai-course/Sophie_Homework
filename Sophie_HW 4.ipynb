{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b5954-16b0-4b6e-abcb-e1f320ce3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Install dependencies\n",
    "\n",
    "# !pip install --upgrade pip\n",
    "# !pip install sentence-transformers faiss-cpu PyMuPDF tqdm fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09dbca46-28c5-4808-b6a5-21a00d59fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Imports and config\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import faiss\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "OUTPUT_DIR = Path(\"rag_project/output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "CHUNKS_JSON = OUTPUT_DIR / \"chunks.json\"\n",
    "INDEX_FILE = OUTPUT_DIR / \"faiss.index\"\n",
    "META_FILE = OUTPUT_DIR / \"meta.pkl\"\n",
    "\n",
    "DIM = 384  # embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6855d482-6639-4ce9-8039-b3ad85cd0ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 122 pages from 5 PDFs\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — PDF text extraction\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[str]:\n",
    "    pages = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            pages.append(page.get_text(\"text\"))\n",
    "    return pages\n",
    "\n",
    "# Example usage:\n",
    "pdf_files = list(Path(\"rag_project/pdfs\").glob(\"*.pdf\"))  # place your PDFs in a folder \"pdfs\"\n",
    "all_pages = []\n",
    "for pdf in pdf_files:\n",
    "    pages = extract_text_from_pdf(pdf)\n",
    "    all_pages.extend(pages)\n",
    "\n",
    "print(f\"Extracted {len(all_pages)} pages from {len(pdf_files)} PDFs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2253933-c831-49b0-bb5e-ce45761b630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 189 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Chunking\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_tokens\n",
    "        chunk = \" \".join(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking\n",
    "all_chunks = []\n",
    "metadata = []\n",
    "\n",
    "for page_num, page_text in enumerate(all_pages, start=1):\n",
    "    page_chunks = chunk_text(page_text)\n",
    "    for chunk_id, chunk in enumerate(page_chunks):\n",
    "        metadata.append({\"page\": page_num, \"chunk_id\": len(all_chunks), \"text\": chunk})\n",
    "        all_chunks.append(chunk)\n",
    "\n",
    "print(f\"Created {len(all_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "696b604d-91cf-4b6b-9dc9-8e5a95909483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 — Save chunks (optional)\n",
    "\n",
    "with open(CHUNKS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([m[\"text\"] for m in metadata], f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b6bcb0e-c2cb-4a19-b873-24197303085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bffe7371e3a4cc787a0908f36b5d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (189, 384)\n"
     ]
    }
   ],
   "source": [
    "# Step 6 — Load embedding model and encode chunks\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "batch_size = 64\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"Embedding batches\"):\n",
    "    batch = all_chunks[i:i+batch_size]\n",
    "    emb = model.encode(batch, show_progress_bar=False)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b418f757-dba1-412c-9e44-686775aec772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 189 vectors\n",
      "Saved index to rag_project\\output\\faiss.index and metadata to rag_project\\output\\meta.pkl\n"
     ]
    }
   ],
   "source": [
    "# Step 7 — Build FAISS index\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dim)\n",
    "faiss_index.add(embeddings)\n",
    "print(f\"FAISS index contains {faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Save index & metadata\n",
    "faiss.write_index(faiss_index, str(INDEX_FILE))\n",
    "with open(META_FILE, \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"Saved index to {INDEX_FILE} and metadata to {META_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45243923-48e3-4f7a-88ea-d6e4fdd1f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 17] distance: 1.5896\n",
      "thereby supporting subsequent reasoning processes. In each instance, the model is required to accurately rotate target objects within a fixed 2D plane while preserving the overall scene structure and structural consistency, followed by performing reasoning tasks like grounding and OCR. The evaluation focuses on both the accuracy of the rotation in terms of angle and direction, and the precision of the resulting reasoning tasks. 17\n",
      "--------------------------------------------------------------------------------\n",
      "[Page 19] distance: 1.6424\n",
      "Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: ✓Good: The rotation is accurate, complete, and strictly confined to the 2D plane, with no extraneous scene motion. The following reasoning tasks are completed correctly. Target objects remain precisely grounded after rotation. ~ Moderate: The rotation is largely correct but may be incomplete or slightly off-angle, though still confined to the 2D plane. The following reasoning tasks are mostly completed. Minor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or object grounding. ✗Bad: The model fails to perform the correct rotation, extends the transformation into 3D space, or introduces substantial scene distortion. Cannot complete the following reasoning task. The original 2D structure is altered, leading to inaccurate grounding of the target objects. Data Source. To specifically assess the rotation reasoning task, we recruit some PhD-level experts with deep expertise in text-image reasoning to design the evaluation data manually, followed by the necessary review process, as mentioned in Section 3.2. Each question is designed following the principle that it must involve a 2D rotation to reach the correct solution, ensuring the task genuinely probes rotational understanding rather than simple visual matching. Moreover, we sample data from the 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions for the video models. Example and Analysis. The results are shown in Figure 12. In case I, we find that Veo-3 handles small-angle rotations and simple planar scenes reasonably well, demonstrating a basic grasp of rotational motion. However, in more complex scenarios like cases II, III, and IV, the model often ignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect rotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such as OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment. These observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather than principle-driven. However, as it demonstrates a partial understanding of planar rotation, this can to some extent facilitate subsequent reasoning tasks. Takeaway 8 Veo-3 exhibits only a superficial understanding of rotation reasoning. While it can approximate small planar rotations, it fails to preserve geometric consistency under larger or compound transformations. 2.9 Table and Chart Reasoning Task Description and Evaluated Aspects. The table and chart reasoning task requires the model to identify and focus on the key elements within visualizations or tabular data. For evaluation, we further consider how effectively the model identifies the regions relevant to the query and whether it can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and proper scaling. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: ✓Good: Camera precisely focuses on the correct chart or table segment, smoothly high- lighting or zooming into the queried data (e.g., correct year, category, or value). Motion is continuous, the chart and table remain clear, and no distortion or overexposure occurs. ~ Moderate: Camera approximately focuses on the right region but partially misses\n",
      "--------------------------------------------------------------------------------\n",
      "[Page 106] distance: 1.6969\n",
      "Produce a ~60-second, 2D flat-design explainer educating viewers on trimming, pruning, stump removal, and tree health. Use bold typography, a natural palette, icon-driven graphics, subtle character animation, and smooth modern transitions. Pair with the supplied voiceover. Produce five short, high-quality 3D product demo animations that clearly showcase the earbuds’ silicone tips, swappable battery stem, sleek charging case. The clips should be polished and visually consistent, with smooth camera moves and lighting that emphasizes materials, fit, and the replaceable battery mechanism. Examples of Unsuccessful Project Completion AI Deliverable Human Deliverable Inputs Project Brief AI Deliverable Human Deliverable Inputs Project Brief Figure 17: Agents fail to successfully complete the vast majority of RLI projects. Here we show failed projects for Gemini 2.5 Pro (top) and GPT-5 (bottom). 29\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 8 — Interactive retrieval\n",
    "\n",
    "def retrieve(query: str, model: SentenceTransformer, index, metadata, k: int = 3):\n",
    "    qvec = model.encode([query]).astype(\"float32\")\n",
    "    distances, indices = index.search(qvec, k)\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        meta = metadata[idx].copy()\n",
    "        meta[\"distance\"] = float(distances[0][i])\n",
    "        results.append(meta)\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"How do transformers work?\"\n",
    "results = retrieve(query, model, faiss_index, metadata, k=3)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[Page {r['page']}] distance: {r['distance']:.4f}\")\n",
    "    print(r['text'])\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad1d66d9-099d-47f9-b097-db8dfce3f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_queries = [\n",
    "    \"How do transformers work?\",\n",
    "    \"Applications of natural language processing\",\n",
    "    \"What is self-supervised learning?\",\n",
    "    \"Data preprocessing techniques for machine learning\",\n",
    "    \"How to fine-tune BERT for classification?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6e11499-ab7d-41c3-b362-f356fda7d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {}\n",
    "\n",
    "for q in example_queries:\n",
    "    top_chunks = retrieve(q, model, faiss_index, metadata, k=3)\n",
    "    report[q] = top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7060d0d-a24f-42b6-a876-b71d73a1a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do transformers work?\n",
      "  Top-1 | distance: 1.5896 | chunk_id: 23\n",
      "    thereby supporting subsequent reasoning processes. In each instance, the model is required to accurately rotate target objects within a fixed 2D plane while preserving the overall scene structure and ...\n",
      "  Top-2 | distance: 1.6424 | chunk_id: 25\n",
      "    Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: ✓Good: The rotation is accurate, complete, and strictly confined to the 2D plane, with no extraneous scene...\n",
      "  Top-3 | distance: 1.6969 | chunk_id: 164\n",
      "    Produce a ~60-second, 2D flat-design explainer educating viewers on trimming, pruning, stump removal, and tree health. Use bold typography, a natural palette, icon-driven graphics, subtle character an...\n",
      "================================================================================\n",
      "Query: Applications of natural language processing\n",
      "  Top-1 | distance: 1.3874 | chunk_id: 143\n",
      "    Neural Information Processing Systems, 37:52040–52094, 2024. 14...\n",
      "  Top-2 | distance: 1.4088 | chunk_id: 75\n",
      "    Gistify! Codebase-Level Understanding via Runtime Execution Disha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your...\n",
      "  Top-3 | distance: 1.4244 | chunk_id: 43\n",
      "    for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. a...\n",
      "================================================================================\n",
      "Query: What is self-supervised learning?\n",
      "  Top-1 | distance: 1.3273 | chunk_id: 23\n",
      "    thereby supporting subsequent reasoning processes. In each instance, the model is required to accurately rotate target objects within a fixed 2D plane while preserving the overall scene structure and ...\n",
      "  Top-2 | distance: 1.3422 | chunk_id: 148\n",
      "    files within the same deliverable. The training detailed the standardized evaluation workflow: Annotators must first gain an understand- ing of the project by reading the brief and reviewing the refer...\n",
      "  Top-3 | distance: 1.3438 | chunk_id: 143\n",
      "    Neural Information Processing Systems, 37:52040–52094, 2024. 14...\n",
      "================================================================================\n",
      "Query: Data preprocessing techniques for machine learning\n",
      "  Top-1 | distance: 1.3795 | chunk_id: 140\n",
      "    Acknowledgments We would like to thank Anders Edson, Hale Guyer and Connor Smith for providing helpful feedback throughout the drafting process. We would also like to thank Michael Jae Byun and Brian ...\n",
      "  Top-2 | distance: 1.4000 | chunk_id: 69\n",
      "    Gistify! Codebase-Level Understanding via Runtime Execution 5 Related Works 5.1 Codebase-level Understanding Benchmark Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase...\n",
      "  Top-3 | distance: 1.4041 | chunk_id: 99\n",
      "    For example, tokens are generated auto-regressively during inference but are processed in parallel during training. Different paralleliza- tion strategies and precision-sensitive operations such as to...\n",
      "================================================================================\n",
      "Query: How to fine-tune BERT for classification?\n",
      "  Top-1 | distance: 1.3004 | chunk_id: 114\n",
      "    7 Conclusion This work demonstrates that the training-inference mismatch, a major source of instability in RL fine-tuning, is fundamentally a problem of numerical precision. While existing algorithmic...\n",
      "  Top-2 | distance: 1.3298 | chunk_id: 75\n",
      "    Gistify! Codebase-Level Understanding via Runtime Execution Disha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your...\n",
      "  Top-3 | distance: 1.3476 | chunk_id: 119\n",
      "    A Detailed Experimental Settings A.1 MoE RL As for experiments of MoE RL, we use Qwen3-30B-A3B-Base as the base model. The training data comes from DAPO-Math-17k [Yu et al., 2025], and we conduct onli...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for query, results in report.items():\n",
    "    print(f\"Query: {query}\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  Top-{i} | distance: {r['distance']:.4f} | chunk_id: {r['chunk_id']}\")\n",
    "        print(f\"    {r['text'][:200]}...\")  # show first 200 chars\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539bf3b-9a33-4b7f-9b9c-965c1ba06c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell 9 — Optional: FastAPI Integration\n",
    "Run this separately in a terminal (not in the notebook):\n",
    "# uvicorn main:app --reload --host 0.0.0.0 --port 8000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
