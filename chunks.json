[
  "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-COF Benchmark Ziyu Guo‚àó‚Ä†1, Xinyan Chen‚àó2, Renrui Zhang‚àó‚Ä°2, Ruichuan An‚àó3, Yu Qi‚àó4, Dongzhi Jiang2 Xiangtai Li3, Manyuan Zhang2, Hongsheng Li2, Pheng-Ann Heng1 CUHK 1IMIXR & 2MMLab 3Peking University 4Northeastern University ‚àóEqual Contribution ‚Ä†Project Lead ‚Ä°Corresponding Author Project Page: https://video-cof.github.io Abstract Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond real- istic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation [70]. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3 [21]. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-COF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. 1 Introduction Video models [21, 63, 55, 81, 11], including text-to-video and video-to-text generation models, have made rapid progress in recent years. Thanks to advances in diffusion [75, 7, 84] and autoregressive [36, 76, 16] architectures, current video models can produce high-fidelity videos maintaining consistent object relations and realistic motion dynamics across frames. This suggests that the models may have internalized substantial visual and structural knowledge about the world. Recent research from Google [70] further hints that, such models are evolving beyond pure content generation: Veo-3 [21] has been shown to perform dozens of distinct vision tasks across perception, modeling, manipulation, and reasoning, without any task-specific training. These emergent capabilities have led researchers to posit that video models could serve as unified, generalist vision models, much like large language models (LLMs) [1, 13, 3, 30] have become foundation models for natural language. Crucially, the sequential nature of video generation provides a new perspective on how such models might reason. Each generated frame builds upon the last, creating a temporal chain of information propagation. This has been dubbed ‚ÄúChain-of-Frame‚Äù (CoF) reasoning [70], an analogy to the chain- of-thought (CoT) process in LLMs [69, 35, 82, 23, 79] and their multi-modal variants (MLLMs) [12, 4, 40, 31, 10]. In essence, as a video model generates a sequence of frames, it can iteratively refine and update the scene, thereby working through a problem step-by-step in time and space. This CoF concept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may emerge from video generative models. arXiv:2510.26802v1 [cs.CV] 30 Oct 2025",
  "problem step-by-step in time and space. This CoF concept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may emerge from video generative models. arXiv:2510.26802v1 [cs.CV] 30 Oct 2025",
  "Embodied Reasoning 3D Geometry Reasoning Real-world Spatial Reasoning Physics-based Reasoning 2D Geometry Reasoning Table and Chart Reasoning Object Counting Reasoning Rotation Reasoning Visual Detail Reasoning Visual Trace Reasoning GUI Reasoning Medical Reasoning Veo Sora Seedance Video Models ... Zero-shot Reasoning? Kling Figure 1: Overview of Our Study on the Reasoning Potential of Video Models. We investigate whether state-of-the-art video models exhibit emergent reasoning potentials beyond content synthesis. The analysis spans 12 reasoning dimensions under a unified perspective, exploring whether large-scale video models can serve as zero-shot visual reasoners via CoF reasoning. However, it remains unclear to what extent current video models truly exhibit reasoning about the content they create. Strong generative performance does not automatically imply robust reasoning potential. Emerging evidence [22, 47, 5, 78] shows that a model may produce coherent videos by learning surface-level patterns in the training data, rather than by internalizing general principles. For instance, a video model can maintain object continuity yet fail to grasp physical plausibility across a long sequence, or it may mimic observed visual sequences without understanding the underlying cause-and-effect relationships. This motivates our central question: Are video models, purely through large-scale visual learning, obtain the zero-shot reasoning potential? To this end, we present the first empirical study to systematically probe the CoF reasoning capabili- ties of modern video models, spanning 12 dimensions such as spatial, geometric, physical, temporal, and embodied logic, as detailed in 1. We carry out our analysis on Veo-3, which has been system- atically examined as a zero-shot learner in prior work [70]. Our preliminary observations suggest that current leading video models exhibit comparable reasoning patterns, making Veo-3 a represen- tative choice. Our analysis builds on reasoning scenarios distilled from diverse reasoning-oriented benchmarks [25, 67, 45, 71, 29, 34], as well as those we design ourselves, providing a compact yet expressive foundation. The prompts for video models are meticulously crafted by transforming the underlying, textual reasoning process of problem-solving into a clear, video-presentation format. Each case receives a qualitative assessment across three performance levels, i.e., good, moderate, and bad, complemented by a quantitative success rate to measure robustness. To standardize evaluation, we curate these tasks into the MME-COF benchmark, as illustrated in Figure 2 and Section 3.2. Leveraging this benchmark, we measure several state-of-the-art video models, i.e., Veo-3 [21], Sora-2 [56], Kling [38], and Seedance [19], to obtain directly comparable scores and qualitative behaviors across categories. Our investigation reveals that the models exhibit promising reasoning patterns in short-horizon spatial coherence, fine-grained grounding, and con- 2",
  "Medical Reasoning Embodied Reasoning Object Counting Reasoning GUI Reasoning Table & Chart Reasoning Rotation Reasoning 3D Geometry Reasoning Real-world Spatial Reasoning Visual Trace Reasoning Visual Detail Reasoning 2D Geometry Reasoning Physics-based Reasoning Kling-v1 Seedance-1.0-pro Sora-2-pro Sora-2 Veo-3-fast Veo-3-preview (a) Evaluation Radar Map. (b) Word Cloud. Figure 2: Illustration of the MME-COF Benchmark. It showcases that different models specialize in distinct reasoning aspects, but most models exhibit limited reasoning capability across all tasks. sistent local dynamics; however, they struggle with complex reasoning conditions, particularly in long-horizon causal consistency, geometric constraint adherence, and abstract logic. Overall, current video models are not yet ready as standalone zero-shot reasoners. Still, they show encouraging signs of emergent reasoning, suggesting strong potential as complementary reasoning agents alongside specialized models. Our main contributions are summarized as follows: ‚Ä¢ A Comprehensive Empirical Study. We provide the first investigation of video models (Veo-3) to analyze their visual reasoning potential, detailing representative successes, char- acteristic errors, and the conditions under which CoF reasoning emerges, holds, or breaks. ‚Ä¢ The MME-COF Benchmark. We curate MME-COF, a compact benchmark providing a standardized taxonomy and an evaluation protocol aligned with CoF reasoning, enabling consistent and category-wise assessment beyond surface-level visual fidelity. ‚Ä¢ Insights and Directions. We summarize common success patterns (e.g., short-horizon coherence and stable spatial layout) and failure patterns (e.g., long-horizon degradation, violations of basic geometry/physics, and temporal logic), making clear when the behavior reflects genuine reasoning versus pattern replay. 2 Deep-Dive Analysis on Veo-3 2.1 Overview To ensure a rigorous empirical study, we detail our core methodology in this section, including the taxonomy of reasoning tasks, test case curation process, the standardized style for prompt design, and the analysis setup. Task Taxonomy. To capture different dimensions of reasoning, our study starts from dozens of reasoning-oriented tasks, which can be organized into the following 12 categories: 3",
  "1) Visual Detail Reasoning 2) Visual Trace Reasoning 3) Real-world Spatial Reasoning 4) 3D Geometry Reasoning 5) 2D Geometry Reasoning 6) Physics-based Reasoning 7) Rotation Reasoning 8) Table and Chart Reasoning 9) Object Counting Reasoning 10) GUI Reasoning 11) Embodied Reasoning 12) Medical Reasoning Each category comprises several representative cases selected to test specific aspects of reasoning. Test Case Curation. We recruit five PhD-level experts with deep expertise in text-image reasoning, who are tasked with selecting representative cases from benchmarks [25, 67, 45, 52, 74] corresponding to each task category. For each reasoning case, the experts manually constructed text prompts that explicitly or unambiguously define the target reasoning objective, aiming to evaluate the potential of video models for multi-modal reasoning. Prompt Design Style. To ensure consistency and fairness, all prompts follow a unified style emphasizing explicit visual constraints, controlled motion, and minimal linguistic ambiguity. Prompts are encouraged to be written in imperative form and designed to reduce variance from language interpretation, focusing the model‚Äôs behavior on the intended visual reasoning objective. The overall design principles are as follows: 1) Static camera and fixed viewpoint, unless motion is explicitly required by the task. 2) Stable spatial composition, consistent framing, and unchanging scene layout across frames. 3) Clear specification of allowed and disallowed changes (e.g., ‚Äúno zoom, no pan, no dolly‚Äù) to constrain camera dynamics. 4) Explicit temporal phrasing to control the pace of motion, using cues such as ‚Äúinstantly‚Äù, ‚Äúsmoothly‚Äù, or ‚Äústep-by-step‚Äù. 5) Avoidance of direct textual hints toward the answer; instructions are purely visual and task-oriented. 6) Inclusion of realistic phrasing and scene context to align with the model‚Äôs natural video priors while minimizing artifacts. The standardized prompt style ensures that differences in output primarily reflect the model‚Äôs internal reasoning potential rather than prompt variability. Analysis Setup. For every reasoning case, we construct a text prompt that explicitly or implicitly specifies the target reasoning objective. Each prompt produces six video samples at a resolution of 1280√ó720, 24 FPS, and a duration of 8 seconds. All experiments are conducted in a unified zero-shot setup without fine-tuning, additional supervision, or auxiliary tools. We evaluate model outputs through qualitative judgments along three levels of performance, i.e., Good , Moderate , and Bad, based on the clarity, correctness, and temporal stability of the visual reasoning process. Detailed definitions and examples of these evaluation criteria are provided in the corresponding task subsections. Note that, since we observe that most video models struggle to follow the requirement of ‚Äòstatic shot‚Äô reliably, we apply more permissive qualitative criteria for static-shot evaluations. We further define a success rate to measure robustness across generations for each case, computed as the proportion of successful samples among the six generated. For cases categorized as Bad, the success rate is always 0. Non-zero success rates only appear in cases evaluated as Good or Moderate , indicating that Veo-3 exhibits some potential to perform as a visual reasoner. A higher success rate reflects a more stable reasoning capability of the model. 4",
  "rates only appear in cases evaluated as Good or Moderate , indicating that Veo-3 exhibits some potential to perform as a visual reasoner. A higher success rate reflects a more stable reasoning capability of the model. 4",
  "Input Image: Reasoning Video: Zoom in on the black bag with the Apple logo to focus on the logo's color. Static shot. I. Question: Text-to-Video Prompt: Q: What is the color of the Apple logo? 1st frame Input Image: Reasoning Video: II. Question: Text-to-Video Prompt: 1st frame III. Question: Text-to-Video Prompt: A: The color of the Apple logo is polychromatic. IV. Question: ~ Moderate Success Rate: 17% Gradually zoom in on the group of people walking along the path, centering on the person carrying the handbag. Keep the surrounding park and benches softly blurred to emphasize the handbag‚Äôs color. Static shot. Q: What is the color of the handbag? A: The color of the handbag is white. ‚úì Good Success Rate: 33% Input Image: Reasoning Video: 1st frame Smoothly zoom in on the dog near the lower right corner of the scene, then highlight the motorcycle parked near it. Keep the surrounding jeeps and people slightly blurred to emphasize spatial relation. Static shot. Q: Is the motorcycle on the left or right side of the dog? A: The motorcycle is on the left side of the dog. ‚úì Good Success Rate: 83% Text-to-Video Prompt: Input Image: Reasoning Video: 1st frame Gradually zoom in on the area near the cone along the pathway, centering both the cone and the baby carriage in the frame. Keep the surrounding trees and grass softly blurred to emphasize these two objects. Static shot. Q: Is the baby carriage on the left or right side of the cone? A: The baby carriage is on the right side of the cone. ‚úó Bad Figure 3: Showcase of Visual Detail Reasoning by Veo-3. It illustrates Veo-3‚Äôs ability to localize targets and maintain fine-grained visual attributes across frames, together with common failure modes when targets are small, occluded, or embedded in clutter. 5",
  "2.2 Visual Detail Reasoning Task Description and Evaluated Aspects. In the visual detail reasoning category, the objective is to assess a model‚Äôs ability to discern and maintain fine-grained visual attributes and spatial relations within generated video sequences. It covers attribute recognition, e.g., identifying color, texture or material of an object, and spatial relation identification, e.g., recognizing that one object is on the left of or behind another object. The model is evaluated on the capacity both to attend to the correct target region and to maintain visual consistency, across frames, of the attribute or relation in question. Definition of Good / Moderate / Bad. We define the three-level evaluation criteria as follows: ‚úìGood: The reasoning video accurately centers on the correct target region, clearly resolves the relevant attribute, such as color, texture or position, and maintains sharp, stable and natural rendering throughout the sequence. There are no visible frame drops, artifacts or unintended motion. ~ Moderate: The region of interest is approximately correct, and the attribute remains inferable, but the sequence suffers from minor blur, incomplete framing, slight instability mild unnatural motion, or sometimes deviates from the textual instruction and produces a plausible but unaligned or self-directed visual interpretation, limiting confident interpretation. ‚úóBad: The target region is incorrect or ambiguous, the attribute cannot be reliably inferred, or the video exhibits severe artifacts: abrupt frame jumps, major jitter, unintended zoom or crop, extraneous objects interfering, or conspicuous quality degradation that obstructs the reasoning task altogether. Data Source. We sample data from the V‚àóBench [71], which provides a comprehensive set of evaluation dimensions including spatial relationship and color/attribute consistency tasks. Example and Analysis. We illustrate typical behaviors of Veo-3 in visual detail reasoning through four representative cases in Figure 3. In case I, the model performs well in localizing the target: although it does not strictly execute the ‚Äúzoom in‚Äù instruction, it instead achieves an equivalent visual outcome through a semantically consistent motion with a person‚Äôs hand. This slight deviation suggests that the model may exhibit certain generation preferences in how it interprets and realizes spatial instructions, possibly reflecting stylistic tendencies learned from training data. In cases II and III, the model achieves better success rates when the targets are visually salient and contextually distinct. For the handbag and dog-motorcycle scenes, Veo-3 attends to the correct regions and maintains smooth temporal coherence. However, when the object (e.g., the motorcycle) is small or surrounded by distracting elements, the model occasionally fails to locate it accurately, indicating limited fine-grained spatial discrimination in cluttered scenes. In case IV, when the target object is tiny and visually indistinct, Veo-3 cannot identify it even with explicit positional hints, highlighting that the model‚Äôs perceptual grounding and reasoning weaken sharply when object size and salience are too low for reliable attention. Takeaway 1 Veo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded targets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic generation biases that lead to plausible yet instruction-divergent outcomes. 2.3 Visual Trace Reasoning Task Description and Evaluated Aspects. The visual trace reasoning",
  "Takeaway 1 Veo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded targets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic generation biases that lead to plausible yet instruction-divergent outcomes. 2.3 Visual Trace Reasoning Task Description and Evaluated Aspects. The visual trace reasoning category evaluates a model‚Äôs ability to represent and maintain causal continuity across sequential actions. Typical tasks include maze navigation, path following, and multi-step object manipulation, where the video must visually encode a coherent sequence of intermediate decisions that lead to the correct goal. Performance is assessed based on two major aspects: (i) temporal coherence, which is the smoothness and logical 6",
  "Input Image: Reasoning Video: Starting at the red dot in the middle-right cell, animate step-by- step moves: go down 1 cell, left 1, left 1, up 1, and up 1, drawing arrows for each step and finishing with a glow around the final cell. Static shot. I. Question: Text-to-Video Prompt: Q: Starting from the red dot, follow the given movement instructions and determine the final position. Down 1, left 1, left 1, up 1, up 1. A: A 1st frame Input Image: Reasoning Video: II. Question‚Ä†: Text-to-Video Prompt: 1st frame III. Question‚Ä†: Text-to-Video Prompt: IV. Question: Animate the elf moving step by step toward the gift while carefully avoiding the icy frozen lake. Highlight the successful path and end with the elf standing beside the gift. Static shot. Q: The character must avoid falling into the frozen lake and reach the gift pack safely. Input Image: Reasoning Video: 1st frame Animate the red triangle moving step by step toward the white printer, picking it up once it reaches it. Then have the triangle carry the printer upward and place it on the brown area representing the table. End with a subtle highlight around the printer to show it is toggled on. Static shot. Q: Move the character (red triangle) to pick up the white printer and place it anywhere on the desk. Text-to-Video Prompt: Input Image: Reasoning Video: 1st frame Animate a bright path tracing from the blue point at the top through the maze‚Äôs open corridors toward the red point at the bottom, highlighting each green numbered mark it passes. Keep the maze and all walls fixed while the glowing path moves smoothly through the correct route. Static shot. Q: The given picture is a maze, and the black lines represent walls that cannot be walked. Now you want to walk from the blue point to the red point. Is there a feasible path? If so, which of the green marks numbered 1-5 In the picture must be passed in the path? A: Yes, 3. ‚úóBad ‚úìGood Success Rate: 17% ‚úóBad ‚úóBad Figure 4: Showcase of Visual Trace Reasoning by Veo-3 (Part I). It shows short-horizon path- following successes, object-grounding failures, and a certain bias that causes step omissions/mistakes in multi-step traces. ‚Ä† The ground-truth answers of cases II and III are intuitive and non-unique, which are omitted to highlight the key reasoning behaviors. 7",
  "Input Image: Reasoning Video: Create a 2D animation based on the provided diagram. The red arrow is the initial arrow, and the green arrow is the final arrow. The arrow can move in four directions (forward, backward, left, right), where 'forward' always refers to the current direction the arrow is pointing. After each movement, the arrow's direction is updated to the direction of movement. Movement commands: - The red arrow moves forward for 1 unit. - The red arrow moves left for 1 unit (relative to its new current direction after step 1). Then turns green. Scene: - No change in scene composition. - No change in the layout of the diagram. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. V. Question: Text-to-Video Prompt: Q: In the diagram, the red arrow is the initial arrow, and the green arrow is the final arrow. The arrow can move in four directions (forward, backward, left, right), where 'forward' always refers to the current direction the arrow is pointing. After each movement, the arrow's direction is updated to the direction of movement. Which of the following paths can make the arrow move from the starting position to the ending position? A: (Forward, 1 unit) - (Left, 1 unit) 1st frame Input Image: Reasoning Video: VI. Question: Text-to-Video Prompt: 1st frame Two small characters start from the same purple origin at the same time, and move along the red and green paths toward another purple destination at the same speed. Static camera, no zoom, no pan. Q: What are the advantages of the green route and the red route respectively? ‚úóBad ‚úóBad Figure 5: Showcase of Visual Trace Reasoning (Part II) by Veo-3. The examples highlight long-horizon planning breakdowns, inconsistent arrow/trajectory rendering, and failures to preserve comparative or sequential information across frames. progression between consecutive steps; and (ii) goal consistency, which means whether the full sequence visually completes the intended reasoning trajectory without deviation or contradiction. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: ‚úìGood: Each movement step is depicted continuously and logically toward the correct goal. The motion is smooth, temporally consistent, and follows causal order with no skipping, stuttering, or direction reversal. ~ Moderate: The overall trajectory roughly aligns with the intended sequence, but small discontinuities, timing irregularities, or partial missteps occur. The reasoning path remains interpretable, and the goal can still be inferred. ‚úóBad: Key steps are missing, reversed, or illogical. The sequence shows abrupt jumps, inconsistent object trajectories, or goal confusion, breaking the temporal and causal coherence of the reasoning process. Data Source. We select samples from MVoT [41], FrozenLake [8, 72], MiniBehavior [32], RBench- V [25], SpatialViz-Bench [66], and OmniSpatial [29], which provide controlled multi-step envi- ronments for evaluating temporal reasoning, sequential planning, and causal continuity in visual simulations. 8",
  "temporal reasoning, sequential planning, and causal continuity in visual simulations. 8",
  "Example and Analysis. In Figure 4 and Figure 5, we showcase six representative visual-trace examples. In case I, the model repeatedly fails to execute the exact step sequence and instead drifts toward a visually salient central cell. However, case II is one of the few successes: the model can produce a coherent step-by-step path in simple, low-branching settings, but this behavior is not robust across trials. Case III largely fails, where the model often does not ground the specified object (printer), sometimes hallucinating its appearance or placement rather than performing a consistent pickup-and-place. Case IV shows near-uniform failure on long-horizon, highly branched navigation: outputs contain wrong turns, discontinuities, and no faithful global plan. Case V reveals difficulty grounding abstract movement rules, producing inconsistent arrow trajectories. Case VI produces visually plausible motions along individual paths but fails to preserve or present the comparative information required for contrastive reasoning. Taken together, these examples indicate that the model can simulate locally coherent short traces but systematically fails at long-horizon planning, rule-grounded execution, and object-persistent manipulations. Takeaway 2 Veo-3 can produce locally coherent, short-horizon trace animations in simple, low-branching scenarios, but it does not reliably execute long-horizon plans or rule-grounded sequences. 2.4 Real-World Spatial Reasoning Task Description and Evaluated Aspects. This task investigates Veo-3 [21]‚Äôs ability to perceive and maintain spatial relations within natural scenes, with a focus on reasoning about viewpoint change, orientation consistency, and reference-frame alignment. We assess whether the model preserves a stable global coordinate frame and coherent scene orientation under varying viewpoints, and whether objects retain correct relative positions and orientations with respect to each other across different views. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: ‚úìGood: Scene orientation, reference frame, and viewpoint are consistent and correctly represent spatial relations. The camera remains steady and the motion is natural. ~ Moderate: Scene roughly matches the instruction but contains small perspective errors, unnatural transitions, or partial mirroring. Motion remains interpretable but not physically coherent. ‚úóBad: Reference frame or direction is wrong; viewpoint shifts abruptly or inconsistently. Video suffers from strong camera drift, disorienting motion, or spatial chaos. Data Source. To evaluate on orientation and layout reasoning, we specifically sample data from MMSI-Bench [74]. Also, the tasks of perspective taking and spatial interaction are selected from the OmniSpatial dataset [29]. Example and Analysis. As shown in Figure 6, Veo-3 can correctly handle basic spatial layouts in case I, but struggles with complex viewpoints or orientation changes in case II. The perspective transformations are sometimes inaccurate or even incorrect, suggesting that the model tends to prioritize visual plausibility over precise spatial reasoning, which hinders further reasoning in case IV. Moreover, case III demonstrates that Veo-3 has difficulty understanding depth, further limiting its spatial reasoning capability. Takeaway 3 While Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability remains insufficient for handling more complex spatial understanding tasks. 9",
  "While Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability remains insufficient for handling more complex spatial understanding tasks. 9",
  "Input Image: Reasoning Video: ‚úìGood Success Rate: 33% I. Question: Text-to-Video Prompt: 1st frame Input Image: Reasoning Video: An arrow points from the player wearing jersey number 10 in purpleto the basketball. Static camera view, no zoom or pan. II. Question: Text-to-Video Prompt: Question: From the perspective of the player wearing jersey number 10 in purple, where is the basketball? A: Left front. 1st frame Input Image: Reasoning Video: The camera slowly and smoothly elevates from its current isometric view, gradually rising upwards while maintaining focus on the apartment layout. It continues to ascend until it reaches a complete overhead, bird's-eye perspective, providing a full top-down view of the entire floor plan, displaying all rooms and furniture clearly from above. The movement is fluid and controlled, ending with a static shot from the high vantage point. IV. Question: Text-to-Video Prompt: Q: If you are facing the washing machine, how should you walk to the stove and face the stove? A: Turn around and go straight, then turn left and go straight, then turn right and go straight, finally turn left to face the stove. 1st frame A red arrow point from the green chair toward the balcony. Another red arrow point from the door to the balcony. Static camera view, no zoom or pan. Question: The balcony is north relative to the door, in which direction on the balcony is the chair? A: Southwest. Input Image: Reasoning Video: III. Question: Text-to-Video Prompt: 1st frame ‚úóBad The image transitions to a depth-map of the scene: Darker colors represent pixels further from the camera, lighter colors represent pixels closer to the camera. The exact color map to use is provided on the right side of the image. Static scene, no pan, no zoom, no dolly. Q: From the dunker's viewpoint, which white- uniformed player is the farthest from them? A: Five. ~ Moderate Success Rate: 17% ~ Moderate Success Rate: 20% Figure 6: Showcase of Real-World Spatial Reasoning by Veo-3. Although Veo-3 can reason about simple spatial layouts, it still struggles to maintain consistency under complex perspective or orientation changes. 10",
  "Input Image: Reasoning Video: ‚úìGood Success Rate: 83% I. Question: Text-to-Video Prompt: 1st frame Input Image: Reasoning Video: A hand moves the object to the left along the y-axis and then moves it up. Static camera view, no zoom or pan, and the perspective of the object remains unchanged throughout. II. Question: Text-to-Video Prompt: Q: Move the object to the left along the y-axis and up. 1st frame Input Image: Reasoning Video: ‚úóBad Smoothly zoom in to the \"Initial State\" figure. The yellow block, starting at (1,0,0), moves one unit in the positive Y direction to position (1,1,0). Then, move back to (1,0,0). The cyan block, starting at (2,0,0), moves one unit in the positive Y direction to position (2,1,0), exchange the position with the purple block. Static shot. No pan. No glitches, noise, or artifacts. IV. Question: Text-to-Video Prompt: Q: The sequence of moves that turns the first cube stack into the final one is _______. A: (1,0,0)y+ (1,1,0)y- (2,1,0)y+ 1st frame A: D. ‚úóBad Move the object up. Static camera view, no zoom or pan, and the perspective of the object remains unchanged throughout. Q: Move the object up. A: D. Input Image: Reasoning Video: III. Question: Text-to-Video Prompt: 1st frame ‚úóBad The net is folded to form a single cube, with folding edges clearly shown. Static camera perspective, no zoom or pan. Q: Check out a net with 6 faces below: Can the net be folded to form a cube, yes or no? A: Yes. Figure 7: Showcase of 3D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows certain potential in basic 3D geometry reasoning, its performance remains unstable for complex geometry transformations. 11",
  "cropped_0.jpg Input Image: Reasoning Video: Rotate only the left 3D shape in place with a small, constant-speed yaw at fixed scale; keep everything else static. V. Question: Text-to-Video Prompt: Q: From any angle, which one on the right is not a view of the three dimensional shape given on the left? A: C 1st frame ~ Moderate Success Rate: 17% Figure 8: Showcase of 3D Geometry Reasoning by Veo-3 (Part II). The model often generates misaligned or self-intersecting structures, compromising geometric consistency. 2.5 3D Geometry Reasoning Task Description and Evaluated Aspects. We also evaluate Veo-3‚Äôs potential on 3D geometry reasoning tasks, such as geometric object motion and three-dimensional structural transformations like reconstructing a cube net. The assessment focuses on three key dimensions: geometric accuracy, structural completeness throughout the transformation, and visual continuity across frames. Definition of Good / Moderate / Bad. We categorize the model‚Äôs performance into three levels: ‚úìGood: Transformations like folding, rotation and assembly are geometrically correct, visually smooth, and continuous, maintaining structural integrity and realistic motion. No broken edges, jumps, or spatial artifacts. ~ Moderate: Transformations are partially correct but show local misalignment, unrealistic deformation, or discontinuous motion; geometry is roughly interpretable but imperfect. ‚úóBad: Transformation fails. For example, wrong fold, structure collapse, or impossible geometry. Motion is erratic, discontinuous, or visually implausible, breaking the sense of physical realism. Data Source. To construct diverse and representative evaluation data, we adapt tasks from estab- lished geometric spatial reasoning datasets, including the 3D-Text-Instruct and Folding Nets subsets of the STARE benchmark [43], the BlockMoving subset from the SpatialViz-Bench [66], as well as VisuLogic [73] benchmark. Example and Analysis. We showcase the results of Veo-3 on 3D geometry reasoning tasks in Figure 7 and Figure 8. Veo-3 demonstrates a degree of potential on 3D geometry reasoning, performing reasonably well on simple, single-step geometric transformations, as shown in case I. However, its performance degrades noticeably when facing multi-step or compositionally complex transformations in case II. As presented in cases III and V, the model frequently produces misaligned or self-intersecting structures, leading to a loss of geometric consistency. Further observations in case IV, show that while the model can partially understand the geometric shape of individual objects, it lacks a coherent understanding of coordinate systems and the spatial relationships among multiple objects. Takeaway 4 Veo-3 exhibits emerging reasoning potential on basic 3D transformations but breaks down on complex or multi-step geometry, often yielding misaligned or self-intersecting structures. Its 12",
  "Takeaway 5 3D geometric reasoning remains fragile, revealing substantial gaps in its ability to function as a reliable 3D geometry reasoner. 2.6 2D Geometry Reasoning Task Description and Evaluated Aspects. To assess a model‚Äôs competence in 2D geometric reasoning, we evaluate its zero-shot performance on planar geometric construction tasks. These tasks involve drawing geometric relations by connecting points, adding auxiliary lines, and moving geometric shapes. The evaluation focuses on whether the generated constructions or movements accurately reflect the described geometric relationships and adhere to the given instructions, while maintaining smooth, stable operations that ensure visual clarity and coherence throughout the process. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: ‚úìGood: Constructions and movements are geometrically accurate and visually smooth. Endpoints, intersections, angles, and motion trajectories align correctly with the instructions. Both drawing and movement processes are stable, fluid, and natural, resembling human sketching or manipulation. ~ Moderate: Constructions and movements roughly follow the intended geometry but exhibit minor inaccuracies in line placement, shape alignment, trajectory, or smoothness. Some local jitter or abrupt motion may appear, but the overall structure and motion remain interpretable. ‚úóBad: Constructions or movements deviate substantially from geometric correctness. Lines or shapes may be misplaced, disconnected, or moved in a chaotic or discontinuous manner (e.g., jittering, overlapping, or distorted paths), leading to visual instability and loss of interpretability. Data Source. The evaluation data are drawn from multiple established sources, including the Geo170k dataset [18], the VarsityTutors subset of Math-PUMA [85] dataset, the line-connection subset of RBench-V [25], the MAVIS-Gen [80], Tangram Puzzle subsets of the STARE [43] benchmark, and data from VAT [46]. Example and Analysis. The representative examples of the 2D geometry reasoning task are presented in Figures 9 and 10. Veo-3 demonstrates a foundational capability for simple geometric connection tasks, correctly identifying and linking elements in straightforward scenarios like in case III. However, this basic competence is inconsistent. The model often prioritizes producing visually symmetric or semantically meaningful patterns rather than strictly adhering to geometric instructions (cases I and II). Furthermore, case II reveals instances where the model unintentionally modifies the original figures, indicating a limited awareness of geometric constraints and poor spatial consistency. When tackling more complex connection tasks, the model frequently fails to interpret the intended drawing order or point indices, resulting in incorrect connection sequences, as demonstrated in cases V, VI, and VII. This is often coupled with an inability to control task termination, as the model tends to continue drawing beyond the required constructions. Finally, for tasks involving the movement of geometric shapes in cases IV and VIII, the model struggles to maintain geometric structural consistency throughout the motion. Takeaway 6 Veo-3 shows initial 2D geometric reasoning ability but still falls short of consistent, constraint- aware geometric understanding, remaining far from a robust geometric reasoner. 13",
  "aware geometric understanding, remaining far from a robust geometric reasoner. 13",
  "Input Image: Reasoning Video: A line connecting point A and point C. The video ends once the connection process is complete. Static view, no zoom or pan. I. Question: Text-to-Video Prompt: Q: In the figure shown, let 'n' represent the length of side AB of the inscribed rectangle ABCD, where n is an undetermined value. With BC equal to 6.0 and the diameter of circle O equal to 10.0, what is the value of 'n‚Äô? A: 8 1st frame Input Image: Reasoning Video: II. Question: Text-to-Video Prompt: 1st frame III. Question: Text-to-Video Prompt: IV. Question: ~ Moderate Success Rate: 83% Smoothly connecting point M and point N. The video ends once the connection process is complete. Static view, no zoom or pan. Q: The figure presented depicts a square designated as ABCD. Within this square, point M is identified as the midpoint of the side AB, while point N is the midpoint of the opposing side CD. Additionally, point O is located at the midpoint of segment CN. Your task is to draw the segment MO. It is given that the length of segment AM is represented by t. The objective is to determine which of the following expressions accurately represents the length of the segment MO in terms of t. A: 17 4 ùë° Input Image: Reasoning Video: 1st frame Smoothly connecting point C and point D with a line. The video ends once the connection process is complete. Static view, no zoom or pan. Q: AB equals to 8.0. What would the area of the entire shape ABCD be? A: 62.87 Text-to-Video Prompt: Input Image: Reasoning Video: 1st frame Place piece A with its upper- left corner at (x, y) = (0, 3). Q: Check out an Tangram puzzle below. The left panel is an empty Tangram puzzle, while the right panel shows available pieces to complete the puzzle. Keep in mind that you can rotate or flip the pieces. Can the Tangram puzzle be completed with the available pieces, yes or no? A: Yes. ‚úóBad ‚úóBad ~ Moderate Success Rate: 33% Figure 9: Showcase of 2D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows potential in recognizing simple patterns, it lacks the robust constraint awareness essential for accurate geometric manipulation. 14",
  "Input Image: Reasoning Video: Animate the dots connecting sequentially from 1 to 25, each straight line appearing smoothly until the full outline emerges. Keep the background with the smiling sun and plants unchanged. Static shot. V. Question: Text-to-Video Prompt: Q: Connect the black dots in the image sequentially with straight lines according to the edge numbers (i.e., connect dot 1 to dot 2, dot 2 to dot 3, and so on). The final result will form a simple line drawing. What does this drawing represent? A: Duck. 1st frame Input Image: Reasoning Video: VI. Question: Text-to-Video Prompt: 1st frame VII. Question: Text-to-Video Prompt: ~ Moderate Success Rate: 83% Input Image: Reasoning Video: 1st frame ‚úóBad Animate the numbered dots connecting sequentially from 1 to 118, each straight line appearing smoothly as the outline gradually emerges. Keep all numbers and dots visible while the connecting lines form step by step. Static shot. Q: Connect the black dots in the image sequentially with straight lines according to the edge numbers (i.e., connect dot 1 to dot 2, dot 2 to dot 3, and so on). The final result will form a simple line drawing. What does this drawing represent? A: Lion. Animate the numbered dots connecting sequentially from 1 to 118, each straight line appearing smoothly as the outline gradually emerges. Keep all numbers and dots visible while the connecting lines form step by step. Static shot. Q: Connect the black dots in the image sequentially with straight lines according to the edge numbers (i.e., connect dot 1 to dot 2, dot 2 to dot 3, and so on). The final result will form a simple line drawing. What does this drawing represent? A: Bird. ‚úóBad cropped_0.jpg Input Image: Reasoning Video: Pan vertically at a steady speed to center the top yellow line, then the bottom one, keeping both visible with identical scale and exposure. Ensure all elements stay unchanged and finish with a full view showing both lines together. VIII. Question: Text-to-Video Prompt: Q: Is the top yellow line shorter than the bottom yellow line? A: No. 1st frame ‚úóBad Figure 10: Showcase of 2D Geometry Reasoning by Veo-3 (Part II). Veo-3‚Äôs reasoning abilities are further challenged by complex sequential instructions and the need to preserve structural integrity. 15",
  "Input Image: Reasoning Video: Show the rough semicircular track with height label h and a block at P; release it, add faint friction streaks as it slides down and up the right side, stopping below the rim. Show the move quickly and completely. Static shot. I. Question: Text-to-Video Prompt: Q: The figure shows a rough semicircular track whose ends are at a vertical height h. A block placed at point P at one end of the track is released from rest and slides past the bottom of the track. Which of the following is true of the height to which the block rises on the other side of the track? A: It is between zero and h; the exact height depends on how much energy is lost to friction. 1st frame Input Image: Reasoning Video: II. Question: Text-to-Video Prompt: 1st frame III. Question: Text-to-Video Prompt: IV. Question: Animate the red ball moving along the blue arrow's direction, bouncing off the black walls according to reflection rules, keeping speed consistent. Continue its path upward until it reaches and collides with one of the numbered top bricks. Static shot. Q: The red ball moves in the direction indicated by the blue arrow and bounces off the black side walls upon collision; the component of its velocity perpendicular to the wall reverses in direction but maintains its magnitude, while the component parallel to the wall remains unchanged. Based on this behavior, please estimate which numbered brick (from 1 to 10) at the top the red ball will hit first. A: 1. Input Image: Reasoning Video: 1st frame Dynamically depict the attraction between magnets, paying attention to speed and intensity. Static shot. Q: Think about the magnetic force between the magnets in each pair. A: The magnetic force is stronger in Pair 2. Text-to-Video Prompt: Input Image: Reasoning Video: 1st frame The orange gear rotates counterclockwise in the given view. Animate the provided planetary gear system. The orange gear is fixed on the green gear. The central orange sun gear rotates counterclockwise, driving the yellow planet gear. All components must maintain their relative axial positions and proper gear meshing. The camera is static, with no zoom or pan. Q: The orange gear is fixed on the stationary green gear. If the orange gear rotates counterclockwise in the given view, what is the motion of the yellow gear relative to the orange gear? A: Clockwise rotation. Counterclockwise revolution. ‚úóBad ‚úóBad ‚úóBad ~ Moderate Success Rate: 83% Figure 11: Showcase of Physics-based Reasoning by Veo-3. The physics scenarios demonstrate lo- cally plausible dynamics and reflections, alongside systematic quantitative and causal inconsistencies under frictional, force-driven, or constrained interactions. 16",
  "2.7 Physics-based Reasoning Task Description and Evaluated Aspects. The physics-based reasoning category assesses a model‚Äôs capacity to depict and reason about motion dynamics, physical causality, and rule-based interactions between objects. Tasks in this group involve gravity, collisions, reflection, momentum, or energy conservation, requiring the model to generate physically plausible and temporally coherent motion. Evaluation focuses on two complementary aspects: (i) physical plausibility, which means whether the simulated motion obeys common physical principles; and (ii) causal correctness, which is whether object interactions are consistent with the underlying cause-and-effect relationships described in the prompt. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: ‚úìGood: The motion sequence adheres to physical laws such as gravity, momentum, and energy conservation. Object interactions are realistic and temporally smooth, and the visual outcome remains coherent and credible throughout. ~ Moderate: The physical relations are approximately correct but include minor inconsisten- cies, such as irregular acceleration, timing mismatch, or slight violation of conservation. The overall motion remains interpretable and visually plausible. ‚úóBad: The motion is physically implausible or visually chaotic‚Äîobjects float, stop abruptly, or behave contrary to basic causal principles. Severe artifacts or temporal discontinuities disrupt the perception of a coherent physical process. Data Source. We draw samples from MMMU [77], ScienceQA [49], and related physical reasoning subsets of RBench-V [26] and SpatialViz-Bench [66], covering scenarios such as object collisions, pendulum motion, frictional sliding, and optical or magnetic interactions. Example and Analysis. Figure 11 presents four representative physics tasks and their outputs. Case I shows that the model can produce a visually coherent slide, but the behavior violates basic physical laws. Case II is the most reliable, where reflections and general trajectory shape are rendered plausibly and the task attains a high success rate, although small angular or timing offsets are common. In case III, the model conveys attraction through motion, yet the depicted dynamics do not reliably track the intended force magnitudes or causal ordering. Finally, case IV exposes structural failures, incorrect meshing, inconsistent relative rotations, and nonphysical contact behavior occur frequently, so the mechanical constraints are not respected. Overall, the model can synthesize locally plausible dynamics and handle simple reflection rules, but it fails to maintain quantitative physical constraints and causal fidelity in frictional, force-driven, or mechanically constrained scenarios. Takeaway 7 Veo-3 often generates visually plausible short-term dynamics, but it systematically fails to preserve quantitative physical constraints (energy, momentum), causal ordering, and contact mechanics in frictional, force-driven, or mechanically constrained scenarios. Thus, its outputs are somewhat useful for qualitative illustration but are not reliable for quantitative physics inference or causal prediction. 2.8 Rotation Reasoning Task Description and Evaluated Aspects. The rotation reasoning task assesses the ability to reason about planar object rotation and maintain consistent spatial grounding under rotational transformations, thereby supporting subsequent reasoning processes. In each instance, the model is required to accurately rotate target objects within a fixed 2D plane while preserving the overall scene structure and structural consistency, followed by performing reasoning tasks like grounding and OCR. The evaluation focuses on both the accuracy of the rotation",
  "thereby supporting subsequent reasoning processes. In each instance, the model is required to accurately rotate target objects within a fixed 2D plane while preserving the overall scene structure and structural consistency, followed by performing reasoning tasks like grounding and OCR. The evaluation focuses on both the accuracy of the rotation in terms of angle and direction, and the precision of the resulting reasoning tasks. 17",
  "Input Image: Reasoning Video: I. Question: Text-to-Video Prompt: 1st frame Input Image: Reasoning Video: Rotate the scene 180 degrees clockwise. Then draw a bounding box around the leftmost vending machine. II. Question: Text-to-Video Prompt: Q: Looking up from the floor, how many rows of drinks are in the leftmost vending machine? A: 2 1st frame Input Image: Reasoning Video: ‚úóBad The entire 'Original' grid figure performs one smooth, continuous 360-degree rotation clockwise within its own 2D plane. The camera stays static, with no pan. IV. Question: Text-to-Video Prompt: QÔºöWhich grid can be obtained by rotating the grid only? 1st frame ‚úóBad Rotate the scene 45 degrees clockwise. Then draw bounding boxes around the frontmost skiing character. Q: Is the frontmost skier wearing a scarf? A: No. Input Image: Reasoning Video: III. Question: Text-to-Video Prompt: 1st frame ‚úóBad Rotate the video frame 90 degrees counterclockwise in the 2D plane, then draw bounding boxes around each 'IKEA' label. Q: On which floors are the 'IKEA' labels located? A: One on the top floor, one on the middle floor, and one on the bottom floor. ~ Moderate Success Rate: 83% A: A Figure 12: Showcase of Rotation Reasoning by Veo-3. Veo-3 struggles in complex scenes. However, its foundational grasp of simple rotations signals its potential to support rotation-based reasoning tasks. 18",
  "Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: ‚úìGood: The rotation is accurate, complete, and strictly confined to the 2D plane, with no extraneous scene motion. The following reasoning tasks are completed correctly. Target objects remain precisely grounded after rotation. ~ Moderate: The rotation is largely correct but may be incomplete or slightly off-angle, though still confined to the 2D plane. The following reasoning tasks are mostly completed. Minor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or object grounding. ‚úóBad: The model fails to perform the correct rotation, extends the transformation into 3D space, or introduces substantial scene distortion. Cannot complete the following reasoning task. The original 2D structure is altered, leading to inaccurate grounding of the target objects. Data Source. To specifically assess the rotation reasoning task, we recruit some PhD-level experts with deep expertise in text-image reasoning to design the evaluation data manually, followed by the necessary review process, as mentioned in Section 3.2. Each question is designed following the principle that it must involve a 2D rotation to reach the correct solution, ensuring the task genuinely probes rotational understanding rather than simple visual matching. Moreover, we sample data from the 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions for the video models. Example and Analysis. The results are shown in Figure 12. In case I, we find that Veo-3 handles small-angle rotations and simple planar scenes reasonably well, demonstrating a basic grasp of rotational motion. However, in more complex scenarios like cases II, III, and IV, the model often ignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect rotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such as OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment. These observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather than principle-driven. However, as it demonstrates a partial understanding of planar rotation, this can to some extent facilitate subsequent reasoning tasks. Takeaway 8 Veo-3 exhibits only a superficial understanding of rotation reasoning. While it can approximate small planar rotations, it fails to preserve geometric consistency under larger or compound transformations. 2.9 Table and Chart Reasoning Task Description and Evaluated Aspects. The table and chart reasoning task requires the model to identify and focus on the key elements within visualizations or tabular data. For evaluation, we further consider how effectively the model identifies the regions relevant to the query and whether it can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and proper scaling. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: ‚úìGood: Camera precisely focuses on the correct chart or table segment, smoothly high- lighting or zooming into the queried data (e.g., correct year, category, or value). Motion is continuous, the chart and table remain clear, and no distortion or overexposure occurs. ~ Moderate: Camera approximately focuses on the right region but partially misses",
  "focuses on the correct chart or table segment, smoothly high- lighting or zooming into the queried data (e.g., correct year, category, or value). Motion is continuous, the chart and table remain clear, and no distortion or overexposure occurs. ~ Moderate: Camera approximately focuses on the right region but partially misses boundaries, introduces slight blur, or transitions abruptly. Data can still be inferred. ‚úóBad: Video fails to locate the correct region or changes the chart or table geometry unnaturally. Motion jitter, scaling errors, or artifacts make data unreadable or misleading. 19",
  "Input Image: Reasoning Video: Start with smoothly zooming in to focus on the 'Nova Scotia' row. Then, smoothly zoom out to the full view of the chart. End with smoothly zooming in to focus on the 'Manitoba' row. The chart itself, including all its data, lines, and labels, must remain completely static and unchanged throughout the video. I. Question: Text-to-Video Prompt: Q: What is the sum of footwear manufacturing establishments in Nova Scotia and Mantioba as of December 2020? A: 3 1st frame Input Image: Reasoning Video: II. Question: Text-to-Video Prompt: 1st frame III. Question: Text-to-Video Prompt: IV. Question: Start with a static, full view of the chart. Then, smoothly zoom the camera in to focus on the vertical area corresponding to the year 2014. The chart itself, including all its data, lines, and labels, must remain completely static and unchanged throughout the video. Q: In the year 2014, which opinion is dominant? A: Unfavorable. Input Image: Reasoning Video: 1st frame Zoom in to focus on the smallest section in the chart. The chart itself, including all its data, lines, and labels, must remain completely static and unchanged throughout the video. Q: What' the color of smallest section in the chart? A: Gray. Text-to-Video Prompt: Input Image: Reasoning Video: 1st frame Draw a bounding box around the end market for the Engineered Systems segment. The table itself, including all its text, lines, and labels, must remain completely static and unchanged throughout the video. Q: What is the end market for the Engineered Systems segment? A: Printing & Identification, Industrials. ‚úóBad ~ Moderate Success Rate: 83% ‚úóBad ‚úóBad Figure 13: Showcase of Table and Chart Reasoning by Veo-3. Veo-3 demonstrates an initial ability to focus on relevant data regions but lacks the precision and consistency required for reliable visual analysis. 20",
  "Data Source. We use samples from the ChartQA [52] dataset and TableVQA-Bench [34]. Example and Analysis. For charts, as presented in cases I, II and III in Figure 13, Veo-3 can often zoom into an approximately correct region but lacks the precision needed to accurately locate the queried data. For tables, as shown in case IV, Veo-3 fails to correctly identify the required element and tends to select entries randomly. The model also frequently adds, modifies, or distorts existing chart and table elements, resulting in visual inconsistencies that undermine the accuracy of chart interpretation. Takeaway 9 Veo-3 demonstrates emerging competence and potential in structured visual understanding, but still falls short of functioning as a precise and reliable chart-table reasoner. 2.10 Object Counting Reasoning Task Description and Evaluated Aspects. In this category, we focus on the ability to accurately enumerate objects within a 2D or 3D scene. In each instance, the model is required to identify, ground, and count target objects, typically by highlighting, drawing bounding boxes, applying numerical labels, or panning. The evaluation focuses on the accuracy of the count and the precision of the spatial grounding, performed within a scene that remains static or experiences only minimal motion, ensuring the counting process is not influenced. Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: ‚úìGood: The model precisely highlights, draws bounding boxes around, or labels the objects with correct numbers, and performs smooth and controlled panning when necessary to cover all targets. Motion is continuous, and the scene remains static or experiences only slight changes that do not influence the counting process. ~ Moderate: The model approximately highlights or draws bounding boxes around the objects, or performs panning with minor instability or incomplete coverage. Objects or the scene may move or change slightly, but this does not strongly affect the counting process. ‚úóBad: The model fails to correctly highlight, label, or draw bounding boxes around the objects, or pans erratically such that parts of the scene are missed or revisited unnecessarily. Objects or the scene move or change substantially, severely affecting the counting process. Data Source. The 2D object counting data are sampled from the counting subset of RBench-V [25]. The 3D object counting data are from the Super-CLEVER dataset [45] and VAT [46]. Example and Analysis. The results are shown in Figures 14 and 15. In the 2D counting tasks from cases I to III, objects frequently move or change during the process, negatively impacting counting stability and accuracy. In the 3D counting tasks, Veo-3 successfully handles simple grounding and counting scenarios, as demonstrated in case V, but struggles with scenes involving complex materials or geometric variations in cases VI and VII, leading to inaccurate counts. Additionally, in the panning process of case VII, the camera fails to precisely move to the regions containing all target objects, further hindering the counting process. Takeaway 10 Veo-3 demonstrates basic counting capability but lacks the spatial control and robustness required for reliable object enumeration in dynamic or complex scenes. 21",
  "the camera fails to precisely move to the regions containing all target objects, further hindering the counting process. Takeaway 10 Veo-3 demonstrates basic counting capability but lacks the spatial control and robustness required for reliable object enumeration in dynamic or complex scenes. 21",
  "cropped_0.jpg Input Image: Reasoning Video: A scanner dot moves along the black line from bottom-left to top-right. As soon as this dot enters a new grid square, that entire square is instantly filled with yellow color and stays yellow. A square only turns yellow if the scanner dot on the line has entered it. Static camera, no zoom. I. Question: Text-to-Video Prompt: Q: How many unit squares does the line segment pass through in the given grid diagram? A: 16 1st frame cropped_0.jpg Input Image: Reasoning Video: Highlight only the rectangles in the figure with a bright yellow color. Not highlight any other shapes like squares, triangles, circles, or irregular polygons. Static camera, no zoom, no pan. II. Question: Text-to-Video Prompt: Q: How many rectangles are there in the figure? A: 8 1st frame ‚úóBad ‚úóBad cropped_0.jpg Input Image: Reasoning Video: Label all the fish with increasing numbers (1, 2, 3, ...). The fish keep static. Static camera, no zoom, no pan. III. Question: Text-to-Video Prompt: Q: How many rectangles are there in the figure? A: 18 1st frame ‚úóBad Figure 14: Showcase of 2D Object Counting Reasoning by Veo-3. Veo-3‚Äôs lack of spatial control often introduces object motion, undermining the stability and accuracy of the counting process. 2.11 GUI Reasoning Task Description and Evaluated Aspects. In the Graphical User Interface (GUI) reasoning task, we focus on the capability to understand and interact with graphical user interfaces across different operating systems, including Android, Linux, and Web environments. In each instance, the model is required to perform actions, such as clicking on specific UI elements. The evaluation focuses on the accuracy of the click and the temporal coherence of the interaction, ensuring the scene and irrelevant UI elements remain consistent. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: ‚úìGood: The click is precise, with no extraneous actions. No superfluous icons appear, and the original data and icons remain unchanged. ~ Moderate: The click is precise but may be accompanied by minor extraneous actions. Superfluous icons might appear but do not obscure the click target, and original data or icons show only slight alterations. 22",
  "cropped_0.jpg Input Image: Reasoning Video: Draw bounding boxes around the brown metal mountain bikes to the right of the origami crane. Static shot. V. Question: Text-to-Video Prompt: Q: There is a small yellow object that is to the left of the tiny metal motorbike; how many brown metal mountain bikes are to the right of it? A: 1 1st frame cropped_0.jpg Input Image: Reasoning Video: Draw bounding boxes around any matte tandem bikes and metal cruisers present in the scene. Static shot. VI. Question: Text-to-Video Prompt: Q: How many cyan things are matte tandem bikes or metal cruisers? A: 1 1st frame ‚úìGood Success Rate: 100% cropped_0.jpg Input Image: Reasoning Video: Pan smoothly to include both the lid‚Äìbody interface and the spout or cap in view at a fixed scale, keeping exposure steady and avoiding any visual or geometric changes. VII. Question: Text-to-Video Prompt: Q: How many burners are on the stove? A: 4 1st frame ~ Moderate Success Rate: 17% ‚úìGood Success Rate: 33% cropped_0.jpg Input Image: Reasoning Video: Draw bounding boxes around the tiny things that have the same material as the green motorbike. Static shot. IV. Question: Text-to-Video Prompt: Q: How many tiny things have the same material as the green motorbike? A: 1 1st frame ‚úóBad Figure 15: Showcase of 3D Object Counting Reasoning by Veo-3. Veo-3‚Äôs basic 3D counting abilities are challenged by complex materials, geometric variations, and imprecise camera control. 23",
  "cropped_0.jpg Input Image: Reasoning Video: Click the pkgs folder to collapse it. Static shot. I. Question: Text-to-Video Prompt: Q: Collapse the pkgs folder. A: 1st frame cropped_0.jpg Input Image: Reasoning Video: Click the calendar icon located to the right of the flight date options, next to the price display for June 6. Static shot. II. Question: Text-to-Video Prompt: Q: A calendar icon located to the right of the flight date options, next to the price display for June 6. 1st frame ‚úó Bad ‚úó Bad cropped_0.jpg Input Image: Reasoning Video: Click the navigation arrow located at the right edge of the browse by category carousel. Static shot. III. Question: Text-to-Video Prompt: Q: A navigation arrow located at the right edge of the browse by category carousel. 1st frame ‚úó Bad A: A: Figure 16: Showcase of GUI Reasoning by Veo-3. Veo-3‚Äôs attempts at graphical interface interaction exhibit visual inconsistencies and logical inaccuracies, indicating only a shallow grasp of underlying GUI logic. Note that the answer to each question is a bounding box. For visual clarity, screenshots with the ground-truth bounding boxes are shown. ‚úóBad: The click is imprecise or erratic. Original data and icons are significantly altered, hindering judgment and assessment. Data Source. The Linux data are selected from the Common Linux Screenshot subset of ScreenSpot- Pro [42], while the Android and Web data are drawn from the OS Android and OS Web subsections of MMBench-GUI [67], respectively. Example and Analysis. Across the three cases in Figure 16, Veo-3 fails to accurately capture the correct click position and often exhibits inconsistencies between the click location and the resulting on-screen effect. In addition, it occasionally alters or generates new icons and text, which can interfere with judgment. In the Web system in case III, however, the model demonstrates partial GUI responsiveness and provides some degree of visual feedback. 24",
  "cropped_0.jpg Input Image: Reasoning Video: Pan to the banana while keeping tray edges in view. Fix scale (banana ~two-thirds of the frame, axis horizontal). Sweep once along the inner concave edge from stem to tip at constant speed, then stop and hold at its midpoint. I. Question‚Ä†: Text-to-Video Prompt: Q: Which point corresponds to the affordance for manipulating the banana? 1st frame ~ Moderate Success Rate: 33% cropped_0.jpg Input Image: Reasoning Video: Keep the cucumber‚Äôs start and the pot opening in view. Sweep once from start to pot at fixed scale and speed, briefly dwelling at four evenly spaced waypoints (p1‚Üíp4) along the path, then hold on both endpoints. II. Question‚Ä†: Text-to-Video Prompt: Q: Which set of 4 points is a right trajectory when doing place a cucumber into a pot? 1st frame ‚úóBad cropped_0.jpg Input Image: Reasoning Video: Pan smoothly to include both the lid‚Äìbody interface and the spout or cap in view at a fixed scale, keeping exposure steady and avoiding any visual or geometric changes. III. Question‚Ä†: Text-to-Video Prompt: Q: Is the container sealed? A: No. 1st frame ~ Moderate Success Rate: 17% A: A: Figure 17: Showcase of Embodied Reasoning by Veo-3. It illustrates plausible static affordance detection in simple settings, common workaround/hallucination behaviors for dynamic manipulations, and failures to reliably localize or preserve manipulation-relevant context. ‚Ä† Green points in the answer image denote ground-truth points or trajectories. Takeaway 11 Veo-3 demonstrates a limited awareness of GUI click actions, imitating interaction behaviors without fully grasping the underlying functional logic. 2.12 Embodied Reasoning Task Description and Evaluated Aspects. This category evaluates the model‚Äôs potential to perceive and reason about object affordances and manipulation dynamics. It involves recognizing both static and dynamic affordances, as well as identifying manipulation-relevant object and scene attributes. Evaluation focuses on two aspects: (i) the generation of stable and contextually relevant visual sequences, and (ii) the maintenance of reasoning fidelity without resorting to implausible planning shortcuts or hallucinated interactions. 25",
  "Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: ‚úìGood: The sweep/framing covers all candidates fairly (equal or near-equal dwell), centers the manipulation-relevant geometry (e.g., handle + frame/gap, lid-body interface, hinge side) with crisp focus and stable scale; no cropping of key context; no content alterations. ~ Moderate: The view roughly includes the right region(s) but with minor bias or coverage issues: slight off-center, brief under-exposure of one candidate, small motion jitter, or shallow context (still enough to infer). ‚úóBad: The camera misses or biases the evidence (e.g., lingers only on one point, crops away the hinge/rail, over-zooms a non-relevant patch), introduces distortion/content edits, or produces footage from which a fair decision cannot be made. Data Source. We select samples from Robobench [51] for the analysis. In addition to a general understanding of static attributes, we also sample data to assess whether Veo-3 can perform direct reasoning on tasks involving the generation of static and dynamic affordances. Example and Analysis. As shown in Figure 17, Veo-3 demonstrates the ability to comprehend objects within real-world scenes. However, its capacity for assisting visual reasoning in embodied scenarios remains constrained by insufficient stability. As illustrated in case I, when provided with a clearly defined object for manipulation, Veo-3 is capable of generating plausible manipulation affor- dances. When it comes to dynamic affordances, Veo-3 tends to employ workarounds to compensate for its planning deficiencies, as evidenced in case II, where it generated a new cucumber instead of the intended object. With respect to static attributes, Veo-3 struggles to accurately differentiate visual prompts and misidentifies the position of containers. As shown in case III, the green box, intended to specify the location of the container, inadvertently led Veo-3 to produce hallucinations. Takeaway 12 Veo-3‚Äôs capabilities are currently limited to basic object recognition rather than true embodied reasoning. It lacks the necessary planning and stability to reliably interpret and act upon dynamic or spatially constrained instructions, indicating its limitations in understanding and reasoning of real-world interactions. 2.13 Medical Reasoning Task Description and Evaluated Aspects. This category assesses the model‚Äôs ability to localize lesions or structures, identify relevant attributes (e.g., side, lobe), recognize pathological patterns (e.g., ‚Äújump distribution‚Äù), and make binary decisions (e.g., presence or absence). The evaluation focuses on both the correctness of object manipulation and the visual stability of the surrounding regions. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: ‚úìGood: The camera cleanly settles on the correct anatomical level/lesion, with clear margins and readable context; motion is reasonable; no geometric distortion or content alteration. ~ Moderate: The view roughly covers the right area but is slightly off (partial coverage, mild blur, small framing mistakes). The general shape of the tissue or organ can still be observed. ‚úóBad: The video misses the target region or introduces distortions/crops that hide key cues. Tissues or organs begin to distort. Misleading results due to confusion of medical terminology. Data Source. We select samples representing different body parts from the ViTAR [9] dataset. Example and Analysis. We showcase",
  "still be observed. ‚úóBad: The video misses the target region or introduces distortions/crops that hide key cues. Tissues or organs begin to distort. Misleading results due to confusion of medical terminology. Data Source. We select samples representing different body parts from the ViTAR [9] dataset. Example and Analysis. We showcase the evaluation results in Figure 18. Veo-3 retains the ability to manipulate images when dealing with medical images. However, due to its lack of medical 26",
  "cropped_0.jpg Input Image: Reasoning Video: Show the full sagittal lumbar view, then sweep smoothly from L1 to S1 at constant speed without pausing. End on a view showing adjacent disc spaces, including narrow and normal levels. Keep image content and geometry unchanged. I. Question: Text-to-Video Prompt: Q: What is the distribution pattern of stenotic segments? A: Jump distribution. 1st frame cropped_0.jpg Input Image: Reasoning Video: Show the full PA chest view, then adjust framing to include both the heart silhouette and the widest inner thoracic diameter at a fixed scale. Keep contrast and geometry unchanged, holding steady for visual CTR estimation. II. Question: Text-to-Video Prompt: Q: Is cardiomegaly presentÔºü A: No. 1st frame ‚úóBad cropped_0.jpg Input Image: Reasoning Video: Show the full axial CT, then pan and zoom smoothly to the right lung so the nodule and nearby fissure appear together. Keep windowing standard and geometry unchanged. III. Question: Text-to-Video Prompt: Q: Which lobe contains the pulmonary nodule? A: Left lobe. 1st frame ‚úóBad ‚úóBad Figure 18: Showcase of Medical Reasoning by Veo-3. As shown in cases I and III, Veo-3 fails to maintain the shape of the rest of medical organization. Veo-3 also can not understand and precisely locate the mentioned medical terminology in the prompt, as demonstrated in case II. knowledge, Veo-3 struggles to accurately manipulate the correct objects when instructions include medical terminology. This phenomenon is evident across all cases. Furthermore, Veo-3 cannot model medical organs effectively. When performing operations such as zooming in, the medical images suffer from significant distortion, resulting in a substantial loss of detail. Takeaway 13 Veo-3‚Äôs failure to handle the reasoning in the medical domain, causing distortion even on simple zoom-ins, highlights its limited grasp of specialized, non-general knowledge. 27",
  "MME-COF Figure 19: Category Distribution. Table 1: Key Statistics of MME-COF. Statistic Number Total entries 59 Total categories 12 Max prompt length 124 Avg prompt length 36.7 Max entries per category 7 Avg entries per category 4.9 3 MME-COF 3.1 Benchmark Overview To standardize the empirical study and systematically evaluate the reasoning potential of state-of-the- art generative video models [21, 55, 56], we introduce MME-COF, which, to our knowledge, is the first benchmark specifically designed to reveal and quantify the reasoning potential of video models. 3.2 Benchmark Composition Data Curation and Distribution. Aligning with the task taxonomy in Section 2.1, the MME-COF benchmark is curated from the cases used in our empirical study. It comprises 59 curated entries and instruction prompts spanning 12 diverse reasoning categories. The key statistics of MME-COF and its overall composition are summarized in Table 1, Figure 2b and Figure 19. Review Process. Following the prompt design protocol in Section 2.1, all prompts undergo a two-stage review process. In the cross-validation phase, each prompt was independently reviewed by another expert to ensure semantic clarity, alignment with the intended reasoning task, and the absence of linguistic bias. In the final adjudication phase, discrepancies were jointly discussed and resolved through consensus. This multi-step procedure ensured that every prompt was conceptually precise, visually grounded, and fully aligned with the evaluation objectives of MME-COF. 3.3 Evaluation Protocol Models and Generation Settings. We evaluate the leading video models in a zero-shot setting, including Kling-v1 [38], Seedance-1.0-pro [19], Veo-3.0-preview [70], Veo-3.0-fast [70], Sora-2 [56], Sora-2-pro [56]. Each model generates six video samples per prompt, and final scores were computed as the mean across all samples. All videos are generated at a 16:9 aspect ratio. We adopt the default 8-second duration for the Sora and Veo series, while retaining the default 5-second length for Kling and Seedance. Note that, since most video models apply automated safety filters and content moderation, which may block sensitive content, we exclude videos that are suppressed by such filters from our evaluation. Evaluation Metrics. We employ Gemini-2.5-Pro [12] as an automatic verifier to evaluate each generated video. Gemini is prompted with the following evaluation criteria and returns structured scores between 0 and 4, where higher values indicate better performance: 1) Instruction Alignment (0-4): Measures how well the video follows the described structure and sequence in the prompt. A high score indicates that the visual steps faithfully reflect the textual instructions. 2) Temporal Consistency (0-4): Evaluates the smoothness and continuity between frames. Disjointed or abrupt transitions will lead to a lower score. 28",
  "Table 2: Model-level Overall and Per-dimension Performance on MME-COF. Mean scores and standard deviations are reported on a 0‚Äì4 scale, as graded by Gemini-2.5-Pro. Model Overall Instruction Alignment Temporal Consistency Visual Stability Content Fidelity Focus Relevance Kling-v1 [38] 0.64 ¬± 0.91 0.01 ¬± 0.09 0.15 ¬± 0.75 2.43 ¬± 1.86 0.21 ¬± 0.79 0.43 ¬± 1.07 Seedance-1.0-pro [19] 1.41 ¬± 1.51 0.30 ¬± 0.86 1.65 ¬± 1.57 2.00 ¬± 1.72 1.13 ¬± 1.65 1.98 ¬± 1.75 Veo-3.0-fast [21] 1.44 ¬± 1.51 0.56 ¬± 1.09 1.37 ¬± 1.51 1.88 ¬± 1.73 1.10 ¬± 1.52 2.27 ¬± 1.69 Veo-3.0-preview [21] 1.45 ¬± 1.50 0.54 ¬± 1.06 1.43 ¬± 1.53 1.89 ¬± 1.71 1.12 ¬± 1.49 2.26 ¬± 1.73 Sora-2-pro [56] 1.66 ¬± 1.53 0.48 ¬± 0.96 1.36 ¬± 1.59 2.39 ¬± 1.65 1.64 ¬± 1.72 2.44 ¬± 1.73 Sora-2 [56] 1.72 ¬± 1.59 0.59 ¬± 1.12 1.52 ¬± 1.69 2.32 ¬± 1.68 1.62 ¬± 1.75 2.52 ¬± 1.71 Table 3: Per-category Scores on MME-COF. Mean scores and standard deviations are reported on a 0‚Äì4 scale, as graded by Gemini-2.5-Pro. Category Kling-v1 [38] Seedance-1.0 Pro [19] Veo-3.0 Fast [21] Veo-3.0 Preview [21] Sora-2 [56] Sora-2 Pro [56] Visual Detail 0.72 ¬± 0.69 1.37 ¬± 1.39 1.10 ¬± 1.24 1.59 ¬± 1.68 1.14 ¬± 1.32 1.08 ¬± 1.89 Visual Trace 0.49 ¬± 0.65 1.23 ¬± 1.13 1.43 ¬± 1.26 1.48 ¬± 1.24 1.51 ¬± 1.37 1.75 ¬± 1.31 Real-world Spatial 0.77 ¬± 0.76 1.79 ¬± 1.53 2.07 ¬± 1.54 2.10 ¬± 1.46 1.84 ¬± 1.43 1.77 ¬± 1.35 3D Geometry 0.61 ¬± 0.58 1.95 ¬± 1.64 1.71 ¬± 1.54 1.54 ¬± 1.43 1.37 ¬± 1.49 1.42 ¬± 1.45 2D Geometry 0.49 ¬± 0.67 0.96 ¬± 1.11 1.18 ¬± 1.15 1.27 ¬± 1.20 1.77 ¬± 1.45 1.77 ¬± 1.21 Physics-based 0.60 ¬± 0.62 1.27 ¬± 1.25 1.44 ¬± 1.39 1.44 ¬± 1.35 2.13 ¬± 1.32 2.10 ¬± 1.33 Rotation 0.22 ¬± 0.34 2.30 ¬± 1.46 1.83 ¬± 1.44 1.60 ¬± 1.29 1.62 ¬± 1.37 1.44 ¬± 1.28 Table & Chart 0.87 ¬± 0.72 0.71 ¬± 1.18 0.82 ¬± 1.30 0.96 ¬± 1.44 1.84 ¬± 1.61 1.48 ¬± 1.59 GUI 1.09 ¬± 0.51 0.70 ¬± 0.76 1.11 ¬± 1.09 1.18 ¬± 0.89 1.88 ¬± 1.64 1.52 ¬± 1.48 Object Counting 0.64 ¬± 0.58 1.15 ¬± 0.97 2.03 ¬± 1.42 1.84 ¬± 1.42 2.06 ¬± 1.48 1.86 ¬± 1.41 Embodied 0.80 ¬± 0.00 1.82 ¬± 1.67 1.33 ¬± 1.57 1.18 ¬± 1.46 1.30 ¬± 1.51 1.40 ¬± 1.42 Medical 1.15 ¬± 1.17 1.56 ¬± 1.41 0.27 ¬± 0.39 0.30 ¬± 0.58 2.08 ¬± 1.56 1.81 ¬± 1.42 3) Visual Stability (0-4): Assesses the stability of the video in terms of camera motion, object appearance, and scene composition. Shaky or glitchy outputs are penalized. 4) Content Fidelity (0-4): Determines how accurately the key elements described in the prompt are preserved. Hallucinated or missing objects/events will reduce the score. 5) Focus Relevance (0-4): Examines whether the video‚Äôs visual attention remains focused on the correct objects or regions throughout. Irrelevant distractions or poorly framed targets are penalized. We adopt a direct prompting",
  "accurately the key elements described in the prompt are preserved. Hallucinated or missing objects/events will reduce the score. 5) Focus Relevance (0-4): Examines whether the video‚Äôs visual attention remains focused on the correct objects or regions throughout. Irrelevant distractions or poorly framed targets are penalized. We adopt a direct prompting strategy, instructing Gemini with the prompt, videos, and evaluation criteria to produce numerical scores in JSON format directly. 3.4 Quantitative Results and Analysis We report the quantitative scores of the five evaluated models across the five reasoning dimensions in Table 2, and provide detailed per-category results in Table 3 and Figure 2a. Overall, most models exhibit limited reasoning capability across all tasks in MME-COF, reflected by generally low scores. Among the five dimensions, Visual Stability achieves the highest average, indicating that current video models can generate smooth and coherent sequences. Yet, their behavior remains largely at the level of pattern replay rather than genuine reasoning. The Sora-2 series [56] shows relative advantages in physics-based, embodied, and medical reason- ing, while the Veo-3.0 series [21] performs comparatively better in real-world spatial reasoning. Seedance-1.0-pro [19] demonstrates relative strength in rotation and 3D geometry reasoning. These trends suggest that different models specialize in distinct reasoning aspects. However, their mean scores remain below 2.0 out of 4, highlighting substantial room for improvement and pointing to opportunities for more targeted enhancement in future development. 29",
  "4 Related Work Video Models. Video models have been progressively evolving both in the fields of video under- standing and generation. For video understanding methods, earlier approaches, such as MViT [14], Video Swin Transformer [48], and VideoMAE [62], aim to learn a robust representation that fosters downstream tasks. With the rise of LLMs, recent approaches encode videos as tokens and exploit the language backbone for captioning [61], event localization [59], and high-level reasoning [28, 83]. Video generation models have also attracted much attention. Closed system, including OpenAI‚Äôs Sora [55, 56], Runway‚Äôs Gen-3 [58], Pika Labs [57], Luma AI [50], and Google DeepMind‚Äôs Veo series [20, 21], have exhibited impressive results. However, they remain inaccessible due to their closed-source nature. Open-source alternatives have recently become available: Stable Video Diffusion [6] introduces efficient training strategies, Hunyan-Video [37] proposes systematic scaling, and Wan-2.1 [64] presents an efficient 3D VAE with expanded pipelines. Reasoning with Video. The advent of large reasoning models [24, 60, 27, 69], such as OpenAI o1 [54] and DeepSeek-R1 [23], has spurred the development of video reasoning benchmarks. Most current methods [15, 44, 53] employ MLLMs specialized in video reasoning understanding. For example, Video-R1 [15] specifically targets temporal reasoning capabilities by introducing a temporal group relative policy optimization (GRPO) loss. VideoChat-R1 [44] focuses on spatio-temporal reasoning abilities by training with GRPO and rule-based rewards. A two-stage training strategy, combining SFT and RL, is used by VideoRFT [65]. When trained on vast collections of images and videos, this strategy boosts the model‚Äôs ability to handle QA tasks, whether in general con- texts or reasoning-focused ones. These methods primarily focus on enhancing specific types of question-answering or captioning tasks. Concurrently, [70] demonstrates the large potential of video generative models in video reasoning. These models have implicitly acquired world knowledge throughdemonstrates impressive performance on various tasks, includinging and reasoning capability. Yet, this direction has rarely been explored and only experimented with in zero-shot settings. Evaluation of Video Models as Zero Shot Learner. Recently, several works have been exploring the zero-shot capability of video generation models in various domains, including general-purpose vision understanding [70, 17], medical imaging [39], and world models [68]. [70] conducts experi- ments on Veo 3 with a variety of vision tasks that have not been explicitly included during training. The video model showcases surprising performance on multiple tasks like object segmentation, image editing, and even maze solving. [39] later adopts a similar paradigm to medical images understanding tasks and finds video generation models also show powerful capabilities, e.g., delineation of anatomi- cal structures in CT scans, medical image segmentation, and even forecasting of future 3D CT phases. Besides, [68] shows that video generation models could also understand complex temporal causality and world knowledge in the real world, thereby serving as a world model [2, 33]. 5 Conclusions and Insights Video models demonstrate an intuitive understanding of the simple visual world. Recent video models can generate high-fidelity videos with realistic motion dynamics, suggesting that they have internalized substantial visual and structural knowledge about the world. Through qualitative results from our empirical study and",
  "model [2, 33]. 5 Conclusions and Insights Video models demonstrate an intuitive understanding of the simple visual world. Recent video models can generate high-fidelity videos with realistic motion dynamics, suggesting that they have internalized substantial visual and structural knowledge about the world. Through qualitative results from our empirical study and quantitative results from the MME-COF benchmark, our work confirms that these models do exhibit intuitive yet local reasoning potential. This emergent behavior, which aligns with the ‚ÄúChain-of-Frame‚Äù (CoF) mechanism, is revealed across several common success patterns. (i) Fine-grained Grounding. Models demonstrate a capability for fine-grained attribute and spatial grounding, especially when targets are visually distinct, as presented in visual detail reasoning tasks. (ii) Short-horizon Trace Consistency. In Visual Trace Reasoning tasks, models can maintain short-term consistency in visual traces. (iii) Emergent Tool-Use Simulation. An emergent ability to follow CoF instructions that mimic tool-use is presented, such as drawing lines in 2D geometry, highlighting targets in object counting, or controlling the camera in table and chart reasoning. (iv) Foundational Spatial and Geometric Grasp. This includes single-step 3D geometry transformations, understanding basic real-world spatial layouts, finding coherent sequential paths, and handling small-angle Rotations. (v) Preliminary Real-world Interaction. Models display a preliminary comprehension of real-world interaction, generating coherent manipulation paths in embodied reasoning. 30",
  "Complex visual reasoning reveals fundamental limitations. However, visual reasoning demands more than these foundational skills. It tests a model‚Äôs ability to maintain long-horizon logical consistency, adhere to abstract constraints, and understand functional principles. In these complex areas, our study reveals fundamental limitations and several common failure patterns. (i) Causal and Physical Logic. This is evident in physics-based reasoning, where the model generates implausible motion that violates basic causal principles, and in visual trace reasoning, where the generated sequences break causal order with illogical steps. (ii) Long-horizon and Rule-grounded Reasoning. In visual trace reasoning, models fail to maintain state and adhere to task-specific rules over extended sequences. (iii) Geometric and Spatial Logic. Models fail at multi-step or complex transformations in 3D/2D geometry and real-world spatial tasks, often breaking constraints or prioritizing visual plausibility over correctness. (iv) Functional and Interaction Logic. They merely imitate GUI actions without grasping their purpose and lack the necessary planning and stability for reliable Embodied tasks, often resorting to workarounds. (v) Perceptual Precision and Specialized Knowledge. This weakness appears when models fail to identify small or indistinct targets in visual detail reasoning, distort data in table and chart tasks, and fail to process specialized medical imagery due to a lack of domain understanding. Current video models are not yet ready as standalone zero-shot reasoners. Overall, our findings show that current video models are not yet reliable as standalone zero-shot reasoners. Strong generative performance does not automatically imply robust reasoning during inference. The model‚Äôs behavior appears to be driven more by learning surface-level patterns and correlations rather than by internalizing general principles. It excels at short-term coherence rather than long-horizon causality. This is evident when the model prioritizes visual plausibility over precise spatial reasoning, or favors visually symmetric patterns over strictly adhering to geometric instructions. This tendency to produce plausible but instructionally flawed outputs reveals a reasoning process that is pattern-driven, not principle-driven, thereby undermining its ability to function as a standalone zero-shot reasoner. The potential in advancing next-generation collaborative visual reasoning. Despite these limitations, the emergent behaviors observed in video models signal strong potential. The CoF concept suggests a novel modality for reasoning through visual problems step by step. While these models are not yet robust standalone reasoners, their foundational capabilities demonstrate that they can be guided through carefully designed prompts. This suggests a path where video models exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang,",
  "for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [5] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378, 2025. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Do- minik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent 31",
  "diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22563‚Äì22575, 2023. [8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [9] Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong Wang, Mu Zhou, and Mianxin Liu. Think twice to see more: Iterative visual reasoning in medical vlms. arXiv preprint arXiv:2510.10052, 2025. [10] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought reasoning. arXiv preprint arXiv:2506.05331, 2025. [11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv‚Äì2407, 2024. [14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6824‚Äì6835, 2021. [15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [16] Zhanzhou Feng, Qingpei Guo, Xinyu Xiao, Ruihan Xu, Ming Yang, and Shiliang Zhang. Unified video generation via next-set prediction in continuous domain. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19427‚Äì19438, 2025. [17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CVPR 2025 Highlight, 2024. [18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [19] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [20] Google DeepMind. Veo 2, 12 2024. Accessed: 2024. [21] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025. [22] Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and Ruihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation and answering. arXiv preprint arXiv:2503.16867, 2025. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms",
  "Liu, Meng Cao, and Ruihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation and answering. arXiv preprint arXiv:2503.16867, 2025. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 32",
  "[24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [25] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: A primary assessment for visual reasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025. [26] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, and Shi-Min Hu. Rbench-v: A primary assessment for visual reasoning models with multi-modal outputs. 2025. [27] Ziyu Guo*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? let‚Äôs verify and reinforce image generation step by step. CVPR 2025, 2025. [28] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [29] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. [30] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. Mistral 7b, 2023. [31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Mart√≠n-Mart√≠n. Mini-behavior: A procedurally generated benchmark for long-horizon decision- making in embodied ai. arXiv preprint arXiv:2310.01824, 2023. [33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Huang Gao, and Jiashi Feng. How far is video generation from world model? ‚Äì a physical law perspective. arXiv preprint arXiv:2406.16860, 2024. [34] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. [35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199‚Äì22213, 2022. [36] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos√© Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/, June 2024. [39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video",
  "Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/, June 2024. [39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video models emerging as zero-shot learners and reasoners in medical imaging? arXiv preprint arXiv:2510.10254, 2025. 33",
  "[40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [41] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vuli¬¥c, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [42] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. [43] Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay Krishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations. arXiv preprint arXiv:2506.04633, 2025. [44] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforce- ment fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [45] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14963‚Äì14973, 2023. [46] Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual abstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025. [47] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:62352‚Äì62387, 2023. [48] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3202‚Äì3211, 2022. [49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507‚Äì2521, 2022. [50] LumaLabs. Dream machine, 06 2024. Accessed: 2024. [51] Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, and Shanghang Zhang. Robobench: A comprehensive evaluation benchmark for multimodal large language models as embodied brain, 2025. [52] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263‚Äì2279, Dublin, Ireland, May 2022. Association for Computational Linguistics. [53] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. [54] OpenAI. Openai o1 system card. https://openai.com/index/ openai-o1-system-card/, December 2024. Accessed: 2024-12-05. [55] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024. [56] OpenAI.",
  "Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. [54] OpenAI. Openai o1 system card. https://openai.com/index/ openai-o1-system-card/, December 2024. Accessed: 2024-12-05. [55] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024. [56] OpenAI. Sora 2 system card. Technical report, OpenAI, September 2025. 34",
  "[57] PikaLabs. Pika 1.5, 10 2024. Accessed: 2024. [58] Runway. Introducing gen-3 alpha: A new frontier for video generation. https://runwayml. com/research/introducing-gen-3-alpha/, June 2024. [59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages 247‚Äì263, 2018. [60] Chengzhuo Tong*, Ziyu Guo*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: A study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. [61] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: A versatile metric for evaluating image and video captions using gpt-4o. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 7419‚Äì7427, 2025. [62] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [64] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [65] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025. [66] Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: An mllm benchmark for spatial visualization. arXiv preprint arXiv:2507.07610, 2025. [67] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025. [68] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and Lei Zhang. Videoverse: How far is your t2v generator from a world model? arXiv preprint arXiv:2510.08398, 2025. [69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824‚Äì24837, 2022. [70] Thadd√§us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [71] Penghao Wu and Saining Xie. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084‚Äì13094, 2024. [72] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. [73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo",
  "Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. [73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic: A benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. 35",
  "[74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: A benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [76] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos√© Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10459‚Äì10469, 2023. [77] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556‚Äì9567, 2024. [78] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. [79] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024. [80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pages arXiv‚Äì2407, 2024. [81] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024. [82] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. [83] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Confer- ence, pages 8475‚Äì8489, 2025. [84] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [85] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive up- ward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 26183‚Äì26191, 2025. 36",
  "Gistify! Codebase-Level Understanding via Runtime Execution Hyunji Lee‚àó1, Minseon Kim2, Chinmay Singh2, Matheus Pereira2, Atharv Sonwane3, Isadora White4, Elias Stengel-Eskin5, Mohit Bansal1, Zhengyan Shi2, Alessandro Sordoni2, Marc-Alexandre C√¥t√©2, Xingdi Yuan2, Lucas Caccia‚àó2 ‚àóEqual contribution 1University of North Carolina at Chapel Hill 2Microsoft Research 3Cornell University 4University of California San Diego 5University of Texas at Austin hyunjil@cs.unc.edu debug-gym@microsoft.com https://microsoft.github.io/debug-gym As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces. 1 Introduction Large language models (LLMs) are increasingly being used in code-related tasks, powering applications in debugging (Yuan et al., 2025) and agentic code generation (Yang et al., 2024; Liang et al., 2025). Thus, the ability to handle isolated snippets and reasoning across entire codebases, including complex file and module relationships, is becoming increasingly essential. Yet, the evaluation toolkit for assessing such capabilities has lagged behind. Recent evidence shows that widely-adopted repository-level benchmarks such as SWE-bench (Jimenez et al., 2024) and RepoBench (Liu et al., 2023b) still do not require full reasoning over the whole execution and could be solved through heuristic shortcuts or retrieval of localized patches (Aleithan et al., 2024; Liang et al., 2025). Moreover, because many of these datasets rely on GitHub issues or pull requests for construction, they are not easily generalizable to arbitrary repositories. At the same time, coding agents are increasingly deployed in large, real-world codebases, highlighting the need for automatically constructed, broadly applicable, and more challenging repository-level evaluation. To fill this gap, we introduce the Gistify task, which is deliberately inspired by a common practice of how developers navigate and understand unfamiliar repositories. Rather than reading files in isolation, they start from a concrete execution point such as test command or entry script often mentioned in READMEs. Then, they iteratively reason over the runtime behavior such as identifying dependencies, following control paths to uncover the codebase‚Äôs structure and functionality. Gistify formalizes this practice by requiring an (agentic) coding model to extract the gist of a given command, i.e. to generate a single, self-contained, minimal, and executable gistified file that faithfully reproduces the runtime behavior of a given command as when using the original full codebase (Figure 1). In addition to serving as a challenging coding task, such gistified repositories might give human coders a better understanding of a specific functionality of a given codebase, or even a way to",
  "faithfully reproduces the runtime behavior of a given command as when using the original full codebase (Figure 1). In addition to serving as a challenging coding task, such gistified repositories might give human coders a better understanding of a specific functionality of a given codebase, or even a way to export the single functionality of interest without inheriting heavy dependencies. To perform well in Gistify, an agent should generate a single gistified file that satisfies four key requirements: it should be self-contained, including all necessary components from the codebase so that it can be executed independently; it should ensure execution fidelity, producing the same outputs as the original codebase under the given command; it should satisfy minimality, retaining only the essential code required for execution without redundant or extraneous lines; and it should guarantee faithful preservation, avoiding hallucinated or fabricated code and relying solely on content from the original codebase. To assess model performance, we introduce evaluation metrics that align with these requirements, providing a systematic arXiv:2510.26790v1 [cs.CL] 30 Oct 2025",
  "Gistify! Codebase-Level Understanding via Runtime Execution from requests.compact import Morsel from adapters import HTTPAdapter class TestMorsel: morsel = Morsel() ... def test_cookie(): s = TestMorsel() s.mount(HTTPAdapter(0, 0)) test_requests.py from http.cookies import Morsel ... compact.py def _basic_auth(username): ... auth.py from auth import _basic_auth class BaseAdapter: def auth(self): _basic_auth(self.name) class HTTPAdapter(BaseAdapter): def __init__(self): ... self.auth() adapters.py Codebase pytest test_requests.py::test_cookie Command from http.cookies import Morsel def _basic_auth(username): ... class BaseAdapter: ... class HTTPAdapter(BaseAdapter): ... class TestMorsel: morsel = Morsel() ... def test_cookie(): s = TestMorsel() s.mount(\"http://\", HTTPAdapter(0, 0)) gistified_file.py Gistify Figure 1: The Gistify task: given a codebase and a command of entrypoint, the goal is to generate a minimal, self-contained gistified code file that faithfully reproduces the original runtime behavior using code from the given codebase. way to measure codebase-level understanding. Gistify requires agents to follow the execution path through the codebase without bypassing modules, i.e., understanding how relevant objects are modified along the way, and identifying which classes or functions can be simplified or removed. Since even moderately sized codebases exceed the context window of current LLMs, success also requires effective search capabilities. The advantages that Gistify brings are multiple: first, it provides direct insight into the ability of models to reason at the codebase level with an understanding of runtime execution, rather than on isolated code snippets. Second, it is lightweight and broadly applicable: it requires only the repository and an entrypoint, without issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary repositories, including private ones. Finally, gistified files themselves are valuable outputs: by compressing a specific feature of a large codebase into a minimal file, they can be applied to various downstream tasks, including automated debugging or error localization. We conduct experiments across a variety of frameworks (mini-SWE-agent, SWE-agent, and Copilot) and models (GPT-5-mini, GPT-5, Claude-3.7-Sonnet, and Claude-Sonnet-4) and uncover several interesting findings. First, even widely used, high-performing frameworks and models struggle to create a successful gistified file, especially when execution traces are long and have high coverage on the repositories. Second, faithfully reproducing the test function in the generated file is a strong indicator of gistified performance, as it serves as the starting step for reasoning about execution traces. Third, enabling execution tools yields small but consistent performance gains, and additionally providing global code context and runtime information further boosts performance. Finally, agentic models benefit from dynamically deciding what to read and refine their reasoning through multi-step trajectories, outperforming static approaches. 2 Gistify 2.1 Task Definition As shown in Figure 1, when given a codebase and a command as input, the coding agent must generate a single gistified file that reproduces the runtime behavior of the original codebase under the given command. Specifically, the gistified file must satisfy the following requirements. Self-Contained: All necessary components from the given codebase must be included so that the gistified file can be executed standalone, i.e. without relying on the codebase. The model must identify all relevant modules and dependencies, demonstrating understanding of inter-file relationships. Execution Fidelity: Executing the gistified file must replicate the original codebase‚Äôs runtime behavior,",
  "components from the given codebase must be included so that the gistified file can be executed standalone, i.e. without relying on the codebase. The model must identify all relevant modules and dependencies, demonstrating understanding of inter-file relationships. Execution Fidelity: Executing the gistified file must replicate the original codebase‚Äôs runtime behavior, ensuring the model captures the dynamic execution, not just static code patterns. 2",
  "Gistify! Codebase-Level Understanding via Runtime Execution Minimalism: Only the code essential to reproducing the runtime behavior should be preserved, with unused functions and objects pruned. This requires fine-grained understanding of the code to identify which lines are actually executed and essential for the task. Grounded Preservation: No hallucinated code may be introduced. All content must be derived directly from the original codebase. This ensures the task evaluates the model‚Äôs understanding of the codebase, rather than its ability to generate arbitrary code that happens to satisfy the command. 2.2 Evaluation Protocol There are two inputs to a Gistify task: i) a docker image containing the target codebase, for consistent evaluation; ii) an entrypoint, such as a pytest command on one of the tests in the codebase. Test cases are existing entrypoints one can easily leverage, but broadly, any command that the user would want to use to run a functionality of the existing codebase is allowed. All models are prompted to generate a gistified file for the entrypoint. We can programmatically verify whether the expected behavior is preserved when the ground-truth test is run within this setup. Here, we focus on comparing outputs of test commands. Once the model generates the gistified file, to ensure that execution for evaluation is based on the original test, we integrate the test code from the original codebase to the gistified file and execute it. This ensures that the model does not cheat by modifying the test. 2.3 Metrics Once a gistified file is generated, we evaluate it using the given execution command. The evaluation considers three dimensions, aligned with the task requirements, to provide a comprehensive measure of a model‚Äôs ability to reason over an entire codebase and understand its execution behavior. See Appendix A.1 for more details. Execution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same output as the original codebase when executed under the given command; otherwise, it is 0. Failures include cases where the file is not runnable or yields different outputs. The comparison checks for tests pass/fail consistency and stdout/stderr matching. Formally, let c denote the given command, C a given codebase, and G a gistified file. Define runs(c, C) as an indicator of whether c executes without crashing when running over C, and out(c, C) returns the set of outputs and error traces from running c with C. Then, execution fidelity is defined as 1 \u0002 runs(c, G) ‚àß(out(c, G) = out(c, C)) \u0003 , (1) where 1[¬∑] is the indicator function. Line Execution Rate measures minimality by calculating the fraction of lines in the gistified file that are actually executed under the given command. A 100% execution rate means all lines are essential, indicating a focused and concise file. This metric is only computed for files that run successfully, since the execution trace is required to determine which lines are run. Formally, let Lexec(G) be a list of executable lines (i.e., no comments) in G. Then, the Line Execution rate is defined as 1 |Lexec(G)| X ‚Ñì‚ààLexec(G) 1[‚Ñìis executed]. (2) Line",
  "is only computed for files that run successfully, since the execution trace is required to determine which lines are run. Formally, let Lexec(G) be a list of executable lines (i.e., no comments) in G. Then, the Line Execution rate is defined as 1 |Lexec(G)| X ‚Ñì‚ààLexec(G) 1[‚Ñìis executed]. (2) Line Existence Rate measures the proportion of code in the gistified file that is directly preserved from the original codebase. Specifically, lines of code are grouped into blocks (classes, functions, or top-level units), and matches are computed block by block while respecting the code hierarchy. This helps avoiding false matches from common lines appearing in unrelated parts of the codebase. To ensure robustness, we normalize across common variations such as indentation, multi-line statements, and imports. A 100% existence rate indicates full fidelity to the original codebase without hallucination. 3",
  "Gistify! Codebase-Level Understanding via Runtime Execution Formally, let BG and BC be the sets of blocks in the gistified file and the original codebase, respectively. For a block b, let L(b) represent its set of lines. Then, the existence rate is defined as 1 P b‚ààBG |L(b)| X b‚ààBG X ‚Ñì‚ààL(b) 1{‚Ñì‚ààLC(b)} , (3) where 1{‚Ñì‚ààLC(b)} = 0, if no matching block exists in BC. 3 Experiments 3.1 Setting We conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent (Yang et al., 2024) and GitHub Copilot (Microsoft, 2025) provide a rich scaffolding to LLM-based agents, enabling them to autonomously perform software engineering tasks. This includes a set of tools for creating and editing code files, navigating repositories, and executing tests. These frameworks also offer the LLM controllable cache management, and LLMs follow the standard tool-calling format. We also experiment with Mini-SWE- Agent (Yang et al., 2024), a lightweight framework where LLMs only have access to a bash terminal to solve the task. Commands are parsed from the agent output and executed directly. As the task objective is for the model to use reasoning over the execution flow rather than the ability of tool usage, for the agentic models, we exclude the execution tools (‚Äúpython‚Äù, ‚Äúpytest‚Äù) in the default setting where execution is disabled. Our evaluation spans four leading LLM variants: GPT-5 (OpenAI, 2025a), GPT-5-mini (OpenAI, 2025b), Claude-3.7-Sonnet (Anthropic, 2025a), and Claude-Sonnet-4 (Anthropic, 2025b), offering different cost / performance tradeoffs. For ease or reading, we will refer to the last two models as Claude-3.7 and Claude-4. We use a 128K token limit for all models. All experiments run are capped at 50 steps, after which whatever is generated at this moment in the gistifed file is submitted for evaluation. On the data side, we experiment with widely used GitHub repositories which are present in SWE- Bench (requests, pylint, flask, scikit-learn, seaborn). We also explore an additional repository, debug-gym (Yuan et al., 2025)1. This library is relatively new and importantly does not overlap with SWE-Bench. We extract and filter test sets for each repository. Namely, we remove tests whose execution is dependent on the test‚Äôs file location. For the main experiment, we evaluate over 25 tests for each of the 5 repositories. More details regarding the evaluation setup and prompt can be found in the Appendix A. 3.2 Results We begin by giving an overview of the main results presented in Table 1. We report results for our main evaluation protocol, where the model does not have access to execution tools (e.g. ‚Äúpython‚Äù and ‚Äúpytest‚Äù commands), as well as the alternative. Examples of gistified files are in Appendix B.1. Strong models and frameworks still struggle with Gistify task. Across models and execution frameworks, performance remains limited: even the strongest model with strong framework (Copilot with Claude-4) achieves 58.7% average Execution Fidelity, a binary success/fail metric, indicating that reliably producing a correct gistified file is still challenging. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets (Section 4.2), suggesting that the benchmark can scale in difficulty and",
  "with Claude-4) achieves 58.7% average Execution Fidelity, a binary success/fail metric, indicating that reliably producing a correct gistified file is still challenging. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets (Section 4.2), suggesting that the benchmark can scale in difficulty and will remain a meaningful target as future models strengthen and require more challenging evaluations. Different model families exhibit distinct strengths. Claude-4 achieves the highest Line Existence scores, indicating that it most faithfully extracts relevant code from the original codebase. In contrast, GPT-5 produces the most concise outputs, with a substantially higher Line Execution rate than other models. We 1We provide a link to all the GitHub repositories used in this work in Table 4. 4",
  "Gistify! Codebase-Level Understanding via Runtime Execution Table 1: Average Performance over three agentic frameworks with four models. We evaluated over 25 tests over 5 repositories. Execution Fidelity is shown as w/o exec, and w execution tools. Line Existence and Execution are average across the two settings for clarity. Framework Model Execution Fidelity Line Existence Line Execution (wo exec / w. exec) mini-SWE-agent GPT-5-mini 17.1 / 24.0 44.9 61.2 GPT-5 51.0 / 54.0 56.8 83.1 Claude-3.7 38.7 / 43.3 66.0 69.2 Claude-4 54.0 / 55.3 67.0 75.7 SWE-agent GPT-5-mini 30.9 / 45.3 47.9 74.8 GPT-5 30.7 / 46.0 48.3 81.7 Claude-3.7 40.7 / 46.0 66.8 69.9 Claude-4 56.7 / 57.3 66.3 72.9 Copilot GPT-5-mini 58.0 / 55.3 62.4 77.8 GPT-5 58.7 / 60.7 66.9 81.4 Claude-3.7 43.3 / 56.0 63.0 74.4 Claude-4 58.7 / 61.3 69.6 80.3 observe a similar trend for GPT-5-mini and Claude-3.7: in general, GPT models achieve higher Line Existence, whereas Claude models achieve higher Line Execution. Small(er) models perform well with scaffolding. We note that GPT-5-mini‚Äôs performance varies significantly across different evaluation settings, from 17% in a bash-only setup to 58% when provided with a large inventory of tools from the Copilot framework (see Appendix B.3 for a full list). We note that this performance increase is also reflected in the quality of the generated gist, where we see a notable increase in line existence and line execution. Frontier models (GPT-5 / Claude-4) are strong bash users. When looking at performance on mini-swe-agent, where the models only have access to a bash terminal to solve the task, both models perform relatively well, solving over half of the tasks. Importantly, this is not the case for smaller and previous-generation models. Execution tools are not a silver bullet. Overall, when comparing performance with and without execution in Table 1, we note that in most cases we observe only a small performance gain. We expected that current coding LLMs could better leverage execution tools: indeed, using tools specifically for runtime execution analysis, such as a debugger, could significantly help solving a gistify task. However, we are not seeing this behavior emerge, even from frontier models. We observed a sharp decrease in performance for the GPT-5 model when evaluated on SWE-Agent without execution tools. We performed a visual inspection and noticed formatting issues when rewriting the input test function. A detailled discussion can be found in Appendix B.2. 3.3 Error Analysis Over Execution Failure We proceed with an analysis of the underlying failure causes, in order to understand which aspect of the Gistify task different models struggle with. Table 2 shows that each model tends to fail for different reasons. See Appendix B.4 for detailed examples of each error case. Import Error occurs when the model incorrectly imports the original codebase (e.g., import requests) instead of inlining the required modules into the gistified file. We note that this error occurs even as coding LLMs are explicitly prompted not to import the specific packages in question. Perhaps surprisingly, the best performing model, Claude-4, commits this seemingly innocuous error the most out of all four",
  "requests) instead of inlining the required modules into the gistified file. We note that this error occurs even as coding LLMs are explicitly prompted not to import the specific packages in question. Perhaps surprisingly, the best performing model, Claude-4, commits this seemingly innocuous error the most out of all four models. File Creation Failure errors arise when the model fails to generate the gistified file. This can happen in two ways: the model exceeds the maximum step limit, or the model terminates the task without any file being generated. 5",
  "Gistify! Codebase-Level Understanding via Runtime Execution Table 2: Average error rates (%) of different failure reasons when running SWE-agent across models. Error cases are categorized into four groups. The numbers in parentheses indicate the number of errors for each category. Models Import Error File Creation Failure Missing Test Function Pytest Runtime Error GPT-5-mini 2.1 (2) 11.3 (11) 76.3 (72) 10.3 (10) GPT-5 5.2 (4) 10.4 (8) 77.9 (60) 6.5 (5) Claude-Sonnet-3.7 20.0 (10) 20.0 (10) 2.0 (1) 58.0 (29) Claude-Sonnet-4 32.5 (13) 10.0 (4) 7.5 (3) 50.0 (20) Missing Test Function errors occur when the generated gistified file does not contain the function implemen- tation for the test specified in the given command, or implements the test in a different structure. This can happen when the model strips out the content of the test and executes it outside of the pytest wrapper, under e.g. if __name__ == __main__:. Claude models tend to avoid this mistake, while this is the main source of error for GPT-5 models, specifically under the SWE-agent framework. Importantly, we observe that this error does not happen at random, but rather alongside other execution errors; we attempted to add the missing test function, and it in most cases the test fails to run, i.e. it results in a runtime error. This aligns with the analysis in the next section, showing a strong correlation between the task‚Äôs success and the fidelity between the original and the generated tests. Pytest Runtime Error occurs when the execution of the generated file fails, either due to a runtime error or because the gistified output does not match the output from the original codebase. The results indicate this is the most common cause of error for the best performing model, Claude-4. 3.4 Importance of Faithfully Preserving the Test Function We observe that models frequently modify the test function, despite being provided with explicit instructions to copy without modification, except for unavoidable adjustments (e.g., removing imports). Again, to ensure consistent evaluation, we replace the test function in the gistified file with the original version before evaluation. To measure such modifications, we define the Test F1 Score as the line-level overlap between the test code of the original file and the gistified version. High Test F1 Score indicates that the model has successfully identified and copied the correct test function to the gistified file. We observe a strong correlation between Test F1 Score and execution fidelity (correlation=0.76, p=0.01); test instances with higher F1 scores are substantially more likely to produce a successful gistified file. We hypothesize that this arises because in the Gistify task, models often reason backwards from the test file, thereby if the model fails from identifying or copying the test function, the subsequent reasoning process is highly likely to fail. To better understand the impact of the first step‚Äîsearching, viewing, and copying the test function‚Äîwe conduct an ablation study where we remove potential failure at this stage. Specifically, we explicitly provide the correct test function body and signature in the prompt, so the model no longer needs to locate or copy it. This",
  "the impact of the first step‚Äîsearching, viewing, and copying the test function‚Äîwe conduct an ablation study where we remove potential failure at this stage. Specifically, we explicitly provide the correct test function body and signature in the prompt, so the model no longer needs to locate or copy it. This isolates the effect of errors in identifying the test function. In this setting, we observe that Test F1 Score improves highly from the base Gistify 68.4 to 85.3, along with execution fidelity (from 42.0% to 60.0%). This suggests that accurately handling the test function is a critical first step to do the Gistify task successfully. Detailed results are in Appendix B.5. 4 Analysis In this section, we analyze how different strategies and tools affect performance on the Gistify task, identify factors that contribute to its difficulty, and experiment with the use of a static coding LLM to gain a deeper understanding of the task. For all experiments, we evaluate 50 test instances drawn from the pylint codebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired with Claude-Sonnet-4. 6",
  "Gistify! Codebase-Level Understanding via Runtime Execution Table 3: Analysis of the effect of different strategies and tools (global information, execution) on the Gistify task. We evaluate SWE-Agent with Claude 4 using 50 test instances from the pylint codebase. Max Steps Reached (%) indicates the percentage of runs that terminated because the maximum step limit was reached. Ablation Type Execution Fidelity Line Existence Line Execution Max Steps Reached (%) Base Gistify 42.0 65.0 58.3 14.6 Prompted Strategies Tracing 48.0 75.4 62.8 0.0 Reading 50.0 77.6 62.6 3.9 Global Info (Tool) RepoGraph 52.0 76.1 60.1 6.0 Tracing 56.0 75.1 65.1 0.0 Execution (Tool) Bash 52.0 73.1 64.2 16.0 Edit And Execute 56.0 74.3 64.2 10.0 4.1 Effect of Various Strategies and Tools In this section, we analyze how different strategies and sources of information affect model performance. We begin with the simplest approach, modifying the prompt to guide the model (Prompt-Based Guidance), and then move to more explicit approaches that rely on additional tools: providing global context (Global Information via Tools) or feedback from code execution (Execution-Based Tools). Detailed descriptions of prompts and tools, along with examples, are provided in the Appendix C.1. Prompt-Based Guidance We first begin with the simplest approach: modifying the prompt to provide explicit task guidance. We experiment in two settings. In the former, we prompt the model to perform step-by-step reasoning, by first predicting the execution traces and then going over them, adding relevant code snippets along the way (tracing). In the latter, a similar approach is used, with explicit instructions on how to recursively determine the execution traces: starting from the test, identify the relevant components and read the files where they are defined, and repeat until the end (reading). As shown in Table 3, we observe that adding such strategies tends to enhance overall metrics, giving both better execution fidelity and more faithful code extractions, as measured by line existence. Global Information via Tools Building on the above observation, we next assess the effect of explicitly providing global context through external tools, rather than predicting it. We examine two tools: (1) RepoGraph (Ouyang et al., 2024), which constructs a graph of the codebase where each node represents a line of code and edges capture connections between lines, enabling graph-based search over the entire codebase; and (2) a Tracing tool that exposes gold execution traces obtained from running the given test command. Results in Table 3 show that both tools improve performance, with the Tracing tool yielding the largest gains. This finding suggests that access to the global context, especially the gold tracing information, substantially strengthens the model‚Äôs ability to perform runtime reasoning, as it can easily identify which file to look at. Execution-Based Tools In Section 3.2, we saw that enabling execution tools resulted in small but consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. For this experiment we compare Bash access with a simple method that executes",
  "consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. For this experiment we compare Bash access with a simple method that executes and prints the output of the gistified file whenever it is edited (Edit And Execute). No other execution tools are available to the agent, including runtime information about the ground truth test. The results are surprising: having access to fewer tools actually increases performance. Indeed, we note that when give access to a full set of bash commands, the coding LLM tends to explore more tools, increasing the overall trajectory length, and potentially reaching the maximum step limit. 4.2 Tests with High Coverage are Harder to Gistify In this section, we investigate what properties makes a given test hard to Gistify. We hypothesize that tests generating a longer and more complex execution trace would entail a harder task for the coding LLM. To this end, we investigate how two axes to measure a runtime execution‚Äôs difficulty affect performance: the length of trace, as measured by the number of function calls executed, and the number of unique files touched by 7",
  "Gistify! Codebase-Level Understanding via Runtime Execution [0.0, 0.2] [0.2, 0.4] [0.4, 0.6] [0.6, 0.8] [0.8, 1.0] Binned Test Quantiles according to difficulty metric 20 40 60 80 100 Execution Fidelity (%) Performance according to Exec. Trace Difficulty Trace Length Number of Files Covered (a) Difficulty of the Gistify task is measured as a function of the execution trace difficulty of the underlying test. Execution Fidelity Line Existence Line Execution 0 20 40 60 80 Scores static coding LLM mini-SWE-Agent SWE-Agent Copilot (b) Performance of a static coding LLM and various agentic coding LLMs (mini-SWE-Agent, SWE-Agnet, Copilot). the tracing procedure. While these metrics correlate with one another, they will differ when, for example, a function is looped over many times or when the location of the relevant functions is in a single file versus across multiple files. For this experiment, we use again the same configuration as prior analysis, namely Claude-4 with 50 tests sampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given Gistify task, and how complex the execution traces are, according to both metrics considered. We leverage this insight to create a Gistify-hard subset, where we select the 30 most difficult examples according to each. We end up with 57 unique datapoints (30 from pylint, 28 from sklearn, 6 from seaborn). On this subset, performance drops to 21%, as compared to 43%, the baseline weighted performance average following the same distribution over repositories. Overall, this selection criteria offers a promising direction for designing challenging evaluation scenarios with Gistify. 4.3 Static Coding LLM In this section, we experiment over how models perform in a static setup, where they have no access to tools and cannot iterate on the generated solution. As such static coding LLMs do not have tools, they cannot search or view files dynamically. Thereby, to measure a possible upper bound for non-agentic approaches, we provide as input all files that were accessed during the original program execution (gold files). Also, as they cannot iterate over multiple steps, they have to output everything at once and are therefore restricted by the context window of the LLM. Since solving the Gistify task involves touching multiple files, we observe in many cases that the inputs exceed the model‚Äôs maximum sequence length. Thus, we sample a subset of test examples where the combined content fits within the 128K token limit of the LLM. As shown in Figure 2b, agentic models outperform static ones even when the latter receive all relevant files. This suggests that selecting files dynamically over multiple iterations is more effective than providing everything at once, which can overwhelm the model2. However, interestingly, the static coding LLM setup achieves the highest Line Existence score. This is likely because the model can copy lines directly from input, yet it performs worse on Line Execution and Execution Fidelity, suggesting that models do not have a good understanding of the codebase, often copying lines that are incomplete or incorrect. 2See Appendix C.2 for detailed statistics on the usage of various tools. 8",
  "copy lines directly from input, yet it performs worse on Line Execution and Execution Fidelity, suggesting that models do not have a good understanding of the codebase, often copying lines that are incomplete or incorrect. 2See Appendix C.2 for detailed statistics on the usage of various tools. 8",
  "Gistify! Codebase-Level Understanding via Runtime Execution 5 Related Works 5.1 Codebase-level Understanding Benchmark Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding. These generally fall into three categories: question answering, code synthesis, and mapping natural language specifications to the entire codebase. Several benchmarks introduce codebase-level question-answering (Strich et al., 2024; Li et al., 2024b; Sahu et al., 2024; Chen et al., 2025; Hu et al., 2024; Fu et al., 2025). In these settings, the model must correctly answer questions that require an understanding of the codebase. The questions are drawn from various sources, including real-world GitHub issues and queries resembling those asked of tools like Copilot. Another line of work evaluates whether models can synthesize code by leveraging information distributed across multiple files in the codebase (Zhang et al., 2023; Liu et al., 2023b; Ding et al., 2023; Li et al., 2024a; Yu et al., 2024). These benchmarks include tasks such as retrieval-augmented completion, cross-file refactoring, and more specialized settings such as sketch-based coding or codebase evolution. Moreover, there is a line of benchmark that maps natural language specifications to entire code repositories, leveraging hierarchical or multi-stage representations to capture inter-file relationships and maintain consistency across a codebase (Tang et al., 2023; Zan et al., 2024; Ni et al., 2025). Our work tackles a more complex setting, where models must reason over full execution traces and examine multiple files, making the task challenging, and even widely used agentic models struggle alongside static ones. There are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies while preserving executability (Xie et al., 2025b; Jain et al., 2024). This sandboxing step is similar to Gistify in that it tries to construct a simplified file that has the feature extracted. However, the sandboxing step is done programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast, our work focuses on generating a simplified file using an LLM, thereby evaluating the model‚Äôs ability to reason about both code dependencies and runtime behavior. Notably, while prior work acknowledges cases in which static programmatic sandboxing fails (e.g., when functions have large dependency slices) and discards those examples, we consider them informative because they require reasoning about more complex runtime behavior. We further observe that these instances also present challenging examples for the Gistify task. 5.2 Methods for Codebase-Level Understanding Recent work on autonomous agents for codebase-level code understanding has focused on improving code navigation, reasoning, and generation through structured representations and planning. Approaches leverage structural information of code for function-call graphs, module-dependency graphs, and hierarchical code structures to provide models with core components of repositories (Wang et al., 2025; Liu et al., 2024). Another line of work integrate multi-step reasoning and state update policies to enable more effective planning over complex tasks (Bairi et al., 2024; Gautam et al., 2025). Additional methods combine various agents with multiple tools to streamline codebase-level exploration and task solving (Luo et al., 2024; Zhang et al., 2023; Shrivastava et al., 2023; Wang et al., 2024; Yang et al., 2024;",
  "enable more effective planning over complex tasks (Bairi et al., 2024; Gautam et al., 2025). Additional methods combine various agents with multiple tools to streamline codebase-level exploration and task solving (Luo et al., 2024; Zhang et al., 2023; Shrivastava et al., 2023; Wang et al., 2024; Yang et al., 2024; Tang et al., 2023; aider, 2025; Microsoft, 2025; cursor, 2025). 5.3 Runtime Execution Various works have introduced benchmarks to evaluate LLMs‚Äô ability to reason over code execution at runtime (Gu et al., 2024; Chen et al., 2024; Xie et al., 2025a; Beger & Dutta, 2025; Hu et al., 2025). These benchmarks typically test whether models can predict execution traces or intermediate states such as variable values, control flow, or data dependencies‚Äîgiven code and inputs, or alternatively, infer inputs from code and outputs. Some benchmarks further extend this paradigm by leveraging execution traces to construct new problems through program composition, thereby varying complexity in a principled way. Beyond evaluation, execution traces have also been incorporated into training pipelines to strengthen models‚Äô runtime reasoning abilities (Liu et al., 2023a; Ding et al., 2024). By augmenting pre-training and fine-tuning with execution states, paths, and coverage signals, these methods help models capture program dynamics and generalize to execution-aware tasks. At inference time, several frameworks leverage runtime feedback to iteratively guide models in debugging or completing partial programs, thereby improving performance on execution-driven 9",
  "Gistify! Codebase-Level Understanding via Runtime Execution tasks (Zhong et al., 2024; Xue et al., 2024). In this work, we extend prior approaches by going beyond reasoning over execution traces to also reformulate programs; the model not only tracks execution but also identifies how to compress and organize code into a concise, coherent file. We further show that this capability serves as a useful tool at inference time, helping models better structure and complete execution-driven tasks. 6 Discussion and Conclusion In this paper, we introduced the Gistify task in which a coding LLM extracts a specific funtionality of a codebase into a single, self-contained file. Beyond serving as a standalone evaluation task that is easily applicable to arbitrary repositories and execution commands, the gistified file itself also opens several promising directions for research and practical applications. Large codebases often overwhelm automated agents due to their complex dependencies, and they especially struggle when tasked with fixing bugs that span multiple files (Ganhotra, 2025). In such scenarios, a gistified file would greatly reduce this challenge, and enable a more efficient reasoning about the codebase without navigating through unrelated code. In other words, this file could be leveraged in other downstream tasks, such as code refactoring or debugging, or even as a way to extract and share a minimal implementation of a specific codebase functionality. In summary, with coding LLMs increasingly being deployed in real-world software development, the need for automatically constructing evaluation setups that require codebase-level understanding of arbitrary repositories is growing. Through extensive experiments across a range of models and frameworks, we found that state-of-the-art LLMs still face challenges on the Gistify task, especially when faced with long, complex execution traces. Our analysis shows that incorporating global code context or execution-aware tools improves performance, and agentic coding LLM tend to handle the task more effectively by reasoning about which files to inspect using various tools. Beyond serving as a benchmark, the gistified files themselves are valuable artifacts. They distill the essential functionality of complex systems into a compact, executable form, making them easier to inspect and understand. Such files could support a range of practical applications, including debugging, refactoring, and code review, which we leave for future work. 10",
  "Gistify! Codebase-Level Understanding via Runtime Execution References aider. Ai pair programming in your terminal. 2025. URL https://github.com/Aider-AI/aider?tab= readme-ov-file. Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024. Anthropic. Claude sonnet 3.7. https://www.anthropic.com/news/claude-3-7-sonnet, 2025a. Hybrid reasoning model; accessed: 2025-09-25. Anthropic. Claude sonnet 4. https://www.anthropic.com/claude/sonnet, 2025b. Improved version over Sonnet 3.7; accessed: 2025-09-25. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675‚Äì698, 2024. Claas Beger and Saikat Dutta. Coconut: Structural code understanding does not fall out of a tree. In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. 128‚Äì136. IEEE, 2025. Jialiang Chen, Kaifa Zhao, Jie Liu, Chao Peng, Jierui Liu, Hang Zhu, Pengfei Gao, Ping Yang, and Shuiguang Deng. Coreqa: uncovering potentials of language models in code repository question answering. arXiv preprint arXiv:2501.03447, 2025. Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. Reasoning runtime behavior of a program with llm: How far are we? arXiv preprint arXiv:2403.16437, 2024. cursor. cursor. 2025. URL https://cursor.com/. Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36:46701‚Äì46723, 2023. Yangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. Traced: Execution- aware pre-training for source code. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1‚Äì12, 2024. Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, et al. Corecodebench: A configurable multi-scenario repository-level benchmark. arXiv preprint arXiv:2507.05281, 2025. Jatin Ganhotra. Do swe-agents solve multi-file issues like humans? a deep dive into swe- bench verified, January 2025. URL https://jatinganhotra.dev/blog/swe-agents/2025/01/05/ swe-bench-mutliple-files/. Blog post. Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refac- torbench: Evaluating stateful reasoning in language agents through code. arXiv preprint arXiv:2503.07832, 2025. Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: A benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 16568‚Äì16621. PMLR, 21‚Äì27 Jul 2024. Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and Cuiyun Gao. Coderepoqa: A large-scale benchmark for software engineering question answering. arXiv preprint arXiv:2412.14764, 2024. 11",
  "Gistify! Codebase-Level Understanding via Runtime Execution Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, and Kaidi Xu. Dynacode: A dynamic complexity-aware code benchmark for evaluating large language models in code generation. arXiv preprint arXiv:2503.10452, 2025. Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: Turning any github repository into a programming agent environment. In ICML, 2024. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024a. Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. Infibench: Evaluating the question-answering capabilities of code large language models. Advances in Neural Information Processing Systems, 37:128668‚Äì128698, 2024b. Shanchao Liang, Spandan Garg, and Roshanak Zilouchian Moghaddam. The swe-bench illusion: When state-of-the-art llms remember instead of reason. arXiv preprint arXiv:2506.12286, 2025. Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, and Nan Duan. Code execution with pre-trained language models. In Anna Rogers, Jordan Boyd- Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 4984‚Äì4999, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.308. URL https://aclanthology.org/2023.findings-acl.308/. Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto- completion systems. arXiv preprint arXiv:2306.03091, 2023b. Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024. Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667, 2024. Microsoft. Github copilot in vs code. 2025. URL https://code.visualstudio.com/docs/copilot/ overview. Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, et al. Gittaskbench: A benchmark for code agents solving real-world tasks through code repository leveraging. arXiv preprint arXiv:2508.18993, 2025. OpenAI. Gpt-5 technical overview. https://platform.openai.com/docs, 2025a. Accessed: 2025-09-25. OpenAI. Gpt-5 mini. https://platform.openai.com/docs/models/gpt-5-mini, 2025b. Compact variant of GPT-5; accessed: 2025-09-25. Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXiv:2410.14684, 2024. Surya Prakash Sahu, Madhurima Mandal, Shikhar Bharadwaj, Aditya Kanade, Petros Maniatis, and Shirish Shevade. Codequeries: A dataset of semantic queries over code. In Proceedings of the 17th Innovations in Software Engineering Conference, pp. 1‚Äì11, 2024. 12",
  "Aditya Kanade, Petros Maniatis, and Shirish Shevade. Codequeries: A dataset of semantic queries over code. In Proceedings of the 17th Innovations in Software Engineering Conference, pp. 1‚Äì11, 2024. 12",
  "Gistify! Codebase-Level Understanding via Runtime Execution Disha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998, 2023. Jan Strich, Florian Schneider, Irina Nikishina, and Chris Biemann. On improving repository-level code QA for large language models. In Xiyan Fu and Eve Fleisig (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 209‚Äì244, Bangkok, Thailand, August 2024. Association for Computational Linguistics. ISBN 979-8-89176-097-4. doi: 10.18653/v1/2024.acl-srw.28. Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, et al. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. arXiv preprint arXiv:2311.09835, 2023. Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, et al. Repomaster: Autonomous exploration and understanding of github repositories for complex task solving. arXiv preprint arXiv:2505.21577, 2025. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, and Xiangyu Zhang. Core: Benchmarking llms code reasoning capabilities through static analysis tasks. arXiv preprint arXiv:2507.05269, 2025a. Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. Repost: Scalable repository-level coding environment construction with sandbox testing. Conference on Language Modeling, 2025b. Zhipeng Xue, Zhipeng Gao, Shaohua Wang, Xing Hu, Xin Xia, and Shanping Li. Selfpico: Self-guided partial code execution with llms. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 1389‚Äì1401, 2024. John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1‚Äì12, 2024. Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, et al. debug-gym: A text-based environment for interactive debugging. arXiv preprint arXiv:2503.21557, 2025. Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, et al. Codes: Natural language to code repository via multi-layer sketch. arXiv preprint arXiv:2403.16443, 2024. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023. Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024. 13",
  "and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023. Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024. 13",
  "Gistify! Codebase-Level Understanding via Runtime Execution A Experimental Setting A.1 Metrics Execution Fidelity Execution fidelity measures whether the generated gistified file reproduces the same functional behavior as the original codebase under the given command. This includes producing the same number of test passes or failures, as well as consistent outputs and error handling. If the file‚Äôs behavior matches the original codebase, it is assigned 100%; otherwise, it receives 0%. Line Execution Rate The line execution rate measures the proportion of lines in the gistified file that are actually executed when running it under the given command. We first analyze the gistified file to identify which lines are executable (e.g., imports, function or class definitions) versus not-executable (e.g., comments). Using a tracing function, we then determine which of the executable lines are touched during execution. The line execution rate is computed as the fraction of executable lines that are executed. A rate of 100% indicates that the gistified file is concise and contains primarily necessary lines that are executed, while 0% indicates that non of the executable lines were touched. When calculating line execution rate, we exclude the tests where the self-containment is 0% as the goal of line execution rate is to evaluate the model‚Äôs ability to construct a concise, executable file, not to penalize failures in generating runnable code. We classify each line of code into three categories: executable, potentially executable, and non-executable. Executable lines include imports and functional code that can be directly run. Potentially executable lines are those that may or may not be executed during a run, such as the except block of a try-except statement or placeholders for classes and function definitions. Non-executable lines, such as comments, are those that have no effect on execution. To calculate the line execution rate, we first classify each line in the gistified file and then consider only the executable lines. Non-executable lines are ignored since their presence or absence does not affect execution outcomes, and potentially executable lines are excluded because they are often ambiguous (e.g., placeholders) and cannot be reliably judged as necessary or removable. Line Existence Rate The line existence rate measures the proportion of lines in the gistified file that are directly preserved from the original codebase. We first parse both the gistified file and the original codebase into blocks, where each block corresponds to a class or function. Within classes, functions are nested under their parent class, forming a hierarchy. Lines outside of any block (e.g., top-level statements) are treated as standalone units. For each block in the gistified file, we locate the corresponding block in the original codebase using its name and hierarchical position. If a matching block exists, we compare the two line by line to determine which lines are preserved; whether the lines in the gistified block appear in the corresponding original block. If no match is found, all lines in that block are treated as non-existent. For lines outside any block, existence is determined by direct comparison with top-level lines in the original codebase. An existence rate of 100% indicates perfect preservation",
  "in the gistified block appear in the corresponding original block. If no match is found, all lines in that block are treated as non-existent. For lines outside any block, existence is determined by direct comparison with top-level lines in the original codebase. An existence rate of 100% indicates perfect preservation of the original code without hallucinated content. Normalization for Line-wise Code Matching When checking the existence of a code line within a file, as our objective is to determine semantic equivalence rather than strict syntactical identity, we do normalization; code that is functionally identical may differ in formatting, such as multiline statements, indentations, or space, which can hinder direct line-wise comparison. To address this, we normalize each code block before performing line-wise matching. Specifically, we parse the code into an Abstract Syntax Tree (AST) and ignore comments; split combined import statements into individual imports; merge statements that span multiple lines into a single line; remove inline comments (e.g., for i in range(5): # comment); and eliminate indentation and redundant spaces. These normalizations ensure robustness by making the comparison focus on the code‚Äôs underlying structure and functionality rather than superficial formatting differences. A.2 Framework We evaluate experiments with three agentic frameworks: mini-SWE-Agent (Yang et al., 2024), SWE- Agent (Yang et al., 2024), and Copilot (Microsoft, 2025). Unless otherwise noted, all experiments are run in the default Gistify setup, where the model is restricted from executing any commands (e.g., python, pytest). 14",
  "Gistify! Codebase-Level Understanding via Runtime Execution SWE-Agent and Copilot Agent enable LLMs to interact with a codebase through a suite of tools, including bash commands. These tools support capabilities such as viewing, searching, editing, and creating files or directories. In addition, Copilot Agent extends this functionality with browser integration, explicit reasoning, and API usage. mini-SWE-agent is a simplified variant of SWE-Agent that only supports bash commands. Despite its minimal design, it achieves strong performance on the SWE-Bench Verified benchmark (Jimenez et al., 2023). For both mini-SWE-Agent and SWE-Agent, we set the maximum number of steps to 50 and run them in the same Docker environment, using the current version of the repositories. A.3 Experimental Test Set Construction Table 4: Details of the GitHub repositories used as the test set. Repository URL License flask https://github.com/pallets/flask BSD 3-Clause requests https://github.com/psf/requests Apache-2.0 pylint https://github.com/pylint-dev/pylint GPL 2.0 scikit-learn https://github.com/scikit-learn/scikit-learn BSD 3-Clause seaborn https://github.com/mwaskom/seaborn BSD 3-Clause debug-gym https://github.com/microsoft/debug-gym MIT Table 4 summarizes the repositories used in our evaluation. For each repository, we begin by extracting all available test cases, including parameterized ones. For experimental test runs, we group tests3 that share the same base structure but differ only in parameterization, treating them as a single test. During evaluation, however, we execute all parameterized instances and measure how many are passed, thereby assessing execution fidelity. Finally, we filter out environment-dependent tests, such as those requiring relative file paths or fixed module locations. In the main experiments, we used 25 test instances for each of the five codebases, and the analysis was conducted using 50 test instances from the pylint codebase. A.4 Prompt for Gistify Figure 3 shows the prompt used in the main experiments. A.5 Providing specific parameters to commands tends to make models generate parameter-specific gistified files We observe that when specific command-line parameters are provided, models often adapt the generated gistified file to those parameters rather than producing a fully general solution. Examples of this parameter- specific behavior are shown in Figures 4 and 5. Accordingly, in our experiments, we group test cases based on the parameters provided to the command. B Results B.1 Example of gistified file Figure 6 and Figure 7 show two gistified files on the same test case with different models; each model succeed or fail in generating a gistified file with execution fidelity of 100% and 0%, respectively. In the successful case (Figure 6), the generated file handles both parameters correctly, achieving a 100% line existence rate, a 65.5% execution rate, and a test F1 score of 100. In contrast, the failed case (Figure 7) cannot execute due to a missing import pytest statement. Moreover, the hallucinated test function yields a test F1 score of 0, and the file shows a much lower line existence rate of 28%. 3We adopt this grouping design as we observe that models often overfit to specific values when parameters are provided. See Appendix A.5 for more details. 15",
  "grouping design as we observe that models often overfit to specific values when parameters are provided. See Appendix A.5 for more details. 15",
  "Gistify! Codebase-Level Understanding via Runtime Execution Prompt for Gistify I‚Äôve uploaded a python code repository in the directory {working dir}. There is an original test invocation (the command that reproduces behavior we want to preserve): problem statement Your job: create a single file named ‚Äòconcise.py‚Äô saved at {working dir} that is **self-contained**, **minimal**, and **executable**, and when used in place of the original test run reproduces the same runtime behavior and outputs. Follow the instructions below when creating the file. OUTPUT - Produce one file only: ‚Äò{working dir}/concise.py‚Äô. - The assistant must return only the contents of ‚Äòconcise.py‚Äô (no extra files, no analysis, no commentary). HIGH-LEVEL RULES for creating ‚Äòconcise.py‚Äô 1. Inline internal dependencies * Copy into ‚Äòconcise.py‚Äô every function, class, or top-level code from the files inside {working dir} that is executed when running {problem statement}. * Do not use ‚Äòimport‚Äô statements for modules defined in {working dir}. 2. Remove unexecuted lines * When copying lines in ‚Äòconcise.py‚Äô, keep only the lines that is actually executed when running {problem statement}. * Delete unused functions, classes, variables, if-else, imports, and unreachable branches. * Ensure the file remains syntactically correct and minimal after removal. 3. Preserve original source lines * Do not rewrite or reformat lines unless necessary to keep the files valid. * Do not arbitrary generate new lines that do not exist in the original {working dir} files. * You may adjust indentation, remove empty ‚Äòelse‚Äô‚Äò blocks, or adapt ‚Äòtry-except‚Äô structures only when required to preserve correctness. 4. Keep external imports * Leave imports to external libraries, frameworks, or standard runtime libraries unchanged. * Only remove or inline dependencies that come from {working dir}. 5. No shortcuts or cheating * Do not stub, fake, or monkey-patch external modules. * Do not reimplement or newly add third-party libraries. * Do not hard-code outputs * Do not replace test logic with simplified equivalents 6. Preserve test behavior * The test function much remain unchanged, except for import adjustments needed to reference inlined code. * The output, exceptions, or exit codes must match the original run of {problem statement}. 7. Do not execute the code * Do not run or simulate the program (e.g., with ‚Äòpytest‚Äô, ‚Äòpython‚Äô, or any other tools) Figure 3: Base Prompt Template for Gistify Task. B.2 Error analysis over execution failure We categorize errors into four types: Import Error Figure 8 shows an example of Import Error. This occurs when the model incorrectly imports the original repository (e.g., import requests) instead of inlining the required modules into the gistified file. File Creation Failure This error arises when the model fails to generate the gistified file. This can happen in two ways: (1) the model exceeds the maximum step limit or (2) the model completes within the time limit but still fails to generate the new file using the tool. 16",
  "fails to generate the new file using the tool. 16",
  "Gistify! Codebase-Level Understanding via Runtime Execution @pytest.mark.parametrize( \"value ,‚ê£expected\", ( (\"application/xml\", (\"application/xml\", {})), ( \"application/json‚ê£;‚ê£charset=utf -8\", (\"application/json\", {\"charset\": \"utf -8\"}), ), (\"text/plain\", (\"text/plain\", {})), ... ) def test__parse_content_type_header (value , expected): assert _parse_content_type_header (value) == expected (a) Original Test Case def test__parse_content_type_header (): \"\"\"Test‚ê£for‚ê£the‚ê£_parse_content_type_header ‚ê£function‚ê£with‚ê£application/json‚ê£and‚ê£charset=utf -8\"\"\" value = \"application/json‚ê£;‚ê£charset=utf -8\" expected = (\"application/json\", {\"charset\": \"utf -8\"}) assert _parse_content_type_header (value) == expected (b) Gistified File Figure 4: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter. @pytest.mark.parametrize( \"url ,‚ê£expected\", ( (\"http ://192.168.0.1:5000/ \", True), ... (\"http :// google.com :5000/ v1.0/\", False), ), ) def test_should_bypass_proxies_no_proxy (url , expected , monkeypatch): \"\"\"Tests‚ê£for‚ê£function‚ê£should_bypass_proxies ‚ê£to‚ê£check‚ê£if‚ê£proxy ‚ê£‚ê£‚ê£‚ê£can‚ê£be‚ê£bypassed‚ê£or‚ê£not‚ê£using‚ê£the‚ê£‚Äôno_proxy ‚Äô‚ê£argument ‚ê£‚ê£‚ê£‚ê£\"\"\" no_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\" # Test ‚Äôno_proxy ‚Äô argument assert should_bypass_proxies (url , no_proxy=no_proxy) == expected (a) Original Test Case def test_should_bypass_proxies_no_proxy (url , expected , monkeypatch): \"\"\"Tests‚ê£for‚ê£function‚ê£should_bypass_proxies ‚ê£to‚ê£check‚ê£if‚ê£proxy ‚ê£‚ê£‚ê£‚ê£can‚ê£be‚ê£bypassed‚ê£or‚ê£not‚ê£using‚ê£the‚ê£‚Äôno_proxy ‚Äô‚ê£argument ‚ê£‚ê£‚ê£‚ê£\"\"\" no_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\" # Test ‚Äôno_proxy ‚Äô argument assert should_bypass_proxies (url , no_proxy=no_proxy) == expected (b) Gistified File Figure 5: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter. Missing Test Function This occurs when the generated gistified file does not contain the modules for specified test in the given command. It typically arises when the model fails to locate or copy the modules necessary for the test into the gistified file. Conceptually, this corresponds to a 0% line existence rate for the test function. Since the presence of the modules for the given test case is essential for validation, we classify this as an error. We also observe an interesting behavior of GPT-5 where it tends to insert __name__ == \"__main__\" even though it is not provided in the original codebase and even though it is explicitly mentioned that we will test on the provided command and expect the same output. They often remove the test function but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10). We hypothesize that this may be because they are more familiar with codebases following this pattern. We also observe cases where the model 17",
  "Gistify! Codebase-Level Understanding via Runtime Execution # Licensed under the GPL: https :// www.gnu.org/licenses/old -licenses/gpl -2.0. html # For details: https :// github.com/pylint -dev/pylint/blob/main/LICENSE # Copyright (c) https :// github.com/pylint -dev/pylint/blob/main/ CONTRIBUTORS .txt from __future__ import annotations import os from collections.abc import Sequence from typing import Any import pytest def discover_package_path (modulepath: str , source_roots: Sequence[str]) -> str: \"\"\"Discover‚ê£package‚ê£path‚ê£from‚ê£one‚ê£its‚ê£modules‚ê£and‚ê£source‚ê£roots.\"\"\" dirname = os.path.realpath(os.path.expanduser(modulepath)) if not os.path.isdir(dirname): dirname = os.path.dirname(dirname) # Look for a source root that contains the module directory for source_root in source_roots: source_root = os.path.realpath(os.path.expanduser(source_root)) if os.path.commonpath ([ source_root , dirname ]) in [dirname , source_root ]: return source_root # Fall back to legacy discovery by looking for __init__.py upwards as # it‚Äôs the only way given that source root was not found or was not provided while True: if not os.path.exists(os.path.join(dirname , \"__init__.py\")): return dirname old_dirname = dirname dirname = os.path.dirname(dirname) if old_dirname == dirname: return os.getcwd () @pytest.mark.parametrize( \" py_mod_base_name \", (\"__init__\", \"impl\"), ids=(\"explicit -namespace\", \"implicit -namespace\"), ) def test_discover_package_path_source_root_as_parent ( py_mod_base_name : str , tmp_path: Any , ) -> None: \"\"\"Test‚ê£discover_package_path ‚ê£when‚ê£source‚ê£root‚ê£is‚ê£a‚ê£parent‚ê£of‚ê£the‚ê£module.\"\"\" # Create this temporary structure: # /tmp_path/ # project/ # my -package/ # __init__.py project_dir = tmp_path / \"project\" package_dir = project_dir / \"mypackage\" package_dir.mkdir(parents=True) (package_dir / f\"{ py_mod_base_name }.py\").touch () # Test with project_dir as source root (parent of package) result = discover_package_path (str(package_dir), [str(project_dir)]) assert result == str(project_dir) Figure 6: Example of a successful gistified file: the code correctly handles all parameters, achieving 100% line‚Äìexistence rate, a test F1 score of 100, and an execution rate of 65.5%. attempts to ‚Äúcheat‚Äù the task by injecting a mock, in-memory version of the original codebase package to satisfy import dependencies, rather than copying the necessary code inline (e.g., Figure 12). Pytest Runtime Error This error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues (e.g., Figure 9). Although the absence of test functions is also one of pytest failures, we explicitly separate those cases by first verifying the presence of the required test functions and running pytest only when they exist. 18",
  "Gistify! Codebase-Level Understanding via Runtime Execution import os import sys from contextlib import contextmanager from pathlib import Path def discover_package_path (path , _search_paths ): \"\"\" ‚ê£‚ê£‚ê£‚ê£Minimal‚ê£version:‚ê£accept‚ê£a‚ê£filesystem‚ê£path‚ê£(string‚ê£or‚ê£Path)‚ê£and‚ê£return‚ê£a‚ê£Path ‚ê£‚ê£‚ê£‚ê£to‚ê£that‚ê£directory.‚ê£This‚ê£matches‚ê£the‚ê£usage‚ê£in‚ê£the‚ê£specific‚ê£test‚ê£where‚ê£a ‚ê£‚ê£‚ê£‚ê£path‚ê£to‚ê£the‚ê£tests‚ê£data‚ê£directory‚ê£is‚ê£passed. ‚ê£‚ê£‚ê£‚ê£\"\"\" if isinstance(path , Path): p = path else: p = Path(path) if p.exists () and p.is_dir (): return p raise ValueError(\"Cannot‚ê£determine‚ê£package‚ê£path\") @contextmanager def augmented_sys_path (paths): \"\"\" ‚ê£‚ê£‚ê£‚ê£Minimal‚ê£context‚ê£manager:‚ê£set‚ê£sys.path‚ê£to‚ê£the‚ê£parent‚ê£directories‚ê£of‚ê£the ‚ê£‚ê£‚ê£‚ê£provided‚ê£paths‚ê£(in‚ê£order),‚ê£restoring‚ê£the‚ê£original‚ê£sys.path‚ê£on‚ê£exit. ‚ê£‚ê£‚ê£‚ê£\"\"\" original = list(sys.path) try: new = [] for p in paths: pth = Path(p) # Ensure we use the parent directory (the \"source root\" in the test) parent = pth.parent new.append(str(parent)) sys.path [:] = new yield finally: sys.path [:] = original @pytest.mark.parametrize( \" py_mod_base_name \", (\"__init__\"), ids=(\"explicit -namespace\"), ) def test_discover_package_path_source_root_as_parent (): # This test ‚Äôs behavior is preserved: it uses discover_package_path on the # tests data directory and ensures augmented_sys_path sets sys.path to its # parent (the project/tests directory). TEST_DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"tests\", \"data\")) PROJECT_ROOT_DIR = os.path.abspath(os.path.join(TEST_DATA_DIR , \"..\")) with augmented_sys_path ([ discover_package_path (TEST_DATA_DIR , [])]): assert sys.path == [ PROJECT_ROOT_DIR ] Figure 7: Example of failed gistified file: the code fails to import pytest. The model hallucinates the function test_discover_package_path_source_root_as_parent(), resulting in a test F1 score of 0 and a low line‚Äìexistence rate of 28.0% B.3 Tools Available in GitHub Copilot Table 5 shows the list of available tools in Github Copilot. B.4 Change Test even high performing models and frameworks (especially GPT-5 and GPT-5-mini) seems to modify test codes even though explicitly mentioned not to. We observed three common modification: (1) removing the test function but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10), (2) adding the \"__main__\" guard even though unnecessary (e.g., Figure 11), and (3) mocking a minimal in-memory package to bypass missing dependencies and force the test to run (e.g., Figure 12). 19",
  "Gistify! Codebase-Level Understanding via Runtime Execution @click.option(\"--all -methods\", is_flag=True , help=\"Show‚ê£HEAD‚ê£and‚ê£OPTIONS‚ê£methods.\") @with_appcontext def routes_command (sort , all_methods): \"\"\"Show‚ê£all‚ê£registered‚ê£routes‚ê£with‚ê£endpoints‚ê£and‚ê£methods.\"\"\" from flask import current_app rules = list(current_app.url_map.iter_rules ()) if not rules: click.echo(\"No‚ê£routes‚ê£were‚ê£registered.\") return Figure 8: Example of an Import Error: the gistified file imports from the original repository (e.g., from flask import current_app). T = t.TypeVar(\"T\") class ConfigAttribute (t.Generic[T]): \"\"\"Makes‚ê£an‚ê£attribute‚ê£forward‚ê£to‚ê£the‚ê£config\"\"\" def __init__( self , name: str , get_converter: t.Callable [[t.Any], T] | None = None ) -> None: self.__name__ = name self.get_converter = get_converter (a) Original Test Case class ConfigAttribute : def __init__( self , name: str , get_converter: t.Callable [[t.Any], T] | None = None ) -> None: self.__name__ = name self.get_converter = get_converter (b) Gistified File Figure 9: Example of an Pytest Runtime Error: gistified file fails with error message E TypeError: type ‚ÄôConfigAttribute‚Äô is not subscriptable B.5 Additional Metrics Table 6 shows the result of additional evaluation metrics, including the Average Pytest Pass Rate, which is defined as the average test pass rate over cases with at least one successful run, and the Test F1 Score, which quantifies the line-wise F1 existence between the test functions in the original codebase and those in the gistified fie. GPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully generate, they tend to pass all pytest. For the Test F1 Score, Claude-4 shows the highest performance, aliging with the trend discussed in Section 3.4. C Analysis C.1 Effect of various strategies and tools Prompt-Based Guidance We experiment with two variants of the prompt, Reading and Tracing, where, on top of the base prompt (Figure 3), we add specific instructions of How to Operate to encourage reasoning using a particular strategy. The addition prompt detail of Reading is in Figure 14, and for Tracing is in Figure 15. Global Information via Tools We experiment with two tools that provide global information: RepoGraph and Tracing. Details of the information provided to the model about each tool are shown in Figure 16. RepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level structure. It parses code at the line level, extracts relationships, and constructs a graph where each node represents a line of code and each edge encodes dependencies between code definitions and their references. 20",
  "Gistify! Codebase-Level Understanding via Runtime Execution Tool Description copilot_getNotebookSummary Returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. Useful for getting cell IDs, execution order, and outputs. edit_notebook_file Edit an existing Notebook file in the workspace. Supports inserting, deleting, or editing cells while preserving whitespace and indentation. apply_patch Edit text files using a special diff/patch format. Do not use for Jupyter notebooks. semantic_search Run a natural language search for relevant code or documentation comments in the workspace. create_directory Create a new directory structure in the workspace (like mkdir -p). create_file Create a new file with specified content. Automatically creates directories if they do not exist. file_search Search for files in the workspace by glob pattern (e.g., **/*.js). Returns matching paths only. test_search For a source file, find the corresponding test file, and vice versa. grep_search Fast text or regex search in the workspace. Useful for exact string or regex queries. run_notebook_cell Run a code cell in a notebook file and return the output. Avoid running Markdown cells. read_notebook_cell_output Retrieve the latest output for a notebook cell, even if not run in the current session. get_search_view_results Returns results from the search view. github_repo Search a GitHub repository for relevant code snippets. Use only for external repos, not local workspaces. insert_edit_into_file Insert or edit code in an existing file using minimal hints, avoiding duplication of unchanged code. install_extension Install an extension in VS Code. Used only during workspace creation. list_dir List the contents of a directory (folders and files). create_new_jupyter_notebook Generate a new Jupyter Notebook (.ipynb) in VS Code. create_new_workspace Set up a complete new project (scaffolding, dependencies, config, boilerplate). get_project_setup_info Provides project setup information for a VS Code workspace after workspace creation. read_file Read the contents of a file. Supports offsets and limits for large files. open_simple_browser Preview or open a URL in VS Code‚Äôs Simple Browser. test_failure Include test failure information in the prompt. think Think deeply about a request and log structured reasoning (no execution). Useful for planning, debugging, and brainstorming. get_vscode_api Retrieve comprehensive VS Code API documentation and references for exten- sion development. run_vscode_command Run a VS Code command by ID with arguments. Used mainly in workspace creation. fetch_webpage Fetch main content from a webpage for summarization or analysis. Table 5: Available tools and their descriptions. We note that many tools available to the agent are never used. Table 6: Average Pytest Pass Rate and Test F1 Score of different models using SWE-Agent on the main table (Table 1) test dataset. Models Execution Fidelity Average Pytest Pass Rate Test F1 Score GPT-5-mini 30.9 49.2 47.9 GPT-5 30.7 88.8 45.0 Claude-3.7 40.7 61.9 55.9 Claude-4 56.7 72.2 60.0 21",
  "Gistify! Codebase-Level Understanding via Runtime Execution class TestGetNetrcAuth : def test_works(self , tmp_path , monkeypatch): netrc_path = tmp_path / \".netrc\" monkeypatch.setenv(\"NETRC\", str(netrc_path)) with open(netrc_path , \"w\") as f: f.write(\"machine‚ê£example.com‚ê£login‚ê£aaaa‚ê£password‚ê£bbbb\\n\") auth = get_netrc_auth (\"http :// example.com/thing\") assert auth == (\"aaaa\", \"bbbb\") (a) Original Test Case if __name__ == \"__main__\": # Reproduce tests/test_utils.py:: TestGetNetrcAuth :: test_works with tempfile. TemporaryDirectory () as tmpdir: netrc_path = os.path.join(tmpdir , \".netrc\") os.environ[\"NETRC\"] = netrc_path with open(netrc_path , \"w\") as f: f.write(\"machine‚ê£example.com‚ê£login‚ê£aaaa‚ê£password‚ê£bbbb\\n\") auth = get_netrc_auth (\"http :// example.com/thing\") assert auth == (\"aaaa\", \"bbbb\") (b) Gistified File Figure 10: Test Modification Case 1: The test TestGetNetrcAuth.test_works is converted from a pytest unit test into a standalone script. # Test class and method - preserved unchanged class TestArgparseOptionsProviderMixin : \"\"\"Tests‚ê£for‚ê£the‚ê£argparse‚ê£implementation ‚ê£of‚ê£OptionsProviderMixIn . ‚ê£‚ê£‚ê£‚ê£The‚ê£logger‚ê£checker‚ê£is‚ê£used‚ê£as‚ê£an‚ê£example‚ê£checker‚ê£for‚ê£this‚ê£implementation . ‚ê£‚ê£‚ê£‚ê£\"\"\" @staticmethod def test_logger_without_options () -> None: \"\"\"Check‚ê£that‚ê£we‚ê£raise‚ê£messages‚ê£when‚ê£we‚ê£do‚ê£not‚ê£supply‚ê£any‚ê£options.\"\"\" with pytest.raises(SystemExit) as ex: Run([ LOGGING_TEST ]) assert ex.value.code == 2 # Main execution for pytest if __name__ == \"__main__\": test = TestArgparseOptionsProviderMixin () test. test_logger_without_options () Figure 11: Test Modification Case 2: Adding unnecessary \"__main__\" guard Thereby, when given a specific module, it returns the relationship with other modules as represented within the constructed graph. Tracing is a tool that uses the tracer provided from the sys module to execute a command and track which components of the codebase are accessed. When the model uses the tool with a specific command, the tool provides the model with the files and functions touched when running the command, in the order in which they are encountered. Execution-Based Tools We experiment with two execution-based tools: the Bash tool and the Edit and Execute tool. The Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. In contrast, the Edit and Execute tool is designed specifically for working with the gistified file: it enables the model to create or modify the gistified file and optionally execute it to verify changes. The primary difference between the two tools is their scope of execution. The Bash tool can run commands on both the original codebase and the gistified file, whereas the Edit and Execute tool is restricted to executing only the gistified file. We include an example of the behavior observed when adding the execution tool in Figure 17. Common patterns we observe are: (1) the model first runs the provided command to identify which files are accessed 22",
  "Gistify! Codebase-Level Understanding via Runtime Execution # Create a minimal in -memory ‚Äôrequests ‚Äô package with required submodules. requests_mod = types.ModuleType(‚Äôrequests ‚Äô) requests_mod .__path__ = [] compat_mod = types.ModuleType(‚Äôrequests.compat ‚Äô) structures_mod = types.ModuleType(‚Äôrequests.structures ‚Äô) # Populate compat with only what ‚Äôs needed by this test suite import paths. compat_mod.Mapping = Mapping compat_mod. MutableMapping = MutableMapping compat_mod.urljoin = urljoin # Populate structures with the classes. structures_mod . CaseInsensitiveDict = CaseInsensitiveDict structures_mod .LookupDict = LookupDict # Wire the package hierarchy and register in sys.modules. requests_mod.compat = compat_mod requests_mod.structures = structures_mod sys.modules[‚Äôrequests ‚Äô] = requests_mod sys.modules[‚Äôrequests.compat ‚Äô] = compat_mod sys.modules[‚Äôrequests.structures ‚Äô] = structures_mod if __name__ == ‚Äô__main__ ‚Äô: import pytest raise SystemExit(pytest.main ([‚Äô-q‚Äô, ‚Äôtests/ test_structures .py:: TestCaseInsensitiveDict :: test_list ‚Äô])) Figure 12: Test Modification Case 3: Manually mocking a minimal in-memory package to bypass missing dependencies and force the test to run. @pytest.mark.parametrize( \"value ,‚ê£expected\", ( (‚Äôfoo=\"is‚ê£a‚ê£fish\",‚ê£bar=\"as‚ê£well\"‚Äô, {\"foo\": \"is‚ê£a‚ê£fish\", \"bar\": \"as‚ê£well\"}), (\" key_without_value \", {\" key_without_value \": None }), ), ) def test_parse_dict_header (value , expected): assert parse_dict_header (value) == expected (a) Original Test Case assert parse_dict_header (‚Äôfoo=\"is‚ê£a‚ê£fish\",‚ê£bar=\"as‚ê£well\"‚Äô) == {\"foo\": \"is‚ê£a‚ê£fish\", \"bar\": \"as‚ê£well\"} assert parse_dict_header (\" key_without_value \") == {\" key_without_value \": None} (b) Gistified File Figure 13: The test function test_parse_dict_header is simplified: in the original, it used @pytest.mark.parametrize to feed multiple input/expected pairs into one function; in the gistified version, this is replaced with two direct assert statements, one per case. Table 7: Analysis of tool usage during the Gistify task Models Avg. tool usage view search execute other GPT-5-mini 10.8 71.9 9.8 1.7 16.6 GPT-5 18.5 72.4 8.3 3.3 16.1 Claude-Sonnet-3.7 17.3 67.5 10.1 4.5 17.9 Claude-Sonnet-4 19.3 74.6 2.1 11.8 11.5 and to gather execution feedback; (2) after creating a file, it iteratively executes it to verify that the generated gistified file behaves as expected; and (3) it repeatedly compares the outputs of the gistified file and the original codebase under the given command. We also observe that, due to this iterative checking process, enabling the execution tool often leads the model to terminate because it reaches the maximum step limit. 23",
  "Gistify! Codebase-Level Understanding via Runtime Execution Behavior Reading How to Operate: 1. Examine the test file and the test function used for {problem statement} 2. Identify which module used by these functions are defined in {working dir} 3. Copy and inline the code from those modules into ‚Äòconcise.py‚Äô 4. Check these modules for any internal functions or classes and inline them as needed. 5. Repeat this process recursively until all internal dependencies are inlined. 6. Do not forget to copy and paste external imports. Figure 14: Prompt for Reading strategy. Trace Reasoning How to Operate: 1. Predict the execution traces. 2. Follow the traces and inline (copy) only the necessary executed lines into ‚Äòconcise.py‚Äô 3. Repeat until all traces are fully handled. Figure 15: Prompt for Tracing strategy. C.2 Tool Usage Rates Table 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four categories: view, search, execute, and other, which includes all remaining tools. For all models, we compute usage rates both with and without execution enabled, and then average across the two settings. Among all models, Claude-4 exhibits the highest average tool usage for each test cases, followed by GPT-5, Claude-3.7, and GPT-5-mini. In terms of specific functionality, Claude-4 shows the highest rate of both view and execute tool usage, while Claude-3.7 shows the highest usage of the search tool. To generate a high-quality gistified file, a model must effectively view relevant files and copy only the necessary content. The strong performance of Claude-4 on line existence may be related to its high usage of the view tool. Also, the execution tool tends to support correctness verification of the generated file, which would lead to high execution fidelity. Tool Information * search repo (repograph): searches in the mentioned repository with a specific function or class, and returns the def and ref relations for the search term. * trace info (tracing): returns the trace log when running the ‚Äòpytest‚Äô command for the test currently being debugged in the given repo Figure 16: Details of the information provided to the model about each tool. 24",
  "Gistify! Codebase-Level Understanding via Runtime Execution { \"role\": \"assistant\", \"tool_calls\": { \"arguments\": \"{\\\" command \\\":\\\" cd‚ê£/app‚ê£&&‚ê£python‚ê£-m‚ê£pytest‚ê£ concise.py:: TestDeprecationOptions :: test_new_names ‚ê£-v\\\"}\", \"name\": \"bash\" } }, { \"role\": \"tool\", \"content\": [ { \"type\": \"text\", \"text\": \"OBSERVATION:‚ê£test‚ê£session‚ê£starts‚ê£... AssertionError \\n... pylint:‚ê£error:‚ê£unrecognized ‚ê£ arguments:‚ê£/app/regrtest_data /empty.py\\n‚ê£FAILED‚ê£ concise.py:: TestDeprecationOptions :: test_new_names ‚ê£...\" } }, ... (fix concise.py based on observation) ... { \"role\": \"assistant\", \"tool_calls\": { \"arguments\": \"{\\\" command \\\":\\\" cd‚ê£/app‚ê£&&‚ê£python‚ê£-m‚ê£pytest‚ê£ tests/config/ test_argparse_config .py:: TestDeprecationOptions :: test_new_names ‚ê£-v‚ê£-s\\\"}\", \"name\": \"bash\" } }, ... { \"role\": \"assistant\", \"content\": \"Excellent!‚ê£The‚ê£messages‚ê£are‚ê£identical.‚ê£Let‚ê£me‚ê£verify‚ê£my‚ê£implementation ‚ê£removes‚ê£all‚ê£ unused‚ê£imports‚ê£and‚ê£only‚ê£includes‚ê£what ‚Äôs‚ê£needed:\" } Figure 17: Trajectory when including execution tool. Models often iteratively execute the given codebase or generated gistified file to ensure that it operates expectedly. 25",
  "Defeating the Training-Inference Mismatch via FP16 Penghui Qi*‚Ä†1,2, Zichen Liu*1,2, Xiangxin Zhou*1, Tianyu Pang1, Chao Du1, Wee Sun Lee2, Min Lin1 1Sea AI Lab 2National University of Singapore ¬á https://github.com/sail-sg/Precision-RL Abstract Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dy- namic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to FP16 effectively eliminates this mismatch. The change is simple, fully supported by mod- ern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger per- formance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning. 0 250 500 750 1000 1250 1500 1750 2000 Training Steps 0.5 0.6 0.7 0.8 0.9 1.0 (a) Sanity GRPO BF16 FP16 0 500 1000 1500 2000 2500 Training Steps 0.6 0.7 0.8 0.9 1.0 (b) Sanity GRPO-Token-TIS BF16 FP16 0 500 1000 1500 2000 2500 Training Steps 0.5 0.6 0.7 0.8 0.9 1.0 (c) Sanity GRPO-Seq-MIS BF16 FP16 0 500 1000 1500 2000 Training Steps 0.6 0.7 0.8 0.9 1.0 (d) Sanity GSPO BF16 FP16 0 500 1000 1500 2000 Training Steps 0.5 0.6 0.7 0.8 0.9 1.0 (e) Sanity PG-Seq-IS BF16 FP16 0 500 1000 1500 2000 2500 Training Steps 0.6 0.7 0.8 0.9 1.0 (f) Sanity PG-Seq-MIS BF16 FP16 0 200 400 600 800 1000 Training Steps 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (g) OctoThinker GRPO BF16 FP16 0 200 400 600 800 1000 1200 1400 Training Steps 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 (h) Lora GRPO-Token-TIS BF16 FP16 0 20 40 60 80 100 120 140 160 Training Steps 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (i) MoE GRPO-Seq-MIS BF16 FP16 0 50 100 150 200 Training Steps 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (j) MoE GRPO-Token-TIS BF16 FP16 0 25 50 75 100 125 150 175 Training Steps 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (k) MoE PG-Seq-TIS BF16 FP16 0 10 20 30 40 50 60 70 80 Training Steps 0.60 0.65 0.70 0.75 0.80 (l) Dense-14B DAPO BF16 FP16 Figure 1: Training reward comparison between BF16 and FP16. We evaluate across diverse settings: our Sanity test (Section 4) with various algorithms (GRPO, GSPO, TIS, MIS, PG); different model families (R1D, Qwen and OctoThinker); alternative fine-tuning methods (Lora); and larger scale models (Dense-14B, MoE). Results are validated on two independent frameworks (VeRL and Oat). ‚àóCore Contributors. ‚Ä†Project Lead. Preprint. Work in process. arXiv:2510.26788v1 [cs.LG] 30 Oct 2025",
  "methods (Lora); and larger scale models (Dense-14B, MoE). Results are validated on two independent frameworks (VeRL and Oat). ‚àóCore Contributors. ‚Ä†Project Lead. Preprint. Work in process. arXiv:2510.26788v1 [cs.LG] 30 Oct 2025",
  "1 Introduction Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large language models (LLMs) to boost the reasoning performance [Guo et al., 2025, Zeng et al., 2025, Liu et al., 2025c, Qi et al., 2025]. However, the path to achieving high-performing models through RL is often fraught with instability. The training process is notoriously sensitive to hyperparameters and can suffer from training collapse, making it a significant challenge to reliably improve model performance [Yao et al., 2025, Liu et al., 2025a, Team et al., 2025a, Zheng et al., 2025, Yu et al., 2025, Cui et al., 2025]. This fragility has spurred a continuous search for methods that can stabilize and streamline the RL fine-tuning process. A critical source of this instability stems from a fundamental discrepancy in modern RL frame- works: the training-inference mismatch. To accelerate training, these frameworks typically use different computational engines, a highly optimized one for fast inference (rollout) and another for training (gradient computation). While mathematically identical, these engines produce numerically different outputs due to precision errors and hardware-specific optimizations. As recent work has highlighted [Yao et al., 2025, Liu et al., 2025a, Team et al., 2025a], this seemingly minor mismatch between the inference and the training introduces significant issues into the optimization process. Existing solutions have attempted to address this mismatch through algorithmic patches based on importance sampling. Notably, Yao et al. [2025] introduced a token-level importance sampling ratio as a patch to the GRPO [Shao et al., 2024] gradient. While this simple correction can prolong training, it was later shown by Liu et al. [2025a] to be insufficient to fully stabilize training due to its biased gradient. As an alternative, they proposed using an unbiased, sequence-level importance sampling ratio for the correction. Although this method is more stable, its effectiveness is hampered by slow convergence speed, a direct consequence of the high variance inherent in sequence-level ratios. Furthermore, both of these algorithmic approaches suffer from two fundamental problems: 1. They are computationally inefficient. The implementations from Yao et al. [2025] and Liu et al. [2025a] require an extra forward pass to compute the importance sampling ratio for their correction. Assuming a backward pass is twice the cost of a forward pass [Qi et al., 2023], this adds approximately 25% to the training cost. 2. The deployment gap persists. By design, these solutions correct for the mismatch during training, but the final model parameters are optimized with respect to the training engine‚Äôs probability distribution. This means the resulting model is not truly optimal for the inference engine used in deployment, which can lead to a tangible performance drop. This calls for a solution that eliminates the mismatch at its source, rather than merely compensating for it. In this work, we take a step back from the complex algorithmic fixes and investigate the root cause of the numerical mismatch: floating-point precision. We identify that the modern standard for mixed-precision training, BFloat16 (BF16), is the primary culprit. While BF16 has a wide dynamic range which is excellent for stable pre-training, its low precision makes it highly",
  "from the complex algorithmic fixes and investigate the root cause of the numerical mismatch: floating-point precision. We identify that the modern standard for mixed-precision training, BFloat16 (BF16), is the primary culprit. While BF16 has a wide dynamic range which is excellent for stable pre-training, its low precision makes it highly susceptible to rounding errors that accumulate and eventually cause the training and inference policies to diverge. Our key finding is super simple: by switching from BF16 to the FP16 during RL fine-tuning, we can virtually eliminate the training-inference mismatch. With more mantissa bits, FP16 offers higher numerical precision, making results less sensitive to the implementation differences between training and inference. The benefits of this simple change are multifold. It eliminates the complex algorithmic workarounds and the accompanying probability evaluations, restoring RL to its purest importance weighted policy-gradient form. It also closes the deployment gap that none of the existing fixes address. Empirical evaluations show a significant and uniform boost over both performance and stability, presenting a clean, efficient, and universally applicable solution to a critical challenge in RL-based LLM alignment. 2 Background In modern RL frameworks for LLM fine-tuning, different engines are used for inference and training to maximize system efficiency, which inevitably creates a mismatch between the inference policy ¬µ(¬∑|Œ∏) and training policy œÄ(¬∑|Œ∏) due to subtle numerical discrepancies, even though, in principle, the two should be mathematically identical (¬µ = œÄ). This mismatch brings two issues elaborated below, 2",
  "Biased Gradient To optimize the trainer policy œÄ(¬∑|Œ∏), we typically adopt the following objective: J (Œ∏) = Ex‚àºpX h J (x, Œ∏) i = Ex‚àºpX h Ey‚àºœÄ(¬∑|x,Œ∏)[R(x, y)] i , (1) where x is the prompt sampled from a distribution pX , y is the response, and R(x, y) is the reward of y. The policy gradient can be calculated by REINFORCE estimator [Williams, 1992, Sutton and Barto, 2018]: ‚àáŒ∏J (Œ∏) = Ex‚àºpX h ‚àáŒ∏J (x, Œ∏) i , ‚àáŒ∏J (x, Œ∏) = Ey‚àºœÄ(¬∑|x,Œ∏) h ‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ R(x, y) i . (2) In practice, we sample the responses from the inference policy ¬µ, instead of the training policy œÄ. As noted by Yao et al. [2025] and Liu et al. [2025a], the policy gradient would become biased if simply ignoring this mismatch. ‚àáŒ∏Jbiased(x, Œ∏) = Ey‚àº¬µ(¬∑|x,Œ∏) h ‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ R(x, y) i Ã∏= ‚àáŒ∏J (x, Œ∏) (3) Deployment Gap Another important but hard to fix issue is the deployment gap. Though it is œÄ(¬∑|Œ∏) that we train, it is ¬µ(¬∑|Œ∏) that we use for deployment and evaluation. However, the parameter Œ∏ optimized under the training engine œÄ is not necessarily optimal for the inference engine ¬µ: arg max Œ∏ Ex‚àºpX ,y‚àº¬µ(¬∑|x,Œ∏)[R(x, y)] Ã∏= arg max Œ∏ Ex‚àºpX ,y‚àºœÄ(¬∑|x,Œ∏)[R(x, y)] (4) This deployment gap results in a non-trivial performance degrade due to this mismatch. While algorithmic patches [Yao et al., 2025, Liu et al., 2025a] fix the biased gradient, by nature they cannot close the deployment gap, which calls for a fundamental solution to remove the mismatch altogether. 2.1 Correcting Biased Gradient via Importance Sampling To correct the biased gradient introduced by the training-inference mismatch, a principled approach is to use importance sampling (IS). This method re-weights the gradient calculation using a sequence- level probability ratio, ensuring the gradient estimator remains unbiased. The policy gradient for a given prompt x is thus corrected as: ‚àáŒ∏Jpg-is(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) \u0014 œÄ(y|x, Œ∏) ¬µ(y|x, Œ∏‚Ä≤)‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ A(x, y) \u0015 , (5) where Œ∏‚Ä≤ denotes the parameters used for sampling, which may differ from Œ∏ in an off-policy setting. The term A(x, y) = R(x, y) ‚àíB(x) is the advantage, with B(x) serving as a baseline for variance reduction [Sutton and Barto, 2018]. While theoretically sound, this estimator often suffers from high variance, particularly in the context of LLMs where response sequences are long, leading to extreme probability ratios. To mitigate this, techniques that trade a small amount of bias for a significant reduction in variance, such as Truncated Importance Sampling (TIS) [Espeholt et al., 2018, Yao et al., 2025] and Masked Importance Sampling (MIS) [Zheng et al., 2025, Team et al., 2025b, Liu et al., 2025a], have been proposed: ‚àáŒ∏Jpg-tis(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) \u0014 min \u0012 œÄ(y|x, Œ∏) ¬µ(y|x, Œ∏‚Ä≤), C \u0013 ¬∑ ‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ A(x, y) \u0015 , (6) ‚àáŒ∏Jpg-mis(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) \u0014 œÄ(y|x, Œ∏) ¬µ(y|x, Œ∏‚Ä≤) ¬∑ I \u001a œÄ(y|x, Œ∏) ¬µ(y|x, Œ∏‚Ä≤) ‚â§C \u001b ¬∑ ‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ A(x, y) \u0015 , (7) where C is a clipping hyperparameter and",
  "¬µ(y|x, Œ∏‚Ä≤), C \u0013 ¬∑ ‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ A(x, y) \u0015 , (6) ‚àáŒ∏Jpg-mis(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) \u0014 œÄ(y|x, Œ∏) ¬µ(y|x, Œ∏‚Ä≤) ¬∑ I \u001a œÄ(y|x, Œ∏) ¬µ(y|x, Œ∏‚Ä≤) ‚â§C \u001b ¬∑ ‚àáŒ∏ log œÄ(y|x, Œ∏) ¬∑ A(x, y) \u0015 , (7) where C is a clipping hyperparameter and I{¬∑} is the indicator function. These methods stabilize training by controlling the magnitude of the importance weights. 2.1.1 Existing Implementations Although generally inspired by the importance sampling principle, recent methods [Yao et al., 2025, Liu et al., 2025a] are effectively implemented as auxiliary patches on top of GRPO, rather than 3",
  "adhering to the strictly principled formulation. Unfortunately, many widely used RL frameworks (e.g., VeRL [Sheng et al., 2024]) are GRPO-centric and do not natively provide the standard importance- weighted estimators outlined in Equation (5), Equation (6), and Equation (7). The standard GRPO gradient [Shao et al., 2024, Liu et al., 2025c], which does not correct for the training-inference mismatch, is calculated as follows:1 ‚àáŒ∏Jgrpo(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) Ô£Æ Ô£∞ |y| X t=1 ‚àáŒ∏ min (rtAt, clip(rt, 1 ‚àíœµ, 1 + œµ)At) Ô£π Ô£ª, where rt = œÄ(yt|x, y<t, Œ∏) œÄ(yt|x, y<t, Œ∏‚Ä≤) and At = R(x, y) ‚àí 1 G ‚àí1 G‚àí1 X i=1 R(x, yi). (8) For each prompt x, a group of G responses {yi}G i=1 is sampled from the inference policy ¬µ(¬∑|x, Œ∏‚Ä≤) to compute the advantage function At as in GRPO and RLOO [Ahmadian et al., 2024, Kool et al., 2019]. Based on GRPO, Yao et al. [2025] introduced a token-level TIS correction: ‚àáŒ∏Jgrpo-tok-tis(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) Ô£Æ Ô£∞ |y| X t=1 min(œÅt, C) ¬∑ ‚àáŒ∏ min (rtAt, clip(rt, 1 ‚àíœµ, 1 + œµ)At) Ô£π Ô£ª, where œÅt = œÄ(yt|x, y<t, Œ∏‚Ä≤) ¬µ(yt|x, y<t, Œ∏‚Ä≤). (9) Subsequently, Liu et al. [2025a] advanced this approach by proposing a sequence-level MIS variant. This correction is applied to the entire GRPO gradient term, using a single ratio for the whole sequence to determine whether the update is applied: ‚àáŒ∏Jgrpo-seq-mis(x) = Ey‚àº¬µ(¬∑|x,Œ∏‚Ä≤) Ô£Æ Ô£∞œÅ ¬∑ I{œÅ ‚â§C} ¬∑ |y| X t=1 ‚àáŒ∏ min (rtAt, clip(rt, 1 ‚àíœµ, 1 + œµ)At) Ô£π Ô£ª, where œÅ = œÄ(y|x, Œ∏‚Ä≤) ¬µ(y|x, Œ∏‚Ä≤). (10) Compared to the vanilla policy gradient estimators (Equation (5) and its TIS/MIS variants), existing GRPO-based implementations require an additional forward pass to compute œÄ(¬∑|Œ∏‚Ä≤) for their off- policy correction. This extra step incurs approximately 25% computational overhead during training, assuming a backward pass is twice as costly as a forward pass [Qi et al., 2023]. 2.2 Engineering Attempts to Reduce the Mismatch Another line of work attempts to mitigate the training-inference mismatch from an engineering perspective, but with limited success. Early attempts, such as using an FP32 language model head by Chen et al. [2025], is shown to be insufficient to prevent training collapse [Yao et al., 2025, Liu et al., 2025a]. Very recently, Team et al. [2025a] reported promising results by manually aligning training and inference implementations. However, this approach requires deep domain knowledge and substantial engineering effort, and it is unclear whether such bespoke fixes can be generalized across different frameworks or models. A tangentially related work by He [2025] demonstrated how to enforce determinism in inference, their method incurs a significant efficiency cost and cannot directly address the training-inference mismatch. Despite these engineering efforts, the mismatch persists due to fundamental differences between training and inference computations that are difficult to reconcile. For example, tokens are generated auto-regressively during inference but are processed in parallel during training. Different paralleliza- tion strategies and precision-sensitive operations such as top-k expert selection in Mixture-of-Experts (MoE) models, further complicate the situation. This inherent difficulty highlights the need for a more fundamental solution that avoids such complex",
  "For example, tokens are generated auto-regressively during inference but are processed in parallel during training. Different paralleliza- tion strategies and precision-sensitive operations such as top-k expert selection in Mixture-of-Experts (MoE) models, further complicate the situation. This inherent difficulty highlights the need for a more fundamental solution that avoids such complex and brittle engineering workarounds. 1We use the Dr.GRPO variant to remove the length and difficulty biases of the vanilla GRPO. 4",
  "3 Revisiting FP16 Precision In our investigation of the training‚Äìinference mismatch, we identify a surprisingly simple yet highly effective remedy that avoids complex algorithmic or engineering fixes. Rather than introducing additional machinery, we focus on a more fundamental factor: numerical precision. We find that merely switching the training precision from the now-dominant BF16 format [Dean et al., 2012, Kalamkar et al., 2019] to the earlier Float16 (FP16) format [Micikevicius et al., 2017] substantially mitigates the policy mismatch and yields significant performance improvements across RL algorithms. This section revisits the history and characteristics of these floating-point formats to shed light on this counterintuitive but powerful result. 3.1 FP16 vs. BF16 Floating-point formats represent real numbers by dividing their bit budget between two components: exponent bits, which determine the range (how large or small a value can be), and mantissa bits (also known as fraction bits), which determine the precision (how finely values can be distinguished within that range). Both FP16 and BF16 use 16 bits in total, but they allocate these bits differently, resulting in distinct trade-offs between range and precision (see Table 1). FP16 (IEEE 754 half-precision) allocates 5 bits to the exponent and 10 bits to the mantissa. The relatively large mantissa gives FP16 higher numerical precision, allowing it to represent small differences between nearby values accurately. However, its limited 5-bit exponent severely constrains the dynamic range, making FP16 prone to overflow (values exceeding the representable maximum) and underflow (values rounding to zero). Training with FP16 often requires stability techniques such as loss scaling to mitigate these issues (see Section 3.2). BF16 (bfloat16), introduced by Google, allocates 8 bits to the exponent‚Äîmatching the range of the 32-bit FP32 format‚Äîand only 7 bits to the mantissa. This design provides a wide dynamic range comparable to FP32, making BF16 highly resistant to overflow and underflow, at the cost of reduced precision. The resulting numerical robustness under low precision is the key reason for its widespread adoption in large-scale deep learning systems. Table 1: Comparison of 16-bit Floating-Point Formats. Property FP16 BF16 Bit Allocation Exponent Bits 5 8 Mantissa Bits 10 7 Dynamic Range Smallest Positive Normal ‚âà6.1 √ó 10‚àí5 ‚âà1.2 √ó 10‚àí38 Largest Value ‚âà6.6 √ó 104 ‚âà3.4 √ó 1038 Precision Next Representable > 1 1 + 2‚àí10 ‚âà1.000977 1 + 2‚àí7 ‚âà1.007812 3.2 Stabilizing FP16 Training with Loss Scaling The primary challenge with FP16‚Äôs limited range is gradient underflow, which can be effectively solved early in the history of mixed-precision training with a technique called loss scaling [Micikevi- cius et al., 2017]. The procedure is straightforward: 1. The loss is multiplied by a large scaling factor S before backpropagation. 2. This scales up all gradients by S, shifting small gradient values out of the underflow region and into the representable range of FP16, thus preserving them. 3. Before updating the weights, the gradients are scaled back by dividing S. Modern implementations have further improved this with dynamic loss scaling. The scaling factor S is automatically adjusted during training, increased if no overflows (infinity values in gradients) are detected for a number of",
  "FP16, thus preserving them. 3. Before updating the weights, the gradients are scaled back by dividing S. Modern implementations have further improved this with dynamic loss scaling. The scaling factor S is automatically adjusted during training, increased if no overflows (infinity values in gradients) are detected for a number of steps, and decreased immediately if an overflow occurs. 5",
  "Table 2: Evaluation scores of DeepSeek-R1-Distill-Qwen-1.5B using under different precisions (BF16, FP16 and FP32) and token budgets (8K and 32K). dtype AMC23 (8K) AIME24 (8K) AMC23 (32K) AIME24 (32K) BF16 50.38 22.60 62.35 29.90 FP16 50.60 20.10 63.10 30.94 FP32 51.54 22.30 62.42 28.44 Crucially, these loss scaling techniques are standard, mature components in mainstream training frameworks (e.g., PyTorch [Paszke et al., 2019], Megatron [Shoeybi et al., 2019], DeepSpeed [Rasley et al., 2020]). Enabling them typically requires only a single configuration change or a few lines of code, making the adoption of FP16 training both simple and robust. 3.3 The Rise of BF16 in Modern LLM Training Despite the effectiveness of loss scaling, it complicates the system in distributed settings. Because a global synchronization is needed before the optimizer step to check for overflows and ensure the scaling factor is aligned across all workers. The introduction of BF16 on hardware like Google TPUs and later NVIDIA GPUs (starting with the Ampere architecture) is a game-changer. Having a same dynamic range as FP32, BF16 offered a ‚Äúdrop-in‚Äù replacement for FP32 that obviates meticulous loss scaling. Its resilience to overflow and underflow made training LLMs significantly more stable and straightforward. Consequently, BF16 quickly became the de-facto standard for modern mixed-precision training. 3.4 Why FP16 is the Key for RL Fine-Tuning While BF16‚Äôs stability is an advantage for pre-training models, our findings reveal that its low precision is the origin of the training-inference mismatch. Modern RL frameworks often use different engines or optimized kernels for training and inference. Even if both are configured to use BF16, subtle differences in their implementation (e.g., CUDA kernel optimizations, parallel strategies) can lead to different rounding errors on BF16. When these small discrepancies accumulate over a sequence of tokens during autoregressive sampling, the resulting probability distributions for œÄ and ¬µ can diverge significantly. This divergence is the source of the biased gradients and the deployment gap discussed earlier. This is precisely why switching to FP16 provides a fundamental solution. With its 10 mantissa bits, FP16 offers 8 times more precision (210 values vs. 27 values) than BF16. This higher fidelity means that the outputs of the training and inference engines are much more likely to be numerically identical. The increased precision creates a buffer that absorbs the minor implementation differences between the two engines, preventing rounding errors from accumulating and causing a policy divergence. For RL fine-tuning, the dynamic range of the model‚Äôs weights and activations has already been established during pre-training. Therefore, the extreme range of BF16 is less critical, while the precision it sacrifices becomes a dominant drawback. By reverting to FP16, we trade the unnecessary range of BF16 for the critical precision, effectively closing the gap between training and inference without any complex algorithmic or engineering workaround. 3.5 Offline Analysis Results Before proceeding to RL fine-tuning, we first perform an offline analysis to examine performance and training‚Äìinference mismatch under different numeric precisions. We begin by sampling 32 responses per question from the AMC and AIME benchmarks [Li et al., 2024] using the DeepSeek-R1-Distill- Qwen-1.5B model2 [Guo",
  "workaround. 3.5 Offline Analysis Results Before proceeding to RL fine-tuning, we first perform an offline analysis to examine performance and training‚Äìinference mismatch under different numeric precisions. We begin by sampling 32 responses per question from the AMC and AIME benchmarks [Li et al., 2024] using the DeepSeek-R1-Distill- Qwen-1.5B model2 [Guo et al., 2025], with a 32K total token budget under both BF16 and FP16 precisions. As shown in Table 2, their performance is largely comparable, suggesting that higher inference precision alone does not necessarily yield improvements. 2We follow their recommended decoding settings: temperature 0.6 and top-p 0.95. 6",
  "0.0 0.2 0.4 0.6 0.8 1.0 Inference policy 0.0 0.2 0.4 0.6 0.8 1.0 Training policy Token Probability (BF16) No mismatch ( = ) 0.0 0.2 0.4 0.6 0.8 1.0 Inference policy 0.0 0.2 0.4 0.6 0.8 1.0 Training policy Token Probability (FP16) No mismatch ( = ) 0 5 10 15 20 25 Sequence length (K) 50 40 30 20 10 0 log Slope = -1.01 KL[ | ] = 7.64 Seq mismatch v.s. Len (BF16) 0 5 10 15 20 25 Sequence length (K) 50 40 30 20 10 0 log Slope = -0.07 KL[ | ] = 0.32 Seq mismatch v.s. Len (FP16) Figure 2: FP16 significantly reduces the training-inference mismatch. The left two plots show the token-level probability distribution, and the right two plots present the distribution of sequence-level log probability ratio between the inference policy (¬µ) and the training policy (œÄ). Dashed lines in black denote perfect precision without mismatch. Next, we re-generate 32 responses per question using temperature 1.0 and no top-p sampling (so that ¬µ is directly comparable to œÄ), and evaluate the token log-probabilities using the same model weights within the DeepSpeed training engine, under both BF16 and FP16 settings. The left two plots in Figure 2 show the resulting distributions of token probabilities. We find that FP16 notably reduces the mismatch between ¬µ and œÄ, with data points more tightly concentrated around the diagonal. Beyond token-level discrepancies, we also analyze sequence-level mismatch, since œÄ(y|x) ¬µ(y|x) serves as an unbiased estimator of the importance sampling weight for a full response. The right two plots in Figure 2 depict the distribution of sequence-level log-probability ratios across different generation lengths. The results clearly indicate that BF16 introduces an exponentially larger mismatch, which worsens with longer responses due to cumulative autoregressive errors, whereas FP16 maintains the mismatch at a much milder level (approximately 24√ó smaller). 4 A Sanity Test for RL Algorithms To rigorously assess the reliability and robustness of RL algorithms, we introduce a novel sanity test. Standard benchmarks often contain a mix of problems with varying difficulty, including questions that are either overly trivial or unsolvable by the initial model. Trivial questions waste computational resources, while unsolvable ones make it difficult to determine whether poor performance stems from a flawed algorithm or the model‚Äôs inherent limitations. Our sanity test is designed to remove this ambiguity with efficiency. By creating a perfectible dataset where every problem is known to be solvable but not trivial, we can cleanly isolate and evaluate an RL algorithm‚Äôs ability to unlock a model‚Äôs latent potential. On this perfectible dataset, a reliable RL algorithm should theoretically be able to achieve 100% training accuracy. We construct this perfectible dataset by filtering out those overly trivial and unsolvable ques- tions for the initial model. Specifically, we unroll 40 responses for each problem in the MATH dataset [Hendrycks et al., 2021], and only keep problems where the initial accuracy is between 20% and 80%. This process yielded a targeted dataset of 1,460 questions for the DeepSeek-R1-Distill- Qwen-1.5B model [Guo et al., 2025]. The smaller size",
  "Specifically, we unroll 40 responses for each problem in the MATH dataset [Hendrycks et al., 2021], and only keep problems where the initial accuracy is between 20% and 80%. This process yielded a targeted dataset of 1,460 questions for the DeepSeek-R1-Distill- Qwen-1.5B model [Guo et al., 2025]. The smaller size of this dataset makes achieving near-100% accuracy computationally feasible, allowing for efficient and conclusive testing. We define our sanity test with a clear criterion: an RL algorithm passes if its training accuracy on this perfectible dataset converges above a high threshold (e.g., 95%). An algorithm that fails this test can be considered unreliable or fundamentally flawed, as it is unable to guide the model to solve problems known to be within its reach. While passing is not a guarantee of universal success, failing is a strong indicator of an ill-suited algorithm design, making this test a crucial diagnostic tool. 4.1 Experimental Setup Under this sanity test, we evaluate several representative RL algorithms, particularly those designed to address the training-inference mismatch (see Section 2.1). All experiments use DeepSeek-R1- Distill-Qwen-1.5B as the initial model, with a context length of 8,000. We run each experiment on 8 NVIDIA A100 80G GPUs. For each policy iteration [Schulman et al., 2017], we use a batch size of 64 questions (with 8 rollouts per question) and perform 4 gradient steps. For algorithms in the 7",
  "0.5 0.6 0.7 0.8 0.9 1.0 VeRL Rewards BF16 GRPO BF16 GRPO-Token-TIS BF16 GRPO-Seq-MIS BF16 GSPO FP16 PG-Seq-IS 0.20 0.23 0.25 0.28 0.30 0.33 0.35 0.38 0.40 AIME 2024 0.00 0.01 0.02 0.03 0.04 0.05 0.06 Mean[Abs(pi - )] 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Max&Min of - No mismatch ( = ) 0 500 1000 1500 2000 0.5 0.6 0.7 0.8 0.9 1.0 OAT Rewards BF16 GRPO BF16 GRPO-Token-TIS BF16 GRPO-Seq-MIS BF16 GSPO FP16 PG-Seq-IS 0 500 1000 1500 2000 0.20 0.23 0.25 0.28 0.30 0.33 0.35 0.38 0.40 AIME 2024 0 500 1000 1500 2000 10 5 10 4 10 3 10 2 10 1 100 KL[ | ] 0 500 1000 1500 2000 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Max&Min of - No mismatch ( = ) Figure 3: Simply switching from BF16 to FP16 stabilizes and prolongs RL training. The basic importance-weighted policy gradient algorithm in FP16 outperforms all baselines in BF16. Note that the third metric reported in each row slightly differs in implementation due to the use of separate codebases (VeRL and Oat). These metrics are semantically similar, and the minor differences do not affect our conclusions. GRPO family, we set the clip_higher to 0.28 by default [Yu et al., 2025]. The clipping threshold for importance sampling methods (Equation (7) and Equation (10)) is set to C = 3. We evaluate a suite of methods designed to address the training-inference mismatch. This includes: ‚Ä¢ A vanilla GRPO baseline (specifically, the Dr.GRPO variant from Equation (8)) [Shao et al., 2024, Liu et al., 2025c]. ‚Ä¢ GRPO with a token-level TIS correction (Equation (9)) from Yao et al. [2025]. ‚Ä¢ GRPO with a sequence-level MIS correction (Equation (10)) from Liu et al. [2025a]. ‚Ä¢ The standard policy gradient algorithm with importance sampling (Equation (5)). In addition, we include GSPO [Zheng et al., 2025] in our experiments, although it was primarily designed to address the mismatch introduced by MoE models. 4.2 Comparison with Existing Algorithmic Corrections To ensure robustness and rule out implementation-specific artifacts, we conducted experiments across two different frameworks: VeRL3 [Sheng et al., 2024] and Oat [Liu et al., 2025b]. The results, shown in Figure 3, highlight the instability of existing methods when using BF16 precision. The vanilla GRPO baseline collapses early in training, reaching a peak accuracy of only 73% in VeRL and 84% in Oat before its performance degrades. The token-level TIS correction [Yao et al., 2025] prolongs training slightly but ultimately fails, collapsing after reaching 82% (VeRL) and 88% (Oat) accuracy, an observation that aligns with findings from Liu et al. [2025a]. Surprisingly, GSPO demonstrates more stable training for a longer period than GRPO with token-level TIS, achieving higher rewards despite not using the inference policy ¬µ at all.4 Among all the algorithmic corrections in BF16, only GRPO with sequence-level MIS [Liu et al., 2025a] maintains stable training without collapsing. However, this stability is costly. The method suffers from slow convergence due to the high variance of its sequence-level importance ratio (see Figure 2). More importantly, even at",
  "all.4 Among all the algorithmic corrections in BF16, only GRPO with sequence-level MIS [Liu et al., 2025a] maintains stable training without collapsing. However, this stability is costly. The method suffers from slow convergence due to the high variance of its sequence-level importance ratio (see Figure 2). More importantly, even at its peak, it exhibits a significant deployment gap compared to 3We identified and corrected an implementation bug in VeRL‚Äôs Dr.GRPO for our experiments. We optimized the training speed of VeRL based on https://github.com/sail-sg/odc. 4In our VeRL experiment, the GSPO gradient norm became ‚ÄòNaN‚Äô after 1200 steps, halting further model updates. 8",
  "0 250 500 750 1000 1250 1500 1750 2000 0.5 0.6 0.7 0.8 0.9 1.0 Rewards FP16 GRPO FP16 GRPO-TIS FP16 GRPO-Seq-MIS FP16 GSPO FP16 PG-Seq-IS 0 250 500 750 1000 1250 1500 1750 2000 3000 3500 4000 4500 5000 5500 6000 Response Length 0 250 500 750 1000 1250 1500 1750 2000 0.225 0.250 0.275 0.300 0.325 0.350 0.375 0.400 AIME 2024 0 250 500 750 1000 1250 1500 1750 2000 0.20 0.22 0.24 0.26 0.28 0.30 AIME 2025 Figure 4: Comparisons between various algorithms based on FP16. our FP16 approach. It achieves a maximum training accuracy of only 95% (vs. 99% in FP16) and a score of 34% (vs. 39% in FP16) on the AIME 2024 benchmark, demonstrating a clear performance ceiling. More evidence on deployment gap can be found in Figures 1 and 6. The Efficacy of FP16 Precision In contrast to these algorithmic approaches, simply switching both training and inference precision from BF16 to FP16 provides a dramatic improvement. As shown in Figures 1 and 6, the FP16 training runs are significantly more stable, converge much faster, and achieve substantially higher final rewards and evaluation scores across all tested algorithms. This result demonstrates that addressing the mismatch at the precision level is a more direct and effective solution than applying unstable or inefficient algorithmic corrections. The most surprising finding is that FP16 precision fundamentally improves the behavior of importance sampling. The sequence-level ratio, which is notoriously high-variance, becomes much more concentrated and stable in FP16 (see Figure 2). This stabilization makes it practical to use the classic, unbiased policy gradient estimator without any modifications (Equation (5)). As shown in Figure 3, this simple, unbiased approach, when powered by FP16, dramatically outperforms all existing algorithmic corrections in BF16. Training Dynamics Our experimental results reveal an interesting phenomenon: algorithms that eventually collapse consistently exhibit a growing training-inference mismatch beforehand, making it a potential early-warning signal (see Figure 3). During this period, the policy difference œÄ(¬∑|Œ∏‚Ä≤) ‚àí ¬µ(¬∑|Œ∏‚Ä≤) also converges to extreme values, where one policy‚Äôs probability approaches 1 while the other‚Äôs approaches 0, despite using the same copy of weights. We suspect this is driven by a particular optimization bias, though further validation is required. In contrast, stable algorithms maintain a bounded mismatch. Crucially, FP16 training shows a much lower mismatch level than any BF16 method. This inherent stability at the precision level explains why a simple policy gradient with FP16 can outperform all existing, more sophisticated solutions. Framework-Specific Differences While our core conclusions hold across both the VeRL [Sheng et al., 2024] and Oat [Liu et al., 2025b] frameworks, we observed subtle implementation-dependent differences. Initially, the training-inference mismatch is slightly smaller in Oat than in VeRL; for example, the initial policy difference œÄ(¬∑|Œ∏‚Ä≤) ‚àí¬µ(¬∑|Œ∏‚Ä≤) has a minimum near -0.9 in Oat versus -1.0 in VeRL. Even under FP16, where both frameworks exhibit a small mismatch, VeRL was more prone to occasional numerical spikes. These subtle stability differences, which we attribute to their different distributed backends (DeepSpeed ZeRO vs. PyTorch FSDP), likely explain why Oat yields slightly higher training rewards,",
  "in Oat versus -1.0 in VeRL. Even under FP16, where both frameworks exhibit a small mismatch, VeRL was more prone to occasional numerical spikes. These subtle stability differences, which we attribute to their different distributed backends (DeepSpeed ZeRO vs. PyTorch FSDP), likely explain why Oat yields slightly higher training rewards, particularly for the algorithms that eventually collapse. 4.3 Reviewing RL Algorithms under FP16 We then reviewed the performance of various RL algorithms when trained with FP16 precision. As shown in Figure 4, the performance differences between algorithms become almost indistinguishable. We attribute this convergence in performance to the significantly reduced training-inference mismatch in FP16, which effectively transforms the optimization problem into a nearly on-policy setting. In this state, the complex corrections offered by different algorithms provide little to no additional benefit. We did observe a minor exception where the original GRPO scored slightly lower on the AIME 2024 benchmark; however, it also scored slightly higher on AIME 2025, making it difficult to draw a definitive conclusion about its relative performance. 9",
  "0 500 1000 1500 2000 2500 0.5 0.6 0.7 0.8 0.9 1.0 Rewards fp32vllm-bf16fsdp fp16vllm-bf16fsdp fp16vllm-fp16fsdp bf16vllm-bf16fsdp 0 500 1000 1500 2000 2500 0.20 0.23 0.25 0.28 0.30 0.33 0.35 0.38 0.40 AIME 2024 0 500 1000 1500 2000 2500 0 50 100 150 200 250 300 350 Rollout Time 0 500 1000 1500 2000 2500 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Max&Min of - No mismatch ( = ) Figure 5: Ablation on the precision combinations. 4.4 Ablation on the Precision To isolate the effects of training and inference precision, we conducted an ablation study on the VeRL framework, using vLLM [Kwon et al., 2023] for inference and PyTorch FSDP [Zhao et al., 2023] for training. The results are presented in Figure 5. When training with BF16 precision, we found that increasing the inference precision consistently prolonged training stability and improved performance. Notably, when paired with FP32 inference, the training run became fully stable with no signs of collapse. However, this stability came at an immense cost: FP32 inference was nearly three times slower than FP16 or BF16 inference, making this combination impractical for large-scale experiments. In contrast, using FP16 for both training and inference yielded the best results. This combination not only produced the lowest training-inference mismatch but also resulted in the most stable training dynamics. It successfully reached nearly 100% training accuracy on the perfectible dataset without any loss of speed, demonstrating a clear superiority in both stability and efficiency. 5 Generalization Across Models, Data, and Training Regimes In Section 4, we scrutinized various algorithmic fixes under the sanity-check setting and found that simply switching from BF16 to FP16 can substantially improve training stability (Section 4.2), with its effect often overshadowing algorithmic tweaks (Section 4.3). In this section, we move beyond the sanity-check setting and validate our findings across more diverse scenarios, including Mixture-of-Experts (MoE) RL, Low-Rank Adaptation (LoRA) RL, and RL on larger prompt sets and alternative model families. 5.1 MoE RL Mixture-of-Experts (MoE) reinforcement learning (RL) training is known for its instability and often requires sophisticated stabilization strategies [Zheng et al., 2025]. Both training and inference of MoE models typically involve distinct parallelization strategies and precision-sensitive operations such as top-k expert selection, which further complicate the situation and usually lead to a larger training‚Äìinference mismatch compared to dense models. Given the widespread adoption of MoE architectures in modern LLMs, we conduct RL experiments on MoE models using Qwen3-30B-A3B- Base. We evaluate three different algorithms: GRPO-Seq-MIS, GRPO-Token-TIS, and PG-Seq-TIS, with detailed experimental settings provided in Section A.1. Experiments using FP16 show greater stability and consistently higher training accuracies (see (i), (j), and (k) in Figure 1) as well as higher validation rewards (see (i), (j), and (k) in Figure 6). The improvement is consistent across all three algorithms, indicating that adopting FP16 effectively mitigates the training‚Äìinference mismatch and enhances overall performance. 5.2 LoRA RL LoRA [Hu et al., 2022] has recently regained popularity in LLM RL [Wang et al., 2025a, Schulman and Lab, 2025] due to its efficiency and performance comparable to full fine-tuning. To",
  "all three algorithms, indicating that adopting FP16 effectively mitigates the training‚Äìinference mismatch and enhances overall performance. 5.2 LoRA RL LoRA [Hu et al., 2022] has recently regained popularity in LLM RL [Wang et al., 2025a, Schulman and Lab, 2025] due to its efficiency and performance comparable to full fine-tuning. To examine how LoRA-based RL is affected by numeric precision, we train Qwen2.5-Math-1.5B models on the standard MATH dataset using GRPO-Token-TIS (Equation (9)). LoRA is applied to all layers with a 10",
  "rank of 32 and scaling factor Œ± = 64. Following Schulman and Lab [2025], we adopt a slightly larger learning rate (4 √ó 10‚àí5) than that used in full fine-tuning. As shown in Figure 1 (h), BF16-based LoRA training collapses after roughly 600 steps, whereas FP16 maintains stable training throughout. 5.3 RL on Large Dense Models Large-scale parameters are typically required in modern LLMs, yielding significantly better perfor- mance compared to smaller models. This motivates us to conduct RL experiments on large dense models. Specifically, we experiment with Qwen3-14B-Base and follow the algorithm of DAPO [Yu et al., 2025]. Refer to Section A.1 for details of experimental settings. As shown in Figure 1 (l), the training rewards with FP16 increase much faster than those with BF16. Figure 6 (l) demonstrates that FP16 achieves higher validation accuracy on AIME 2024. These results suggest that using FP16 instead of BF16 effectively mitigates the training‚Äìinference mismatch in large models, highlighting the potential of this approach for scaling RL training on large models. 5.4 RL on Other Model Families The base models, which serve as the initial policies for RL, can substantially influence the learning dynamics, as they determine not only the scope of exploration but also the numerical range and sensitivity of network parameters and activations. To strengthen our experimental conclusions, we extend our study beyond Qwen-based models and train OctoThinker-3B [Wang et al., 2025b], a model mid-trained from Llama3.2-3B [Grattafiori et al., 2024] on reasoning-intensive data using GRPO. As shown in Figure 1 (g), BF16 training destabilizes after around 150 steps due to numerical mismatch, while FP16 continues to train smoothly without collapse. 6 Discussions Rethinking the Precision Tradeoff in RL Fine-Tuning Numerical precision is a foundational choice in the LLM training stack, yet this choice has long been dominated by BF16 for both pre- training and post-training, prized for its wide dynamic range and ease of use. Our results, however, suggest this default deserves careful rethinking for RL fine-tuning. In this phase, the training-inference mismatch becomes a critical source of instability, and BF16‚Äôs low precision exacerbates this problem. We demonstrate that by simply trading BF16‚Äôs wide dynamic range for FP16‚Äôs higher precision, one can achieve significantly more stable RL training, faster convergence, and superior final performance. It is important to note that we are not claiming FP16 is a universally optimal choice. The pursuit of efficiency may lead developer to even lower precisions like FP8. Furthermore, using FP16 for extremely large models might present engineering challenges related to its limited range, such as managing potential overflows. However, we believe these are solvable challenges, as evidenced by the recent successes in large-scale FP8 training. Ultimately, we hope this work inspires the community to reconsider FP16 as a powerful and often more suitable alternative for stabilizing RL fine-tuning. The Bias-Variance Tradeoff under BF16 Precision Our results in Section 4.2 reveal a bias- variance trade-off among RL algorithms operating under BF16 precision. Methods with lower variance but higher bias (like GRPO, token-level TIS, and GSPO) initially converge quickly but prove unstable and eventually collapse. Conversely, less",
  "stabilizing RL fine-tuning. The Bias-Variance Tradeoff under BF16 Precision Our results in Section 4.2 reveal a bias- variance trade-off among RL algorithms operating under BF16 precision. Methods with lower variance but higher bias (like GRPO, token-level TIS, and GSPO) initially converge quickly but prove unstable and eventually collapse. Conversely, less biased algorithms that more accurately correct for the policy mismatch (like PG-Seq-IS and GRPO-Seq-MIS) achieve stability but at the cost of high variance, which slows their convergence. This trade-off, however, becomes far less critical under FP16 precision. By fundamentally reducing the training-inference mismatch, FP16 naturally lowers both the bias induced by the mismatch and the variance of the importance sampling corrections. This enhanced stability allows even the most naive policy gradient estimator to converge efficiently, creating a training dynamic where all tested algorithms perform well and the tension between stability and speed is effectively resolved. 11",
  "7 Conclusion This work demonstrates that the training-inference mismatch, a major source of instability in RL fine-tuning, is fundamentally a problem of numerical precision. While existing algorithmic fixes are often complex and inefficient, we show that simply switching from the standard BF16 format to the higher-precision FP16 format can virtually eliminate the mismatch. This single, efficient change leads to more stable training, faster convergence, and superior performance, proving that addressing the problem at the precision level is a more effective strategy. We conclude that FP16 should be reconsidered as a foundational option for robust RL fine-tuning of LLM. References Arash Ahmadian, Chris Cremer, Matthias Gall√©, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet √úst√ºn, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jian- shu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Revisiting reinforcement learning for llm reasoning from a cross-domain perspective, 2025. URL https://arxiv.org/abs/2506.14965. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc‚Äôaurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in neural information processing systems, 25, 2012. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages 1407‚Äì1416. PMLR, 2018. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Horace He. Defeating nondeterminism in llm inference. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20250910. https://thinkingmachines.ai/blog/defeating-nondeterminism- in-llm-inference/. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner series. https://capricious-hydrogen-41c. notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025. Notion Blog. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi",
  "Yahui Zhou. Skywork open reasoner series. https://capricious-hydrogen-41c. notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025. Notion Blog. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 12",
  "Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get a baseline for free!, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611‚Äì626, 2023. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Jiacai Liu, Yingru Li, Yuqian Fu, Jiawei Wang, Qian Liu, and Yu Shen. When speed kills stability: Demystifying rl collapse from the inference-training mismatch, 2025a. https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from- the-Inference-Training-Mismatch-271211a558b7808d8b12d403fd15edda. Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, and Min Lin. Oat: A research-friendly framework for llm online alignment. https://github.com/sail-sg/oat, 2025b. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783, 2025c. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpass- ing o1-preview with a 1.5b model by scaling rl. https://github.com/agentica-project/ deepscaler, 2025. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023. Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438, 2025. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza- tions enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 3505‚Äì3506, 2020. John Schulman and Thinking Machines Lab. Lora without regret. Thinking Machines Lab: Connec- tionism, 2025. doi: 10.64434/tml.20250929. https://thinkingmachines.ai/blog/lora/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 13",
  "Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 13",
  "Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan- zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, and Jun Zhou. Every attention matters: An efficient hybrid architecture for long-context reasoning. arXiv preprint arXiv:2510.19338, 2025a. Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, et al. Every step evolves: Scaling reinforcement learning for trillion-scale thinking model. arXiv preprint arXiv:2510.18855, 2025b. Shangshang Wang, Julian Asilis, √ñmer Faruk Akg√ºl, Enes Burak Bilgin, Ollie Liu, and Willie Neiswanger. Tina: Tiny reasoning models via lora. arXiv preprint arXiv:2504.15777, 2025a. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025b. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229‚Äì256, 1992. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. https://fengyao.notion.site/off-policy-rl. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl- zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 14",
  "A Detailed Experimental Settings A.1 MoE RL As for experiments of MoE RL, we use Qwen3-30B-A3B-Base as the base model. The training data comes from DAPO-Math-17k [Yu et al., 2025], and we conduct online evaluation on AIME 2024 using the avg@32 metric. The training is performed with the VeRL framework [Sheng et al., 2024], and the key hyperparameters are summarized in Table 3. Dr.GRPO [Liu et al., 2025c] proposes using a constant normalizer instead of a token-count-based normalizer. Notably, the open-source VeRL implementation does not correctly implement this. We refer to our corrected version as ‚Äúseq-mean-token-sum-norm‚Äù for actor.loss_agg_mode in VeRL. A.2 RL on Large Dense Models For experiments on large dense models, we use Qwen2.5-14B-Base as our base model. The training data is sourced from the mathematical domain dataset curated by Cheng et al. [2025]. They aggregated recent math reasoning collections including OR1 [He et al., 2025], DAPO [Yu et al., 2025], and DeepScaler [Luo et al., 2025], and then performed deduplication and filtering to derive a final collection of 54.4k math training samples. We conduct online evaluation on AIME 2024 using the avg@8 metric. The training algorithms and hyperparameters follow the setup described in Yu et al. [2025], as summarized in Table 3. Table 3: Hyperparameters used for RL training of MoE models and large dense models. Parameter MoE RL Large dense RL trainer.nnodes 8 8 trainer.n_gpu_per_node 8 8 model.path Qwen3-30B-A3B-Base Qwen3-14B-Base vllm_version 0.10.0 0.10.0 data.train_batch_size 512 512 data.gen_batch_size N/A 1536 data.max_prompt_length 2048 2048 data.max_response_length 20480 20480 rollout.n 16 16 rollout.temperature 1.0 1.0 rollout.top_p 1.0 1.0 val_kwargs.temperature 0.6 1.0 val_kwargs.top_p 1.0 0.7 actor.ppo_mini_batch_size 32 32 actor.ppo_max_token_len_per_gpu 22528 22528 optim.lr 1e-6 1e-6 optim.lr_warmup_steps N/A 10 optim.weight_decay 0.0 0.1 optim.betas [0.9, 0.95] [0.9, 0.999] optim.eps 1e-15 1e-8 algorithm.use_kl_in_reward False False actor.use_kl_loss False False actor.clip_ratio_high 0.28 0.28 actor.clip_ratio_low 0.2 0.2 actor.clip_ratio_c N/A 10.0 C in Equations (6) and (7) 3.0 N/A actor.loss_agg_mode seq-mean-token-sum-norm token-mean overlong_buffer.enable False True overlong_buffer.len N/A 4096 overlong_buffer.penalty_factor N/A 1.0 filter_groups.enable False True filter_groups.metric N/A acc filter_groups.max_num_gen_batches N/A 10 15",
  "B More Experimental Results 0 250 500 750 1000 1250 1500 1750 2000 Training Steps 0.20 0.25 0.30 0.35 0.40 (a) Sanity GRPO BF16 FP16 0 500 1000 1500 2000 2500 Training Steps 0.20 0.25 0.30 0.35 0.40 (b) Sanity GRPO-Token-TIS BF16 FP16 0 500 1000 1500 2000 2500 Training Steps 0.20 0.25 0.30 0.35 0.40 (c) Sanity GRPO-Seq-MIS BF16 FP16 0 500 1000 1500 2000 Training Steps 0.20 0.25 0.30 0.35 0.40 (d) Sanity GSPO BF16 FP16 0 500 1000 1500 2000 Training Steps 0.20 0.25 0.30 0.35 0.40 (e) Sanity PG-Seq-IS BF16 FP16 0 500 1000 1500 2000 2500 Training Steps 0.20 0.25 0.30 0.35 0.40 (f) Sanity PG-Seq-MIS BF16 FP16 0 200 400 600 800 1000 Training Steps 0.0 0.1 0.2 0.3 0.4 0.5 (g) OctoThinker GRPO BF16 FP16 0 200 400 600 800 1000 1200 1400 Training Steps 0.2 0.3 0.4 0.5 0.6 0.7 0.8 (h) Lora GRPO-Token-TIS BF16 FP16 0 25 50 75 100 125 150 Training Steps 0.00 0.10 0.20 0.30 0.40 (i) MoE GRPO-Seq-MIS BF16 FP16 0 50 100 150 200 Training Steps 0.00 0.10 0.20 0.30 0.40 0.50 (j) MoE GRPO-Token-TIS BF16 FP16 0 25 50 75 100 125 150 175 Training Steps 0.00 0.10 0.20 0.30 0.40 0.50 (k) MoE PG-Seq-TIS BF16 FP16 0 20 40 60 80 Training Steps 0.20 0.30 0.40 0.50 (l) Dense-14B DAPO BF16 FP16 Figure 6: Evaluation comparisons between BF16 and FP16 across various frameworks, algorithms, datasets and training regimes. While Figure 1 presents the training reward curves under different precisions, Figure 6 shows evaluation results using checkpoints trained with these precisions. The results indicate that FP16- trained models generalize well to unseen benchmarks, further supporting our claim. 16",
  "Remote Labor Index: Measuring AI Automation of Remote Work Mantas Mazeika‚àó1, Alice Gatti‚àó1, Cristina Menghini‚àó‚Ä†, Udari Madhushani Sehwag‚àó2, Shivam Singhal‚àó‚Ä†, Yury Orlovskiy‚àó1 Steven Basart1, Manasi Sharma2, Denis Peskoff2, Elaine Lau2, Jaehyuk Lim1, Lachlan Carroll1, Alice Blair1, Vinaya Sivakumar1, Sumana Basu2, Brad Kenstler2, Yuntao Ma‚Ä†, Julian Michael‚Ä†, Xiaoke Li1, Oliver Ingebretsen1, Aditya Mehta1, Jean Mottola1, John Teichmann‚Ä°, Kevin Yu‚Ä°, Zaina Shaik‚Ä°, Adam Khoja1, Richard Ren1, Jason Hausenloy1, Long Phan1, Ye Htet2, Ankit Aich2, Tahseen Rabbani2, Vivswan Shah‚Ä†, Andriy Novykov1, Felix Binder‚Ä† Kirill Chugunov2, Luis Ramirez2, Matias Geralnik2, Hern√°n Mesura2, Dean Lee‚Ä†, Ed-Yeremai Hernandez Cardona2, Annette Diamond‚Ä† Summer Yue‚àó‚àó‚Ä†, Alexandr Wang‚àó‚àó‚Ä†, Bing Liu‚àó‚àó2, Ernesto Hernandez‚àó‚àó2, Dan Hendrycks‚àó‚àó1 1Center for AI Safety 2Scale AI Abstract AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation. 1 Introduction The potential for AI to automate human labor is a subject of profound societal interest and concern. As AI capabilities advance, understanding their impact on the workforce becomes increasingly urgent. However, we lack standardized, empirical methods for monitoring the trajectory of AI automation. Without reliable metrics grounded in real-world economic activity, stakeholders may struggle to build consensus and proactively navigate AI-driven labor automation. While AI systems have demonstrated rapid progress on a variety of benchmarks, it remains unclear how these gains translate into the capacity to perform economically valuable work. Many existing AI agent benchmarks measure performance on specialized skills such as software engineering [13, 18, 26] and basic computer use [34, 7, 14, 17, 32], while some focus on simple tasks shared across several professions [23]. These provide valuable signals of capabilities in isolation, yet they often do not capture the vast diversity and complexity inherent in the broader landscape of remote work. Consequently, performance on these benchmarks offers limited insight into the trajectory of human labor automation. ‚àóEqual contribution ‚àó‚àóSenior authors ‚Ä†Work done while at Scale AI ‚Ä°Work done while at CAIS arXiv:2510.26787v1 [cs.LG] 30 Oct 2025",
  "Project Brief Build an interactive dashboard for exploring data from the World Happiness Report. Files Create a 2D animated video advertising the offerings of a tree services company. Build a brewing-themed version of the ‚ÄúWatermelon Game‚Äù, where players merge falling objects to reach the highest-level item Features: Files Case Back Front Top Battery Files Create 3D animations to showcase the features of a new earbuds design and case. VoiceOver.wav WAV Develop architectural plans and a 3D model for a container home based on an existing PDF design Human Deliverable: Data Visualization Animated Video Game Development 3D Product Render Architecture Example Projects from RLI Project Brief Human Deliverable: Project Brief Files Human Deliverable: Human Deliverable: Project Brief Human Deliverable: Project Brief ‚Ä¢ Physics-based interaction ‚Ä¢ Use the provided objects ‚Ä¢ Minimalist UI ‚Ä¢ Relaxing background music ‚Ä¢ <5 MB total Requirements: ‚Ä¢ Use provided data ‚Ä¢ Overview map ‚Ä¢ Detailed score breakdown Features: ‚Ä¢ Silicone tips ‚Ä¢ Replaceable battery ‚Ä¢ Sleek charging case Files Format a paper using the provided figures and equations for an IEEE conference. Scientific Document Preparation Project Brief Human Deliverable: Requirements: ‚Ä¢ Use provided voiceover file. ‚Ä¢ Flat design; no subtitles Figure 1: The Remote Labor Index (RLI) represents a broad range of projects from across the remote labor economy, including game development, product design, architecture, and data analysis. All projects represent real work that was performed by human professionals. We introduce the Remote Labor Index (RLI) to provide the first standardized, empirical measurement of AI‚Äôs capability to automate remote work. RLI is designed to evaluate AI agents on their ability to complete real-world, economically valuable work, spanning the large share of the economy that consists of computer-based work. RLI is composed of entire projects sourced directly from online freelance platforms, reflecting the diverse demands of the remote labor market. These projects exhibit significantly higher complexity than tasks found in existing agent benchmarks. Crucially, by sourcing the majority of projects from freelancing platforms, RLI is grounded in actual economic transactions, encompassing the original work brief and the gold-standard deliverable produced by a human freelancer. This structure allows for a direct assessment of whether AI agents can produce economically valuable work. We evaluate several frontier AI agent frameworks on RLI, utilizing a rigorous manual evaluation process to compare AI outputs against the human gold standard. The results indicate that performance on the benchmark is currently near the floor. The best-performing current AI agents achieve an automation rate of 2.5%, failing to complete most projects at a level that would be accepted as commissioned work in a realistic freelancing environment. This demonstrates that despite rapid 2",
  "100 Current AI Agents Obtain Low Automation Rates Gemini 2.5 Pro ChatGPT agent GPT-5 Sonnet 4.5 Grok 4 Manus 0 2 4 6 8 10 Automation Rate (%) Figure 2: All AI agents tested automate at most 2.5% of tasks on RLI, showing that most economically valuable remote work currently remains far beyond their capabilities. progress on knowledge and reasoning benchmarks, contemporary AI systems are far from capable of autonomously performing the diverse demands of remote labor. To detect more granular shifts in performance, we employ an Elo-based pairwise comparison system. While all models fall well short of the aggregate human baseline, we observe that models are steadily approaching higher automation rates across projects. By introducing RLI, we aim to ground discussions of AI automation in empirical evidence and provide a common basis for understanding AI automation capabilities on economically valuable projects. We hope this provides an empirical foundation for researchers, policymakers, and the public to navigate the onset of AI automation of remote labor. 2 Related Work Evaluating AI agents. The potential impact of AI automation on the global economy and labor mar- kets has been the subject of significant economic analysis [3, 1]. Complementing this macroeconomic perspective, the machine learning community has increasingly focused on empirically measuring AI‚Äôs capacity to perform economically valuable work. The scope of benchmarks evaluating AI systems on valuable work has expanded considerably over time. Efforts have broadened from evaluating closed-ended academic knowledge [25, 10, 27, 11] to include agentic tasks that require interaction with dynamic environments. This shift encompasses autonomous computer use [32, 17, 16], web browsing [34, 7, 14], and realistic API calls [33]. Benchmarking real-world value. Knowledge benchmarks at the limits of human skill are becoming saturated, and current agent benchmarks often rely on simplified environments, representing only a small fraction of the remote work economy. There have been a number of domain-specific benchmarks measuring specific kinds of work, including software engineering [13, 18, 26], ML engineering [5, 30, 8, 28], and others [24, 29]. Most similar to our work, Patwardhan et al. [23] show AI models are near human parity on specific kinds of tasks shared across a wide range of professions, such as writing, web search, and administrative tasks. This indicates that current AIs have significant potential for augmentation but does not enable measuring the capacity for end-to-end project automation. In contrast to prior benchmarks, RLI measures the automation ability of AI agents on end-to-end projects sourced from real-world work in remote labor markets, thereby grounding the evaluation in actual economic transactions. Hendrycks et al. [12] measure general human-level cognitive ability 3",
  "representing well-educated individuals, whereas RLI targets automation capacity relative to the remote work economy, which is an aggregate of diverse human specializations and skills. 3 Remote Labor Index We introduce the Remote Labor Index (RLI), a new benchmark composed of end-to-end remote freelance projects for evaluating AI agents on practical, economically valuable work. Our data is sourced directly from professionals on freelance platforms, grounding the benchmark in economic value and capturing the diversity and complexity of real remote labor markets. The final dataset comprises 240 projects. 3.1 Dataset Description Here, we describe the contents of RLI projects and high-level statistics of the data. More details on these topics are available in Appendix C. Project composition. Each project in RLI consists of three components: ‚Ä¢ Brief: A text document describing the work to be done ‚Ä¢ Input files: A directory containing files needed to complete the project ‚Ä¢ Human deliverable: A gold-standard deliverable that successfully completes the project, produced by a professional These components are visualized for a sample of projects in Figure 1. For each project, the brief and input files are provided by the professional who produced the human deliverable. This ensures the brief and input files contain sufficient information to complete the project. For each project, we also record the time and cost to produce the gold-standard human deliverable, as reported by the professional who carried out the work. Other 31% Product Design 6% Architecture 7% Audio 10% Game Dev 10% Graphic Design 11% CAD 12% Video 13% RLI Project Categories Figure 3: RLI captures a wide array of project types, spanning 23 categories of work from the Upwork taxonomy. Here, we show the top seven categories. Coverage of types of work. RLI is diverse along two axes central to real knowledge work: (i) the range of jobs represented (measured by the Upwork taxonomy) and (ii) the file formats of the artifacts required to complete them. The Upwork taxonomy is well-suited for end-to-end remote freelance labor. In preliminary analysis, we found that the O*NET taxonomy, while valu- able for long-term occupations, was less tailored to the remote labor markets represented in RLI (see Appendix C.1). Following the collection and review process detailed in Section 3.2, our final dataset covers 23 categories of work out of Upwork‚Äôs 64. These categories are reported in Appendix C.1. In addition, the input files and deliverables in RLI span a wide variety of file types (Figure 14), substantially more than previous comparable benchmarks. A useful lens on project composition is the dis- tinction between software/research/writing tasks and the wider landscape of remote labor. Prior agent benchmarks tend to emphasize the former, where today‚Äôs models already perform relatively well. As Figure 6 shows, however, real freelance remote labor is far less concentrated in these activities. RLI is designed for this broader reality: it includes substantial coverage of design, operations, marketing, administration, data/BI, audio‚Äìvideo production, and other categories, sampling across task complexity and deliverable types to reflect end-to-end freelance remote labor. Difficulty and economic value. Finally, we report the effort required to produce the gold-standard human deliverables.",
  "is designed for this broader reality: it includes substantial coverage of design, operations, marketing, administration, data/BI, audio‚Äìvideo production, and other categories, sampling across task complexity and deliverable types to reflect end-to-end freelance remote labor. Difficulty and economic value. Finally, we report the effort required to produce the gold-standard human deliverables. As shown in Figure 6, the completion time for RLI projects exceeds previous 4",
  "101 102 103 104 Cost ($) 0 10 20 30 40 Frequency Min: 9.0 Mean: 632.6 Median: 200.0 Max: 22500.0 Project Costs 100 101 102 Time (hours) 0 5 10 15 20 25 30 35 Frequency Min: 0.2 Mean: 28.9 Median: 11.5 Max: 450.0 Project Completion Times Figure 4: RLI spans a broad range of difficulty, with project costs reaching over $10,000 and completion times for human professionals reaching over 100 hours. All project costs and completion times come directly from human professionals who completed the projects. In total, the projects in RLI represent over 6,000 hours of real work valued at over $140,000. benchmarks by more than 2√ó, with a mean of 28.9 hours and median of 11.5 hours. This matches the completion time of a random sample of jobs on Upwork, demonstrating how RLI comes closer than previous benchmarks to capturing the true complexity of remote labor markets. The average cost of projects in RLI is $632.6 with a median of $200. Taken together, these properties yield a benchmark that is challenging and, in aggregate, more representative of contemporary remote freelance work than previous benchmarks. For more details on the dataset cost and time, see Appendix C.5 3.2 Dataset Collection Here, we describe how the data were collected, the expertise of the contributors, and the cleaning process. The full pipeline is visualized in Figure 5. Sourcing strategy and scope. Our collection methodology is bottom-up, engaging directly with human professionals who were willing and authorized to provide their past work samples for our research. This approach ensures that our projects reflect genuine market demands and complexities. We defined the scope of collection using the Upwork taxonomy. Starting from the full list of 64 categories, we filtered out categories that did not meet predefined criteria necessary for a standardized benchmark. For example, we excluded work requiring physical labor (e.g., local photography), work that requires waiting to evaluate (e.g., SEO), or work that cannot be easily evaluated in a web-based evaluation platform (e.g., back-end development). For the full set of exclusion criteria, see Section C.2. This filtering resulted in 43 eligible categories. We sourced projects in two stages: 1. Freelance Platform Sourcing: We submitted a job post for each category within the 43 eligible categories (e.g., 3D animation, Mechanical Engineering, Presentation Design; the full list is in Appendix C.4). Hired freelancers provided samples of their prior work, yielding a diverse pool of projects. In total, this yielded 207 projects. 2. Long-Tail Sourcing: Digital labor marketplaces contain a substantial long tail of work. To sample from this long tail, we hired freelancers to provide work samples from additional categories not in the Upwork taxonomy and commissioned custom work. In total, this yielded 7 projects. We also expanded beyond Upwork, identifying high-quality examples of digital work available online. For these examples, we contacted the authors to request permission to use their work in our study and to ascertain the time taken and the monetary value of their labor on the project. We only include projects where authors gave permission and provided this timing and pricing",
  "work available online. For these examples, we contacted the authors to request permission to use their work in our study and to ascertain the time taken and the monetary value of their labor on the project. We only include projects where authors gave permission and provided this timing and pricing information, yielding an additional 33 projects. Recruitment and expertise. We recruited 358 freelancers with verified Upwork accounts and specialization in the target categories. These professionals demonstrated significant experience: on average, they had 2,341 hours worked, 89 prior jobs, and $23,364 in total earnings on Upwork. From 5",
  "Upwork Taxonomy Sampling 550 tasks collected from >300 freelancers Long Tail. Other tasks from categories beyond Upwork Task Collection Cleaning Final Filter Improve task to criteria Spot check for issues 240 final tasks Ensure tasks meet our requirements We want! Figure 5: RLI projects were extensively filtered and cleaned to ensure quality. Projects were sourced primarily from the remote labor market and secondarily from deliverables representing uncommon and emerging types of remote work work. (For details, see Appendix C.) these freelancers, we collected 550 initial projects. Freelancers were paid between $15 and $200 per project (average $41) to sell us existing work samples. Review and cleaning. To ensure each project is a self-contained, reproducible benchmark instance, we conducted multiple rounds of review, cleaning and standardization (Figure 5). In each review, we carefully evaluated the brief, input materials, and deliverables for suitability. We excluded project types that failed to meet our criteria (see Appendix C.2). Examples include projects requiring human interaction or those producing deliverables in proprietary formats that could not be readily rendered for evaluation (see Section 3.4). When needed, we followed up with freelancers for clarifications or missing materials. We then normalized all accepted projects to a common schema and, in a final pass, removed additional projects that were ultimately unsuitable. Although this rigorous multi-step filtering process slightly shifted the final project distribution, the resulting benchmark remains a highly representative and challenging sample of remote knowledge work (see Figure 6). Data privacy and release. The final RLI dataset contains 240 projects. To protect PII and prevent benchmark contamination, we maintain a private test set of 230 projects used for quantitative evaluation. We release a public set of 10 projects along with the open-sourced code for the evaluation platform to enable qualitative evaluation. None of the project descriptions in RLI are searchable. For the long-tail data, some human deliverables exist online, but not in a form that can be downloaded and presented as the full deliverable. To further protect against contamination in these cases, we include a blocklist of domains. 3.3 Metrics We use the following metrics to measure performance on RLI for a given AI agent: ‚Ä¢ Automation rate: The percentage of projects for which the AI deliverable is judged by human evaluators to complete the project at least as well as the human deliverable. This measures the absolute success rate of the AI agent across RLI projects. ‚Ä¢ Elo: A score capturing the relative performance of different AI agents. For each project, a deliverable from two different AIs is presented to human evaluators, who judge which deliverable is closer to completing the project successfully. If both agents successfully complete the project, then their deliverables are compared on overall quality. A difference of 400 corresponds to 10:1 odds of winning. 6",
  "HCAST GDPval RLI Upwork 0 10 20 30 Time (hours) Average Completion Time Median HCAST GDPval RLI Upwork 0 20 40 60 80 100 Percent (%) Project Type Distribution Software Research & Writing Other Figure 6: RLI is far closer to the complexity and diversity of real freelance labor than previous comparable benchmarks. Left: The average completion time for humans on RLI projects matches the true Upwork distribution. Right: Previous benchmarks primarily focus on tasks involving software engineering or web-based research and writing, but real remote labor markets have far more diversity. ‚Ä¢ Dollars earned: The combined dollar value of the projects successfully completed by the AI agent, using the cost of the human deliverable cost(H) as the dollar value for each project. The profit earned from completing all projects would be $143, 991. ‚Ä¢ Autoflation: The percentage decrease in the cost of completing the fixed RLI project bundle when using the cheapest-possible method to complete each project (human deliverable or an AI deliverable). We compute this as 1 ‚àí P min \u0000cost(H), minj cost(AIj) \u0001 P cost(H) , where cost(H) is the cost of the human deliverable and cost(AIj) is the cost of an evaluated AI agent solving the project. In cases where the AI deliverable does not complete the project, we set cost(AIj) = ‚àû. This metric is discussed further in Appendix A.2. The automation rate and Elo metrics are fully compatible, in that automation rate equals the probability of a win or tie against the human baseline under the same standards as the Elo evaluation. This allows computing an Elo score for the human baseline. We canonicalize scores so that the human baseline Elo is fixed at 1,000. 3.4 Evaluation The deliverables in RLI are complex and span a wide range of formats. Evaluating these deliverables is itself a demanding task, often requiring on-the-job learning, complex computer use, and lengthy multimodal analysis. As this level of assessment is currently beyond the capabilities of automated evaluation systems, we rely on rigorous manual evaluation. This section details the process for generating AI deliverables, the platform used for evaluation, and the methodologies for assessing both the automation rate and Elo scores. Deliverable generation. To generate deliverables, agents are provided with the project brief and input files. We do not mandate a specific execution environment or agent architecture. However, to ensure that the resulting artifacts can be properly assessed, agents receive an evaluation compatibility prompt before beginning the project. This prompt details the capabilities of our evaluation platform and provides a comprehensive, readable list of supported file formats, guiding the agent to produce outputs that are renderable and reviewable. The specific agents used for our pre-release evaluation are described in Appendix A.3. Evaluation platform. To standardize the review process and manage the diversity of deliverable formats, we developed a specialized web-based evaluation platform (an example is shown in Appendix B.7). This platform allows evaluators to efficiently explore unstructured deliverable directories and 7",
  "of deliverable formats, we developed a specialized web-based evaluation platform (an example is shown in Appendix B.7). This platform allows evaluators to efficiently explore unstructured deliverable directories and 7",
  "Compare Justification AI Agent Output Answer Is the AI deliverable acceptable? Question Human Output V a li d a t e U n d er st a n d Input Files Modify the provided ring design to have a marquise-cut diamond Brief ACCEPT ACCEPT REJECT REJECT Figure 7: Evaluation Pipeline: For each RLI project, AI deliverables are rigorously checked against human gold-standard deliverables and the requirements in the project brief for flaws and to determine whether the AI deliverable would be accepted as work product in a realistic freelance setting. Evaluating AI deliverables is itself a highly agentic task, so automating evaluation with LLMs is not currently feasible. Thus, all evaluations are performed manually by trained workers and subject experts. Inter-annotator agreement is above 94%. natively render dozens of different file formats, facilitating a consistent evaluation experience across varied projects. The code for the evaluation platform is open-sourced. Automation rate evaluation. Our evaluation methodology centers on determining whether an AI deliverable completes the project at least as well as the human gold standard‚Äîspecifically, whether the deliverable would be accepted by a reasonable client as the commissioned work. In preliminary evaluations, we found granular per-project rubrics were often insufficient for capturing project completion. Particularly for projects with hard-to-specify aspects (e.g., design), a deliverable might technically satisfy rubric elements yet fail professional standards. Consequently, we employ a holistic evaluation approach (visualized in Figure 7), drawing from practices for reviewing complex artifacts like papers or grants. Evaluators digest the project context (brief, input files, human deliverable) and compare the human and AI deliverables, examining specific files until confident in their assessment. Given a fixed time per project, they assess the AI deliverable (the alternative) relative to the human deliverable (the reference) using the following 3-point scale, with a written justification: 1. The alternative deliverable does not satisfy the brief as well as the reference deliverable or is of significantly lower quality, such that it would not be accepted by a reasonable client as the commissioned work. 2. The alternative deliverable satisfies the brief as well as the reference deliverable and would be accepted by a reasonable client as the commissioned work. 3. Same as 2, and the alternative deliverable exceeds the reference deliverable in overall quality. The automation rate is calculated based on the percentage of projects receiving an annotation of 2 or 3. This holistic approach allows for targeted analysis, enabling evaluators to ‚Äúzoom into the deliverable‚Äù and quickly identify major issues without navigating extensive rubrics. Once trained, human evaluators can complete evaluations relatively quickly using this approach. Elo evaluation. While the automation rate measures absolute project completion against the human baseline, the Elo metric captures the relative performance between different AI agents, combining project completion with overall quality. This allows models to eventually exceed the human Elo score of 1,000. The Elo evaluation involves a pairwise comparison between two AI Deliverables (AD-1 and AD-2). We use a modified version of the evaluation platform that displays both AI deliverables, along with the human deliverable as a reference for successful completion. Evaluators assess the comparison",
  "exceed the human Elo score of 1,000. The Elo evaluation involves a pairwise comparison between two AI Deliverables (AD-1 and AD-2). We use a modified version of the evaluation platform that displays both AI deliverables, along with the human deliverable as a reference for successful completion. Evaluators assess the comparison along two dimensions using separate 3-point Likert scales: 8",
  "Model Automation Rate Manus 2.5% Grok 4 2.1% Sonnet 4.5 2.1% GPT-5 1.7% ChatGPT agent 1.3% Gemini 2.5 Pro 0.8% Table 1: Current AI agents perform near the floor on RLI, solving less than 3% of tasks in the benchmark. ‚Ä¢ Project completion: Which deliverable is closer to satisfying the brief (i.e., closer to a state where it would be accepted by a reasonable client)? (AD-1 closer / Equally close / AD-2 closer) ‚Ä¢ Overall quality: Which deliverable has higher overall quality for the project? (AD-1 higher / Same quality / AD-2 higher) To compute the Elo score, we derive a unified preference from these two dimensions. We prioritize the project completion judgment when at least one of the AI agents has failed to complete the project. If both agents have successfully completed the project, we switch to using the overall quality judgment. Evaluation standards and statistics. In all evaluations, we instruct evaluators to adopt the perspec- tive of a reasonable client to minimize subjectivity. This grounds quality assessments in the likely reception of the work in a professional context, rather than the evaluators‚Äô personal preferences. We use majority voting across three independent evaluations to determine the final judgment. For Elo evaluations, if the three evaluations are split across the 3-way Likert scale (e.g., one vote for AD-1, one for AD-2, and one for a tie), this is recorded as indifference. The evaluation process demonstrates high reliability, with an inter-annotator agreement of 94.4% for the automation rate metric. For Elo evaluations, ternary inter-annotator agreement is 56.9%, far above random chance of 33.0%. The probability of hard disagreements (one vote for AD-1 and one vote for AD-2) is 5.9%, indicating that evaluators are directionally nearly always in agreement. Evaluation times are shown in Figure 11. Evaluators were requested to take a maximum of 20 minutes for Automation Rate evaluations and 30 minutes for Elo evaluations. These times were selected based on preliminary testing and provided ample time for completing most evaluations. Evaluations took 11.4 minutes on average for Automation Rate and 17.4 minutes for Elo. We hypothesize that the automation rate inter-annotator agreement rate will fall as AI deliverables become more complex, which could be countered with more experienced evaluators and longer evaluation time. 4 Experiments We evaluate the performance of several frontier AI agents on the Remote Labor Index (RLI) to assess the current state of AI automation capabilities on diverse economically valuable projects. We detail our experimental setup (Section 4.1), present quantitative results measuring both absolute and relative performance (Section 4.2), and provide a qualitative analysis of observed failure modes and agent behaviors (Section 4.3). 4.1 Experimental Setup Models and Environments. We evaluate six state-of-the-art AI agents: ChatGPT agent [21], GPT-5 [22], Claude Sonnet 4.5 [2], Grok 4 [31], Gemini 2.5 Pro [9], and Manus [4]. For models that support computer-use, we used a computer-use scaffold developed by Scale AI. For models that do not support computer-use, we use the OpenHands scaffold, which we refer to as a command line interface (CLI) environment as opposed to a computer-use agent (CUA) environment.",
  "Pro [9], and Manus [4]. For models that support computer-use, we used a computer-use scaffold developed by Scale AI. For models that do not support computer-use, we use the OpenHands scaffold, which we refer to as a command line interface (CLI) environment as opposed to a computer-use agent (CUA) environment. For GPT-5, we evaluated both the CUA and CLI scaffolds and report the CLI scaffold in the main tables, as this outperformed the CUA scaffold for this model. A full comparison of performance across environments is available in Appendix A.1. 9",
  "Gemini 2.5 Pro GPT-5 Sonnet 4.5 ChatGPT agent Grok-4 Manus 400 600 800 1000 Elo Score Across All Projects, AI Agents Are Steadily Improving Human Baseline Figure 8: Relative performance (Elo) scores show that AI agents are making steady progress on RLI and there are meaningful differences between models, despite all models falling short of the human baseline of 1,000. Compared to the automation rate metric, Elo score provides a better measure of partial progress across all projects, including projects that are not solved yet. Scaffolding and prompting. To ensure a fair assessment of peak capabilities, we tune prompts and provide standardized tooling scaffolds. This includes equipping agents with necessary execution tools and providing clear instructions on interfacing with the evaluation platform. For comprehensive details on the experimental setup, including the full prompts used, see Appendix B. 4.2 Quantitative Results We analyze the performance of AI agents on RLI using both absolute metrics (measuring success against the human baseline) and relative metrics (measuring progress between models). The main results are summarized in Table 1. Absolute performance is near the floor. The central finding of our evaluation is that current AI agents demonstrate minimal capability to perform the economically valuable projects in RLI. We measure this capacity using the Automation Rate: the percentage of projects completed at a quality level equivalent to or exceeding the human gold standard. Across all models evaluated, absolute performance is near the floor, with the highest Automation Rate achieved being only 2.5% (Manus). Correspondingly, the metrics tracking the economic impact of automation (Dollars Earned and Autoflation) are also close to the floor. These results indicate that contemporary AI systems fail to complete the vast majority of projects at a level that would be accepted as commissioned work in a realistic freelancing environment. Despite rapid progress on other AI benchmarks, current systems remain far from capable of autonomously handling the diverse and complex demands of the remote labor market. Elo score reveals steady improvement. While absolute performance remains low, it is crucial to detect more granular signs of progress. To measure the relative performance between different models, we use pairwise comparisons to compute an Elo score that represents how close models are to completing projects along with the overall quality of their deliverables. This enables tracking improvements between models, even when they fail to fully complete most projects. We find that progress is measurable on RLI. The Elo rankings (Figure 8) indicate that models are steadily improving relative to each other, and the rankings generally reflect that newer frontier models achieve higher performance than older ones. This demonstrates that RLI is sensitive enough to detect ongoing progress in AI capabilities. 10",
  "4.3 Qualitative Findings To understand the limitations of current systems and the reasons for the low automation rates, we conducted a qualitative analysis of agent failures by clustering the written justifications provided by evaluators. This analysis reveals a variety of failure modes, ranging from general quality issues to common systematic errors. Common failure modes. Our qualitative analysis across roughly 400 evaluations shows that rejections predominantly cluster around the following primary categories of failure: 1. Technical and File Integrity Issues: Many failures were due to basic technical problems, such as producing corrupt or empty files, or delivering work in incorrect or unusable formats. 2. Incomplete or Malformed Deliverables: Agents frequently submitted incomplete work, characterized by missing components, truncated videos, or absent source assets. 3. Quality Issues: Even when agents produce a complete deliverable, the quality of the work is frequently poor and does not meet professional standards. 4. Inconsistencies: Especially when using AI generation tools, the AI work often shows inconsistencies between deliverable files. Frequency (%) Corrupted files 17.6 Incomplete 35.7 Poor quality 45.6 Inconsistencies 14.8 Table 2: Percentage of AI deliverables exhibiting issues, by category. Categories are not mutually exclusive; a deliverable may be counted in multiple categories. For each AI deliverable we assigned one or more failure categories based on issues observed dur- ing the evaluations. Table 2 reports the propor- tion of deliverables affected by each category. Representative failure modes include: videos far shorter than requested (e.g., 8 seconds rather than 8 minutes), child-like drawings using ba- sic geometric shapes, inconsistent visual ap- pearance across renderings (e.g., a house‚Äôs ap- pearance changing across different 3D views), robotic or unnatural voice-overs, digital floor plans that do not match the supplied sketches, and web games that function but whose graphics fall short of professional standards. Successful AI deliverables. Across a small subset of projects, AI deliverables were judged com- parable or better than human output. These were predominantly creative projects, especially audio and image related work, along with writing and data retrieval/web scraping. Specifically, across all models we tested, performance matched or exceeded human baselines on several audio editing, mixing and production tasks (e.g., creating bespoke sounds effects for a retro video game, separating vocals from accompaniment in a single track, merging voice-overs with intro and outro music) and on image-generation tasks (e.g., ad and logo creation). AI also performed well on report writing and on generating code for interactive data visualization. We provide examples of successful and unsuccessful AI deliverables (see Appendix C.6). Cognitive skills analysis. Hendrycks et al. [12] show that the skills and weaknesses of LLMs can be decomposed into several distinct categories, such as broad world knowledge, memory, and audiovisual abilities. We observe that many of the failures exhibited by AI agents stem from deficits in these skills. For example, many failures stem from AI agents being unable to verify the correctness of their work and fix mistakes, especially in projects requiring complex and interactive audiovisual verification, such as architecture, game development, and web development. Analogously, many of the successes of AI models lie in domains where current",
  "For example, many failures stem from AI agents being unable to verify the correctness of their work and fix mistakes, especially in projects requiring complex and interactive audiovisual verification, such as architecture, game development, and web development. Analogously, many of the successes of AI models lie in domains where current AI models‚Äô skills are more developed, such as projects where the complexity is primarily in text processing or image creation. 5 Discussion Generalization to automating new jobs. Historically, automation technologies have been task- specific: the electronic calculator automated the job of human calculators, but when these workers 11",
  "Create a self-hosted interactive dashboard that maps World Happiness Report scores on a world map with hover/click tooltips (country name and exact value) and a linked companion chart that highlights the selected country. AI Deliverable Human Deliverable Example of Successful Project Completion Inputs Project Brief Figure 9: Here we show a successful project completion from Sonnet 4.5. Simple web visualizations that only require writing code are well within the capabilities of current AI agents, but this work makes up a small slice of all remote labor. Additional examples of successes and failures are shown in Figures 16 and 17. re-trained and focused on skills that had not yet been automated, the calculator wasn‚Äôt able to automate any of these new tasks. This is because humans have general cognitive skills that calculators do not. AI differs qualitatively from other automation technologies; it is not designed merely to automate specific tasks, but is being explicitly developed to automate human intelligence itself. Indeed, current AIs are not task-specific, but rather have general cognitive skills and are already capturing a substantial fraction of human-level cognitive generality [12]. An AI that automates all current remote work without overfitting is likely to have many of the same general cognitive skills as humans, allowing it to automate new jobs as they arise [15]. In this way, AIs may prove qualitatively different from prior automation technologies. While RLI does not fully represent every part of the remote labor economy, it is a substantial step towards measuring the ability of AI to automate the remote economy in general, rather than just current tasks. Limitations. RLI excludes some types of work found commonly in the remote labor economy, including projects requiring interaction with the client (e.g. tutoring), jobs that require working on a team (e.g., project management), and other types of work that did not meet our requirements (see Appendix C.2 for the full list of requirements). While RLI is the broadest benchmark of its kind, it does not represent several types of remote work due to these constraints. Thus, an AI obtaining 100% automation rate on RLI may still underperform humans on types of work that we do not evaluate. The cost of the projects reported by human professionals reflects the cost at the time of project completion and is not adjusted for inflation. In most cases where we know the project completion date, the projects were completed in the past five years; consequently, the reported costs likely underestimate the current economic value of this work when accounting for inflation. 6 Conclusion RLI establishes an economically grounded measure of AI automation capacity, with 240 projects spanning 23 domains of digital freelance work, each anchored in demonstrated market value. Frontier AI agents perform near the floor on RLI, achieving an automation rate of less than 3%, revealing a stark gap between progress on computer use evaluations and the ability to perform real and economically valuable work. RLI aims to establish the empirical foundation stakeholders need to monitor AI capabilities, forecast labor market impacts, and proactively navigate AI-driven automation. 12",
  "of less than 3%, revealing a stark gap between progress on computer use evaluations and the ability to perform real and economically valuable work. RLI aims to establish the empirical foundation stakeholders need to monitor AI capabilities, forecast labor market impacts, and proactively navigate AI-driven automation. 12",
  "Acknowledgments We would like to thank Anders Edson, Hale Guyer and Connor Smith for providing helpful feedback throughout the drafting process. We would also like to thank Michael Jae Byun and Brian Jang for helpful discussions. References [1] Daron Acemoglu. The simple macroeconomics of ai. Economic Policy, 40(121):13‚Äì58, 2025. [2] Anthropic. Claude sonnet 4.5 system card. System card, Anthropic, September 2025. [3] Erik Brynjolfsson, Bharat Chandar, and Ruyu Chen. Canaries in the coal mine? six facts about the recent employment effects of artificial intelligence. Stanford Digital Economy Lab. Published August, 2025. [4] Butterfly Effect Pte. Ltd. Manus. https://manus.im/, 2025. [5] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [6] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [7] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36:28091‚Äì28114, 2023. [8] Nicholas Edwards, Yukyung Lee, Yujun Audrey Mao, Yulu Qin, Sebastian Schuster, and Najoung Kim. Rexbench: Can coding agents autonomously implement ai research extensions? arXiv preprint arXiv:2506.22598, 2025. [9] Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. 2025. [10] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. [11] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [12] Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alex Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, and Yoshua Bengio. A definition of agi, 2025. [13] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [14] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. 13",
  "preprint arXiv:2401.13649, 2024. 13",
  "[15] Anton Korinek and Donghyun Suh. Scenarios for the transition to agi. Technical report, National Bureau of Economic Research, 2024. [16] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [17] Gr√©goire Mialon, Cl√©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [18] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier LLMs earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. [19] National Center for O*NET Development. DWA reference ‚Äî O*NET 30.0 data dictio- nary. https://www.onetcenter.org/dictionary/30.0/excel/dwa_reference.html, 2025. [20] National Center for O*NET Development. O*NET 30.0 database. https://www.onetcenter. org/database.html, 2025. Licensed CC BY 4.0. [21] OpenAI. Chatgpt agent system card. System card, OpenAI, July 2025. [22] OpenAI. Gpt-5 system card. System card, OpenAI, August 2025. [23] Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Sim√≥n Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, et al. Gdpval: Evaluating ai model performance on real-world economically valuable tasks. arXiv preprint arXiv:2510.04374, 2025. [24] Penrose. Can LLMs do accounting? https://accounting.penrose.com/, 2025. Accessed: 2025-10-14. [25] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity‚Äôs last exam. arXiv preprint arXiv:2501.14249, 2025. [26] David Rein, Joel Becker, Amy Deng, Seraphina Nix, Chris Canal, Daniel O‚ÄôConnel, Pip Arnott, Ryan Bloom, Thomas Broadley, Katharyn Garcia, et al. Hcast: Human-calibrated autonomy software tasks. arXiv preprint arXiv:2503.17354, 2025. [27] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [28] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ai‚Äôs ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. [29] Bertie Vidgen, Abby Fennelly, Evan Pinnix, Chirag Mahapatra, Zach Richards, Austin Bridges, Calix Huang, Ben Hunsberger, Fez Zafar, Brendan Foody, et al. The ai productivity index (apex). arXiv preprint arXiv:2509.25721, 2025. [30] Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents against human experts. arXiv preprint arXiv:2411.15114, 2024. [31] xAI. Grok 4 model card. Model card, xAI, August 2025. [32] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040‚Äì52094, 2024. 14",
  "Neural Information Processing Systems, 37:52040‚Äì52094, 2024. 14",
  "[33] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. œÑ-bench: A benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. [34] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 15",
  "Model Automation Rate Manus 2.5% Grok 4 2.1% Sonnet 4.5 2.1% GPT-5 (CLI) 1.7% ChatGPT agent 1.3% GPT-5 (CUA) 0.8% Gemini 2.5 Pro 0.8% Model Elo Manus 509.9 Grok 4 468.2 ChatGPT Agent 454.3 Sonnet 4.5 441.7 GPT-5 (CLI) 436.7 GPT-5 (CUA) 431.6 Gemini 2.5 Pro 411.8 Table 3: Full automation rate and Elo results. In Appendix A.3, we describe our comparison of two agent scaffolds for GPT-5, a command-line interface (CLI) scaffold and computer-use (CUA) scaffold. In the main paper, we show GPT-5 with the CLI scaffold. Model Dollars Earned/Max Possible Manus $1,720/$143,991 Sonnet 4.5 $1,280/$143,991 GPT-5 (CLI) $1,180/$143,991 Grok 4 $858/$143,991 GPT-5 (CUA) $858/$143,991 ChatGPT agent $520/$143,991 Gemini 2.5 Pro $210/$143,991 Table 4: Current models earn a small fraction of the total cost of projects in the dataset. A Additional Results A.1 Full Results In Table 3, we show the precise Elo score and automation rate for all models, including the CLI and CUA scaffolds for GPT-5. In Table 4, we show the dollars earned for all evaluated models. Current AI agents earn a small fraction of the total cost of projects in the dataset. A.2 Autoflation In Figure 10, we show the reduction in the cost of completing the projects in RLI. Analogous to indices that track the price of bundles of goods, this lets us track deflation in the effective price of the fixed bundle of projects represented by RLI. We refer to this quantity as ‚Äúautoflation‚Äù and plot how it changes over time as new models are released. For each project, we measure the cost difference relative to the human-produced deliverable when using the lowest-cost method of achieving an acceptable deliverable. If no AI method completes the project at a lower effective cost than the human baseline, the reduction is zero for that project. Because the metric is sensitive to false positives in annotation, we audit all AI deliverables marked as successful to minimize the false-positive rate. A.3 Effect of Agent Scaffolds Our results suggest that current models are not yet able to take full advantage of computer-use environments. For instance, GPT-5 demonstrated superior performance when using a CLI-based agent compared to the Computer-Use Agent (CUA) setup. This holds for both the Elo scores (CLI: 436.7; CUA: 431.6) and the automation rates (CLI: 1.7%; CUA: 0.8%). We expect more vertical integration of model scaffolds will yield stronger performance. 16",
  "July 2025 August 2025 September 2025 October 2025 Date 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Autoflation (%) Autoflation: Cost Reduction for Completing Bundle of Projects Figure 10: Autoflation on RLI: the percentage decrease in the cost of completing the fixed RLI project bundle, using AI agents to complete projects if they successfully complete them at lower cost than humans. As AI systems achieve the same deliverables at lower effective cost, the price of this work declines. B Evaluation Details B.1 Model Details The vast majority of Manus deliverables were generated over the course of June, 2025. Some deliverables were generated in September, 2025. Our Gemini evaluations are with Gemini 2.5 Pro, not Gemini 2.5 Computer Use. We found that the latter struggled with our computer-use environment, since it was tuned to work with browser-only environments. B.2 Elo Computation Collecting preference data. Projects and model pairs are randomly sampled for comparison, using random ordering of model pairs to remove order effects. We use stratified sampling across models to ensure each model pair is compared on at least 10 projects (median 25). These are combined with the automation rate evaluations (model vs human) to obtain the final preference data. For each project that a model pair is compared on, we perform majority voting, using two independent evaluations with a third to break ties if needed. In cases where the three evaluations are ‚Äúprefer AD-1‚Äù, ‚Äúindifferent‚Äù, and ‚Äúprefer AD-2‚Äù, we code the preference as indifference on this project. In cases where the majority vote is for indifference, we code the preference as 50/50. We numerically average these preferences across all compared projects to obtain a probabilistic preference for the model pair. These probabilistic preferences make up the preference graph. Fitting Bradley-Terry utilities. Following the Chatbot Arena methodology [6], we use global Bradley-Terry fitting on sampled preference edges to compute utility scores, which we refer to as Elo scores for ease of understanding. We use 100 bootstrap samples to compute 95% confidence intervals in Figure 8. Bootstrap samples are taken over projects, followed by re-averaging preferences on the sampled projects to obtain probabilistic preferences. Normalizing scores. After computing Bradley-Terry utilities, we scale and shift the utilities so that the human baseline obtains a score of 1,000 and a difference in score of 400 corresponds to 10 : 1 odds of winning. 17",
  "0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Evaluation Time (minutes) 0 100 200 300 400 500 Frequency Min: 0.1 mins Mean: 11.4 mins Median: 11.6 mins Max: 20.4 mins Automation Rate Evaluation Time 0 5 10 15 20 25 30 35 Evaluation Time (minutes) 0 50 100 150 200 250 300 350 Frequency Min: 0.1 mins Mean: 17.4 mins Median: 16.9 mins Max: 35.6 mins Elo Score Evaluation Time Figure 11: Each evaluator was given a soft maximum of 20 minutes for model vs human evaluations and 30 minutes for model vs model evaluations (the latter requires inspecting more files and takes more time). In preliminary testing, we found this duration was adequate for nearly all projects. Total evaluation time per project is higher, as 2 to 3 evaluations were performed to obtain a majority vote. B.3 Evaluation and Generation Budgets Evaluation time budget. Evaluators were asked to spend no more than 20 minutes per project for automation rate evaluations and no more than 30 minutes per project for Elo evaluations. These times were selected based on preliminary testing and provided ample time for completing most evaluations. Elo evaluations involve inspecting two AI deliverables, and hence require more time. As shown in Figure 11, most evaluations finished in less than this amount of time, with a small number exceeding it. For current AI agents, evaluations are possible to complete relatively quickly, because AI deliverables often have glaring errors that are easy to spot. As AI deliverables become more complex and come closer to solving the projects in RLI, we expect that the time needed for evaluating each project will increase. B.4 Evaluation Instructions Evaluator training materials. Before beginning evaluations, annotators were required to review detailed instructional videos and documents covering the evaluation workflow and common pitfalls. The training emphasized three core principles for evaluation: ‚Ä¢ Reasonable Client Perspective: We instructed annotators to judge each deliverable holisti- cally from the perspective of a reasonable client commissioning the project. This approach grounds quality assessments in the likely reception of the work in a professional context, minimizing the evaluators‚Äô personal subjectivity. ‚Ä¢ Zone of Acceptable Error: The human reference deliverable establishes the baseline level and quality of work accepted by the original client. Annotators were instructed to view the reference within a zone of acceptable error - if the human deliverable contained minor flaws or was missing non-critical components, the AI deliverable was held to the same standard and not penalized for similar omissions. ‚Ä¢ Common AI Failure Modes: The training materials highlighted specific, common issues prevalent in AI-generated work. Examples included the use of rasterized image generation for projects explicitly requiring vector graphics, the inclusion of unreadable or nonsensical text in images, and a lack of spatial or visual consistency across different files within the same deliverable. The training detailed the standardized evaluation workflow: Annotators must first gain an understand- ing of the project by reading the brief and reviewing the reference deliverable. With this baseline established, they evaluate the AI deliverable(s) based on the requirements of the brief and the human",
  "files within the same deliverable. The training detailed the standardized evaluation workflow: Annotators must first gain an understand- ing of the project by reading the brief and reviewing the reference deliverable. With this baseline established, they evaluate the AI deliverable(s) based on the requirements of the brief and the human reference. Annotators were allotted time limits for evaluation (20 minutes for Human vs. Model; 30 18",
  "minutes for Model vs. Model). However, they were instructed to stop early and fail the project if they identified a critical flaw that rendered the deliverable unusable. We iterated on these evaluation instructions and audited annotator quality until achieving an inter-annotator agreement of ‚â•85% on a random subset of the projects. Our final version of the instructions achieved 94.4% inter-annotator agreement. Automation Rate Evaluation Instructions. In the Automation Rate evaluation, evaluators assess the AI deliverable (‚Äúalternative deliverable‚Äù or ‚ÄúAD‚Äù) using the human deliverable as a reference for what successful project completion looks like (‚Äúreference deliverable‚Äù or ‚ÄúRD‚Äù). After reviewing the project materials according to the trained workflow, evaluators must provide a classification based on the following 3-point scale, accompanied by a written justification: 1. The alternative deliverable does not satisfy the brief as well as the reference deliverable or is of significantly lower quality, such that it would not be accepted by a reasonable client as the commissioned work. 2. The alternative deliverable satisfies the brief as well as the reference deliverable and would be accepted by a reasonable client as the commissioned work. 3. Same as 2, and the alternative deliverable exceeds the reference deliverable in overall quality. The automation rate is calculated based on the percentage of projects receiving a rating of 2 or 3. The distinction between equal (2) and superior (3) quality is maintained to facilitate Elo computations and may help provide greater clarity into the abilities of models near human parity in the future. Elo Score Evaluation Instructions. The Elo evaluation involves a pairwise comparison between two AI deliverables (AD-1 and AD-2). The evaluation platform displays both AI deliverables, along with the human deliverable as a reference for what successful project completion looks like. Annotators assess the comparison along two dimensions using separate 3-point scales: Project completion: 1. AD-1 is closer to satisfying the brief than AD-2, meaning AD-1 is closer to a state where it would be accepted by a reasonable client as the commissioned work. 2. AD-1 is equally close to satisfying the brief as AD-2, meaning both are equally close to a state where they would be accepted by a reasonable client as the commissioned work. 3. AD-2 is closer to satisfying the brief than AD-1, meaning AD-2 is closer to a state where it would be accepted by a reasonable client as the commissioned work. Overall quality: 1. AD-1 has higher overall quality for the project than AD-2. 2. AD-1 has the same overall quality for the project as AD-2. 3. AD-2 has higher overall quality for the project than AD-1. B.5 Evaluation verification To reduce the rate of false positives, we manually audited all annotation cases where the AI deliverable was labeled as good or better than the human deliverable. We were able to audit all of those cases since there were only a small number of these annotations. To get a false negative rate, two co-authors randomly sampled a Human vs Model pair from 50 random projects and did manual evaluation on those projects. We found no false negatives (cases where",
  "able to audit all of those cases since there were only a small number of these annotations. To get a false negative rate, two co-authors randomly sampled a Human vs Model pair from 50 random projects and did manual evaluation on those projects. We found no false negatives (cases where a annotators incorrectly labeled the human deliverable to be preferable). This gives us ‚â§5.8% false negative rate with 95% confidence. B.6 Agent Setup Scaffolds. We use three types of agent scaffold: ‚Ä¢ Integrated agents (ChatGPT agent, Manus) 19",
  "10 1 100 101 Cost (USD) 0 20 40 60 80 100 120 140 Frequency Min: $0.03 Mean: $2.34 Median: $0.92 Max: $29.51 Distribution of Model Running Costs Figure 12: The average cost of generating AI deliverables was $2.34. In all cases, models stopped generating before exceeding $30 of API costs. ‚Ä¢ Computer-use environment developed by Scale AI ‚Ä¢ OpenHands (CLI-based environment) For models that support computer-use (GPT-5, Sonnet 4.5), we default to our computer-use scaffold. For models not supporting computer-use (Grok 4, Gemini 2.5 Pro), we use OpenHands. In Appendix A.3, we compare GPT-5 on both OpenHands and computer-use scaffolds. We refer to these as GPT-5 (CLI) and GPT-5 (CUA), respectively. In the main results, we report GPT-5 (CLI), as this outperformed GPT-5 (CUA). Available software and tools. To support the diverse multimedia requirements of RLI projects, we augmented the OpenHands environment and gave the models access to the following tools: gpt-image- 1 for image generation and editing, openai/tts-1 for speech generation, and veo-3.0-generate-preview for video generation. The agent scaffolding included standardized instructions for managing input and deliverable directories and explicitly directed agents to prioritize using these specialized tools over general LLM queries when the required functionality was available. Our computer use agent was deployed in remote Ubuntu VM instances via the Scrapybara API and given access to three tool categories through the Model Control Protocol (MCP): computer use tools for mouse, keyboard, and screenshot operations, file editor tools for viewing, creating, and modifying files with string replacement and line insertion capabilities, and bash tools for shell command execution with persistent session state. The agent scaffolding employed a modular action system that converted high-level tool calls into standardized operations executed by interface handlers. To generate artifacts, agents received the project brief and input files, were directed to save deliverables to designated directories, and operated within a session timeout (default: 1 hour), with execution traces capturing all actions and outputs for evaluation. Prompts. For all artifact generation setups with the exception of the computer use agent, we used the following base prompt: Read the brief attached and create only the deliverables described. Do not ask any questions. Complete the task and send a download link to the deliverables. You are done once all the deliverables are ready and the download link is sent. There may be auxiliary information necessary to complete the task that is provided in a zipped ‚Äò‚Äòinputs‚Äô‚Äô folder. If this is provided, unzip the folder first and then proceed with completing the task. Make a zip file with all the deliverables. Additionally, we provided the models with specifications regarding the file types supported by the evaluation platform (see Appendix B.7), which accommodates a comprehensive range of file formats with fallback rendering capabilities for unsupported types. For Manus and ChatGPT Agent, no further scaffolding instructions were added. 20",
  "Agent, no further scaffolding instructions were added. 20",
  "For OpenHands, we extended the base prompt with instructions directing agents to use input/output directories and provided access to specialized multimedia tools: NOTE: You can explore ‚Äô./inputs‚Äô directory for extra information and reference material to execute the task. The folder might be empty, meaning that no further information is provided. IMPORTANT: Always save your final deliverables to the ‚Äô./output‚Äô directory. This directory has been created for you. Only put the requested deliverable output files in the ‚Äô./output‚Äô folder and no other extraneous files (eg. README‚Äôs, etc.) . Each deliverable file must also have an appropriate extension (eg. .jpg, .png, .pdf, .csv, etc.). You can save your intermediate scripts or files to the ‚Äô./ auxiliary‚Äô directory but this is not required. SPECIALIZED TOOLS: The ‚Äô./tools‚Äô directory contains specialized tools you can use to complete your tasks. These include: - ‚Äôgpt-image-1‚Äô: Image generation and editing - ‚Äôopenai/tts-1‚Äô: Speech generation - ‚Äôveo-3.0-generate-preview‚Äô: Video generation You should absolutely use these tools if their functionality is needed to complete the task (instead of defaulting to general LLM query). Before using any tool, make sure to read its documentation and install any required dependencies. After execution, wait at least 300 seconds before killing the operation. For the computer use agent, we used the following prompt. Read the brief below and create only the deliverable described. Do not ask any questions. Try to work in /opt/workspace/ directory first, but if that‚Äôs not accessible, work in the current directory. If you cannot find the inputs folder or get permission errors, call the navigate_to_workspace function first, then ensure_workspace_directories if needed. If you get ‚Äò‚ÄòPermission denied‚Äô‚Äô errors when saving files, call the fix_workspace_permissions function to resolve them. Complete the task, and make sure to submit all of the deliverables. You are done once all the deliverables are ready, and you have saved all deliverables to the Deliverables folder (either /opt/workspace/Deliverables/ or ./Deliverables/ depending on what‚Äôs accessible). You are allowed to use temporary or auxiliary files, please save them in the auxiliary folder. Avoid long outputs when using bash, you can control the amount of output by using ‚Äôhead‚Äô or ‚Äôtail‚Äô when using bash. For Claude Sonnet 4.5, we further extended the computer use agent prompt above with quality verification instructions to leverage the model‚Äôs visual reasoning capabilities. Based on best-use recommendations suggested by early users of Claude Sonnet 4.5, we also implemented context management exceeding 1M tokens and included explicit instructions to verify any output code or files and to avoid excessively writing thinking traces to files. No need to write too many text file notes to the filesystem, try to keep your thoughts / reasoning / insights in your context window. Also verify that any outputs you generate (intermediate or final) are of good quality by taking screenshots of files for visual inspection and checking any code for potential errors. B.7 Evaluation Platform Details The evaluation platform is a web-based multimedia viewer and file explorer. It provides native support for viewing the following file types: ‚Ä¢ Documents: 21",
  "visual inspection and checking any code for potential errors. B.7 Evaluation Platform Details The evaluation platform is a web-based multimedia viewer and file explorer. It provides native support for viewing the following file types: ‚Ä¢ Documents: 21",
  "‚Äì Text: .txt, .json, .yml, .py, .js, .ts, .css, .java, .go, .php, .rb, .swift, .sql, .sh, and other common source code files. Any non-binary file not otherwise supported is displayed as text. ‚Äì Formatted: .md, .html, .pdf, .tex (LaTeX), and .ipynb (Jupyter Notebooks). ‚Äì Spreadsheets: .csv, .xls, .xlsx. ‚Äì Microsoft Office: .ppt, .pptx, .doc, .docx. ‚Ä¢ Media: ‚Äì Images: .jpg, .jpeg, .png, .gif, .bmp, .webp, .svg, .ico, .avif, .tif, .tiff. ‚Äì Video: .mp4, .m4v, .mkv, .webm, .mov, .avi, .wmv. ‚Äì Audio: .mp3, .wav, .ogg, .aac, .m4a, .midi, .mid. ‚Ä¢ Design & 3D: ‚Äì Design: .psd (with limited support for complex layer effects). ‚Äì 3D Models: .obj, .mtl, .stl, .gltf, .glb. ‚Äì Autodesk/CAD: .dwg, .dxf, .skp, .stp, .step, .ipt, .3dm, .3ds, .fbx, .rvt, .ifc, and other formats supported by the Autodesk Viewer. ‚Ä¢ Data & Interactive: ‚Äì Databases: .sqlite, .db. ‚Äì Websites/WebGL: Interactive builds with .html entry points and associated .js and .css assets. ‚Äì Anki: .apkg (limited to front and back card formats). The evaluation platform is fully open-source. Project-specific notes for evaluation. For some projects, we display short notes in a popup in the evaluation platform. These evaluator notes contain project-specific details of how the evaluation should be performed. For example, in some projects the human deliverable contains additional features that we exclude from the project brief. In these cases, we instruct the evaluator to ignore those parts of the human deliverable and emphasize that the AI deliverable should not include those features. Less than 20 projects have evaluator notes. C Dataset Details C.1 Categorization Upwork taxonomy. We categorize all projects using the Upwork job taxonomy. We used the version current at the time of this paper‚Äôs release, which contains 12 major categories and 64 subcategories of work. This taxonomy is detailed below. ‚Ä¢ Accounting and Consulting: Accounting & Bookkeeping, Financial Planning, Management Consulting & Analysis, Personal & Professional Coaching, Recruiting & Human Resources, Other - Accounting & Consulting ‚Ä¢ Admin Support: Data Entry & Transcription Services, Market Research & Product Reviews, Project Management, Virtual Assistance ‚Ä¢ Customer Service: Community Management & Tagging, Customer Service & Tech Support ‚Ä¢ Data Science and Analytics: AI & Machine Learning, Data Analysis & Testing, Data Extraction/ETL, Data Mining & Management ‚Ä¢ Design and Creative: Art & Illustration, Audio & Music Production, Branding & Logo Design, Graphic, Editorial & Presentation Design, NFT, AR/VR & Game Art, Performing Arts, Photography, Product Design, Video & Animation ‚Ä¢ Engineering and Architecture: 3D Modeling & CAD, Building & Landscape Architecture, Chemical Engineering, Civil & Structural Engineering, Contract Manufacturing, Electrical & Electronic Engineering, Energy & Mechanical Engineering, Interior & Trade Show Design, Physical Sciences 22",
  "Figure 13: Evaluation platform view with the ring 3D model project example. ‚Ä¢ IT and Networking: Database Management & Administration, DevOps & Solution Ar- chitecture, ERP/CRM Software, Information Security & Compliance, Network & System Administration ‚Ä¢ Legal: Corporate & Contract Law, Finance & Tax Law, International & Immigration Law, Public Law ‚Ä¢ Sales and Marketing: Digital Marketing, Lead Generation & Telemarketing, Marketing, PR & Brand Strategy ‚Ä¢ Translation: Language Tutoring & Interpretation, Translation & Localization Services ‚Ä¢ Web, Mobile, and Software Development: AI Apps & Integration, Blockchain, NFT & Cryptocurrency, Desktop Application Development, Ecommerce Development, Game Design & Development, Mobile Development, Product Management & Scrum, QA Testing, Scripts & Utilities, Web & Mobile Design, Web Development, Other - Software Develop- ment ‚Ä¢ Writing: Content Writing, Editing & Proofreading Services, Professional & Business Writing, Sales & Marketing Copywriting Our final dataset includes projects from 9 major categories and 23 subcategories. In Figure 3, we show the distribution across subcategories. For brevity, we use the following short-form names in the figure: ‚ÄúVideo‚Äù for ‚ÄúVideo & Animation‚Äù, ‚ÄúCAD‚Äù for ‚Äú3D Modeling & CAD‚Äù, ‚ÄúGraphic Design‚Äù for 23",
  "Inputs Human Deliverable 0 5 10 15 Average Files per Project Average Number of Files GDPval RLI Inputs Human Deliverable 0 20 40 60 80 Number of Unique Filetypes Total Unique Filetypes GDPval RLI Figure 14: RLI projects involve significantly more diverse file types than previous comparable benchmarks. Left: Average number of files per project for inputs and human deliverables across benchmarks. Right: Total unique file types found in inputs and human deliverables across benchmarks. ‚ÄúGraphic, Editorial & Presentation Design‚Äù, ‚ÄúGame Dev‚Äù for ‚ÄúGame Design & Development‚Äù, ‚ÄúAudio‚Äù for ‚ÄúAudio & Music Production‚Äù, and ‚ÄúArchitecture‚Äù for ‚ÄúBuilding & Landscape Architecture‚Äù. To better reflect the diversity of projects, we separate out music composition projects into their own subcategory for the figure, as music composition differs considerably from other projects in Audio & Music Production. Music composition projects make up roughly 6% of the benchmark. Further subdivisions of this nature are possible, as most subcategories in the Upwork taxonomy consist of multiple distinct types of work, but for consistency we use the unmodified Upwork taxonomy for all other discussion in the paper. Most of our analysis focuses on the subcategories in the Upwork taxonomy, so for brevity, we refer to these as ‚Äúcategories‚Äù in other parts of the paper. O*NET taxonomy. The O*NET database [20] provides a widely used taxonomy of occupational requirements and work activities within the US labor market. While valuable for capturing activities performed in long-term occupations, it is not tailored to end-to-end freelance labor markets like Upwork, making it unsuitable for classifying RLI projects and estimating coverage. This limitation stems from O*NET‚Äôs structure at both the activity and occupational levels. To categorize a broad range of work, O*NET relies on an abstract hierarchy of Work Activities. Even the most granular taxonomy in O*NET, Detailed Work Activities (DWAs), does not provide meaningful granularity for measuring task breadth. The DWA taxonomy includes many ubiquitous and generic items such as ‚ÄúRetrieve information from electronic sources,‚Äù and ‚ÄúRead materials to determine needed actions,‚Äù [19], and coverage of these DWAs does not indicate meaningful coverage of remote work task types. At the occupational level, O*NET classifications are designed to describe the broad, ongoing responsibilities of long-term workers. This structure does not align with the delivery of specific, self-contained freelance projects. For this reason, we use the Upwork taxonomy of remote freelance labor for coverage analysis, since this taxonomy is designed for categorizing freelance work. C.2 Filtering & Cleaning Criteria Project sourcing criteria. To enable building a high-quality standardized benchmark, we hired freelancers from categories on Upwork that met the following criteria: 1. Remote work: It must be possible to complete projects without any physical labor (e.g., no local photography). 2. No open-ended jobs: Most jobs in the category must be end-to-end projects that can be performed, not open-ended long-term contractor roles. 24",
  "performed, not open-ended long-term contractor roles. 24",
  "3. Can be completed independently: The work can be completed independently by one freelancer and does not inherently require working on a team. 4. Does not require interaction with client: The work does not inherently require interacting with clients (e.g., no tutoring). 5. Does not require interaction with client services: The work does not require testing or interacting with live services set up by the client (e.g., no QA testing of client websites). 6. No scraping without permission: The work does not involve scraping information from low-traffic websites or websites where bots are expressly forbidden. 7. Can be evaluated on the spot: Some categories of work inherently require time to evaluate work outputs (e.g., SEO). These categories were excluded, ensuring that all projects can be evaluated on the spot. Note: This restriction does not apply to projects where evaluations take a long time but can still be performed on the spot. 8. Excluding certain categories: Many projects in the Content Writing category can already be solved by AIs and would not provide much information to include. Thus, this category and related categories were excluded. (Note: These are category-level exclusions; individual projects from other categories were not excluded based on whether current models solved them.) Most legal categories were excluded due to PII concerns. 9. Renderability: Deliverables must be possible to view in a web-based evaluation platform (e.g., no desktop application development). Based on these criteria, we entirely excluded projects from the following categories on Upwork during our initial project collection: Personal & Professional Coaching; Recruiting & Human Resources; Project Management; Com- munity Management & Tagging; Customer Service & Tech Support; Performing Arts; Photography; International & Immigration Law; Public Law; Digital Marketing; Marketing, PR & Brand Strategy; Desktop Application Development; Mobile Development; Product Management & Scrum; QA Testing; Content Writing; Professional & Business Writing; and Sales & Marketing Copywriting. This left us with 45 total Upwork categories to source projects from. These sourcing criteria were also applied during long tail project collection. Data cleaning and filtering. After receiving raw data, we conducted an extensive process of cleaning and filtering to ensure that all projects in the dataset met the following criteria: 1. Completeness: The brief and input files are complete and sufficient, with no additional external information needed to complete the project. 2. Anonymization: The input files and deliverables do not include sensitive personal informa- tion pertaining to the client. Client faces were blurred out, and company names and logos were replaced with fake alternatives that preserve the realism of projects. 3. Human deliverable completes the project: The gold-standard human deliverable success- fully completes the project, such that a reasonable client would accept it as the commissioned work. Note: The majority of projects in RLI were paid for by clients, so this is often guaran- teed by default. 4. File quality: Input files are high-quality. E.g., if the raw data for projects sourced from freelancers includes low-resolution images or screenshots, we request higher-quality replace- ments from freelancers. 5. Faithful to the raw data: For projects sourced from freelancers, we ensure that the cleaned",
  "often guaran- teed by default. 4. File quality: Input files are high-quality. E.g., if the raw data for projects sourced from freelancers includes low-resolution images or screenshots, we request higher-quality replace- ments from freelancers. 5. Faithful to the raw data: For projects sourced from freelancers, we ensure that the cleaned projects are as faithful as possible to the raw data sent by the freelancers, using similar or identical phrasing to original client requests where possible. 6. Standardized structure: All projects are standardized to have briefs with three top-level sections: ‚ÄúWork description‚Äù describing the work to be done, ‚ÄúProvided material‚Äù describing the auxiliary project inputs, and ‚ÄúDeliverables‚Äù describing the expected deliverables. 7. Renderability: We ensure that all inputs and human deliverables are viewable in the evaluation platform. We convert unsupported formats to supported ones (e.g., AI to layered PDF) and exclude projects that cannot be supported. This often required improving the capabilities of the evaluation platform to accommodate projects with new file types. 25",
  "After the cleaning and filtering process, the dataset contains 240 projects from the following 23 Upwork subcategories: Video & Animation, 3D Modeling & CAD, Graphic & Editorial Design, Audio & Music Production, Building & Landscape Architecture, Product Design, NFT, AR/VR & Game Art, Art & Illustration, Interior & Trade Show Design, Web Development, Branding & Logo Design, Game Design & Development, Management Consulting & Analysis, Data Entry & Transcription Services, Data Analysis & Testing, Language Tutoring & Interpretation, Data Extraction/ETL, Presentation Design, Web & Mobile Design, Corporate & Contract Law, Translation & Localization Services, Market Research & Product Reviews. C.3 Analysis Details Completion time comparison. In Figure 6, we extracted completion time data from the papers for GDPval [23] and HCAST [26]. To determine the average completion time and cost for Upwork projects, we analyzed 275 completed jobs from a random sample of 60 freelancers, using the hours worked and dollars earned for each job. Project type comparison. In Figure 6, we computed the distribution over project types for RLI and GDPval by using a judge LLM to classify the project briefs using the following instructions. Classify this task into one of three categories: 1. Software engineering / coding 2. Research and writing 3. Other A task should be classified as category 1 or 2 if the actual work primarily involves these skills, such that with sufficient knowledge one could solve the task by just using these skills. Examples of category 1: - Front-end development - Game development - Website creation Examples of category 2: - Reading PDFs and writing a report - Searching for information online and writing a report - Writing a blog post about a historical event Examples of category 3: - Performing research, running simulations, and writing a report - Making an as-built drawing of a building - Creating an educational video - QA testing for a video game and writing a bug report (involves playing the game ) For HCAST, we manually classify the task distribution shown in Table 1 of the HCAST paper [26]. For an estimate of the Upwork distribution, we apply the above prompt to the category names in the Upwork taxonomy. This provides a distribution over the different types of work performed on Upwork. Note: This is not a distribution at the job-level, which is more skewed toward software tasks. C.4 Data Collection Details 1. For projects sourced from freelancers, we only included projects where freelancers explicitly verified that they had the rights to sell us the work. 2. In cases where the work contains PII or copyrighted content (e.g., logos or company names), we anonymized the project by redacting information. In some cases, redacted information was replaced with synthetic details (e.g., fake company names or logos). 26",
  "100 101 102 Completion Time (hours) 101 102 103 104 Cost ($) Correlation (log-log): 0.785 Project Cost vs Completion Time Figure 15: Project cost and completion time are highly correlated on a log-log scale. 3. For long-tail project collection, we either purchased the work or received permission from the original author of the work to link to it in our study. C.5 Project cost and completion time. Collecting cost and completion time. For the vast majority of projects, the human professionals who created the human deliverable provided the cost and completion time for the project. These metrics were operationalized as follows: ‚Ä¢ Cost: The amount of money in USD earned by the freelancer for completing the project, or a fair price estimated by the professional for recreating the work from scratch. Human professionals self-reported these values. Since these often represent the actual amount of money paid by a client, they provide an accurate measure of the cost of the project. ‚Ä¢ Completion time: The amount of time in hours that it took human professionals to complete the projects. These values were also self-reported to ensure economic accuracy. In some cases, human professionals communicated a range of times or costs; in these instances, we took the midpoint value. Costs are available for 95% of projects. Completion times are available for 84% of projects. 5% of projects have neither cost nor completion time data, but were kept in the dataset due to being high-quality. For experiments or metrics using this data, we drop projects for which the required values are not available. Distribution over project cost and completion time. In Figure 4, we show the distributions over project cost and completion time. Both variables are roughly log-normal distributed, with project cost and completion time reaching up to $22,500 and 450 hours. Individual numbers are often rounded by freelancers who self-report the data, and fixed price projects tend to cluster at whole-number values, explaining peaks in the data. In Figure 15, we plot these variables against each other on a log-log scale for projects where both values are available. We observe a Pearson correlation of 0.785. C.6 AI Deliverable Examples 27",
  "Human Deliverable AI Deliverable Create two fun, Halloween-themed Facebook ads that weave in the provided recipe images and clearly feature the copy: ‚ÄúSPOOKTACULAR SALE,‚Äù ‚Äú20% off site wide,‚Äù and ‚ÄúCoupon Code: SPOOKY20,‚Äù using playful seasonal visuals to highlight the dishes and the promotion. Inputs Example of Successful Project Completion Project Brief Figure 16: AI agents leverage image generation tools to solve some marketing projects in RLI. Here we show a successful project completion from Manus. 28",
  "Produce a ~60-second, 2D flat-design explainer educating viewers on trimming, pruning, stump removal, and tree health. Use bold typography, a natural palette, icon-driven graphics, subtle character animation, and smooth modern transitions. Pair with the supplied voiceover. Produce five short, high-quality 3D product demo animations that clearly showcase the earbuds‚Äô silicone tips, swappable battery stem, sleek charging case. The clips should be polished and visually consistent, with smooth camera moves and lighting that emphasizes materials, fit, and the replaceable battery mechanism. Examples of Unsuccessful Project Completion AI Deliverable Human Deliverable Inputs Project Brief AI Deliverable Human Deliverable Inputs Project Brief Figure 17: Agents fail to successfully complete the vast majority of RLI projects. Here we show failed projects for Gemini 2.5 Pro (top) and GPT-5 (bottom). 29",
  "C.7 Detailed Project Examples Work description Please design the following: Bathroom: 3 interior design options for the existing bathroom (wall-hung WC in the indicated location). Apartment: 6 furniture layout options; pick one ‚Äúfinal‚Äù option for detailed plans. Cadastral notation is \"room no. / gross area (meters squared)\". Rooms in cadastral plan: Rooms 27, 28, 29: habitable rooms Room 26: kitchen Room 26a: living room Room 26b: veranda Room 25: bathroom Room 24: hallway There is a door from the living room to the veranda, as shown in `inputs/additional measurements.jpg` Dimensions in deliverables are design intent; contractor to verify all on site. Provided material Cadastral floor plan (metric): `inputs/cadastral floor plan.jpg` Zoomed bathroom plan: `inputs/bathroom.jpg` Site photos: `inputs/bathroom_photos/photo_#_y.jpg` Additional measurements of the bathroom, living room, and veranda: `inputs/additional measurements.jpg` Deliverables Bathroom interior design - 3 options: Renders: At least 3 views per option, at least 1200 pixels on long edge. Include one render from the top. (JPG) Material board: one combined sheet per option showing the renders + finish swatches Wall finish images: high-res JPGs of each finish used. 3D source: supply native file (e.g., .skp/.3ds/.max/.blend) plus an interchange file (.fbx or .obj) with textures. Furniture layouts - 6 options: One PDF floor plan per option, imperial dimensions (feet- inches) for key clearances and furniture sizes. One consolidated DWG containing all options. ‚ÄúFinal\" chosen furniture option; extra plans: RCP & lighting plan: show ceiling levels, fixture symbols, mounting heights, and a legend (PDF) Toilet installation plan: horizontal dimensions in imperial units and outline the plasterboard boxing; no further details required (PDF) Electrical equipment layout: outlets, switches, appliance points, mounting heights, legend (circuiting by electrician) (PDF) Floor finishes plan: hatch/legend showing material zones and transition/threshold locations (PDF) CAD trace of cadastral plan: Provide a clean DWG + PDF. Trace to scale, align walls, doors, windows Example 1: Animated 3D Product Demonstration of Earbuds Work description We need high-quality animations to showcase the features of a new earbuds design and the case. Create high-quality 3D product demonstration videos that effectively communicates the key features and benefits of the earbuds. We need 5 short, engaging animations to be used in marketing materials. The key features are: Silicone, airpod-like tips Stem of earbud swaps out for a replaceable battery Sleek charging case L/R indicator decal Provided material Earbuds image in `inputs/earbuds_back.jpg` Earbuds image in `inputs/earbuds_front.jpg` Earbuds image in `inputs/earbuds_top.jpg` Image demonstrating replaceable battery functionality in `inputs/replaceable_battery.jpg` Image of portable charging case in `inputs/charging_case.jpg` Deliverables Five short clips showcasing the different features of the earbuds (MP4 format) 3D models for the earbuds and case (e.g., .fbx format) Input Files Deliverables Input Files Deliverables Example 2: Interior Design and Furniture Layout Figure 18: Detailed project examples with extended briefs. 30",
  "Work description Create a casual, web-based game called \"Mega Merge\" where players combine falling objects to reach the highest-level item possible. The game should be inspired by the popular Watermelon Game but incorporate unique mechanics and features. It should be designed for accessibility and smooth play on any device, with a responsive layout suitable for both desktop and mobile play. Objective Players will aim to combine objects and score as many points as possible before the box fills up. By merging identical items, players will create higher-level items and work towards unlocking the ultimate object. The goal is to manage space strategically while maximizing the score. Key Features Platform: Web-based, compatible with all major browsers (Chrome, Safari, Firefox, Edge). Cross-Platform Compatibility: Works seamlessly on desktop and mobile (iOS and Android) with responsive layouts. Controls: Supports both touch gestures (tap, swipe) and mouse clicks for flexible gameplay. Instant Playability: No downloads required; players can start immediately by opening the game in their browser. Technical Requirements Physics and Collisions: Objects should obey basic physics. They should fall naturally within a defined \"box\" and exhibit slight bounce effects when landing or colliding with each other. This behavior can be achieved with a physics engine like matter.js or through Construct 3's physics behavior. File Size: The total file size should be kept under 5 MB to ensure fast loading. Gameplay Mechanics Object Merging: Players combine matching items to generate higher-level objects, aiming to reach the ultimate item. Visual & UI Design Container Box: Objects should fall into a clearly defined \"box\" area with visible boundaries, guiding the player's actions. Falling Indicator: The next item should have an indicator at the bottom of the screen to show where it will fall, helping players plan their moves. Score Display: The score should be displayed prominently at the top of the screen. Minimalist UI: Essential elements only‚Äîscore display, \"Next Item\" preview, and basic pause/reset buttons at the top of the screen. Audio and Sound Design Background Music: The game should include relaxing background music that plays continuously during gameplay to create a calm, enjoyable atmosphere. Sound Effects: A satisfying sound effect should play when objects are dropped into the container. A distinct, gratifying merging sound should play when two objects combine. These sounds should enhance the feedback of each action, creating an engaging and satisfying player experience. Interaction and Controls Touch and Mouse Support: The game should support both touch gestures and mouse input to provide a smooth experience on both mobile and desktop platforms. Provided material None Deliverables Game Files: All files (HTML, CSS, JavaScript, images, and audio files) should be organized in a clear folder structure, with folders for assets, icons, images, scripts, styles, and sounds. Interactive Video Game for the Web; Built with Unity Create a Unity WebGL video game with planets and weapons. Polished UI, weapon glow, audio. Provide commented code, README, tested build, and simple HTML embed. Real Freelancer Deliverable: Digital Assets Unity Build Example 3: Mega-Merge Web Game Deliverables Work description Build an intuitive, self-hosted interactive dashboard that lets visitors explore why some",
  "Unity WebGL video game with planets and weapons. Polished UI, weapon glow, audio. Provide commented code, README, tested build, and simple HTML embed. Real Freelancer Deliverable: Digital Assets Unity Build Example 3: Mega-Merge Web Game Deliverables Work description Build an intuitive, self-hosted interactive dashboard that lets visitors explore why some countries score higher than others in the World Happiness Report. Requirements Overview: The dashboard should include an overview map showing each country's overall happiness score. Data: use the provided data as the sole source for country scores and component metrics. Map: display each country shaded on a gradient that reflects its overall happiness score; add hover and click interactions that surface the country name and exact value. Detailed chart: place a second visual (e.g., stacked bar or spider chart) beside or beneath the map. This chart should be linked to the map, so when the reader interacts with one country on the map, the same country in the second chart is highlighted. Design: intuitive, user-friendly, and align with the theme of happiness. Provided material Happiness data for the dashboard in `inputs/DataForFigure2.1WHR2021C2.xls`. Deliverables A complete, self-contained dashboard package (HTML, CSS, JavaScript, and any required libraries). Example 4: Interactive Dashboard for the World Happiness Index Input Files Deliverables Figure 19: Detailed project examples with extended briefs. 31",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Shengnan An‚àó‚ô¢, Xunliang Cai‚ô¢, Xuezhi Cao‚àó‚ô¢, Xiaoyu Li‚ô¢, Yehao Lin‚ô¢, Junlin Liu‚Ä†‚ô£, Xinxuan Lv‚ô¢, Dan Ma‚ô¢, Xuanlin Wang‚Ä†‚ô°, Ziwen Wang‚ô¢, Shuang Zhou‚ô¢ (Alphabetical order by last name) ‚ô¢Meituan ‚ô£University of Chinese Academy of Sciences ‚ô°Harbin Institute of Technology ABSTRACT We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO- Bench to facilitate further research into advancing the reasoning abilities of language models. Code, Dataset, and Leaderboard: amo-bench.github.io AMO-Bench(Ours) HMMT25 AIME25 AIME24 MATH500 20 30 40 50 60 70 80 90 100 Accuracy (%) Models LongCat-Flash-Thinking GLM-4.5 Qwen3-235B-A22B-Thinking-2507 Gemini-2.5-Pro DeepSeek-V3.1-Thinking GPT-5-Thinking (High) 43.6 83.7 90.6 93.3 99.2 36.8 76.3 85.5 89.3 95.4 47.8 83.8 92.5 93.9 99.6 38.7 79.3 89.2 90.7 98.0 47.6 80.4 87.9 93.9 98.8 52.4 84.8 94.6 92.0 99.2 Figure 1: Performance of top-tier reasoning models on AMO-Bench as well as existing competition-level math benchmarks. Except for the results on AMO-Bench, all other results are sourced from Meituan LongCat Team [2025a]. ‚àóCorrespondence to: {anshengnan, caoxuezhi}@meituan.com. ‚Ä† Work done during the internship at Meituan. arXiv:2510.26768v1 [cs.CL] 30 Oct 2025",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 1 Introduction Recent advances in large language models (LLMs) have demonstrated significant improvements in reasoning capabili- ties [OpenAI, 2024, Gemini Team, 2025, OpenAI, 2025, Anthropic, 2025, xAI, 2025, Yang et al., 2025, Guo et al., 2025, DeepSeek-AI, 2025, Meituan LongCat Team, 2025b, GLM-4.5 Team, 2025, ByteDance Seed, 2025, Tencent Hunyuan Team, 2025, Kimi Team, 2025, Meituan LongCat Team, 2025a]. To track this rapid progress, mathematical problem solving has become a critical metric for evaluation, as it inherently demands complex and multi-step reasoning processes to arrive at correct answers. As a result, many current benchmarks utilize problems from high school mathematics competitions (e.g., HMMT and AIME) to assess the reasoning abilities of LLMs [Balunovi¬¥c et al., 2025, He et al., 2024, Gao et al., 2024, Fang et al., 2025]. Recent results indicate that state-of-the-art models are achieving remarkable performances on these benchmarks, with some even surpassing 90% accuracy on competitions like AIME24/25. However, these impressive results also expose an emerging challenge: many existing mathematics benchmarks are approaching performance saturation and are becoming less effective for assessing further advancements in reasoning capabilities. On the one hand, as LLMs gradually approach or even surpass human-level capabilities in mathematics, some math competitions are becoming less challenging for top-tier models [OpenAI, 2025, DeepSeek-AI, 2025, Yang et al., 2025, Meituan LongCat Team, 2025a]. On the other hand, most current benchmarks are derived from previous competitions, raising concerns about potential data memorization and performance leakage [Sun et al., 2025, Balunovi¬¥c et al., 2025]. While recent efforts have incorporated problems from more difficult and newly held contests such as the International Mathematical Olympiad (IMO), these questions tend to be proof-based and require manual verification by experts [Balunovi¬¥c et al., 2025, Petrov et al., 2025]. This reliance on expert review hinders the implementation of automated scoring processes, leading to inefficiency and inconsistency in large-scale evaluations and result reproductions. To address these limitations, we present AMO-Bench, an advanced mathematical reasoning benchmark consisting of 50 novel and extremely challenging problems. The core features of AMO-Bench are as follows: ‚Ä¢ Original problems. To prevent performance leaks from existing resources as much as possible, all problems in AMO-Bench are newly crafted by human experts. Moreover, we conduct a secondary verification to ensure that there are no highly similar problems in existing competitions or online resources. ‚Ä¢ Guaranteed difficulty. Each problem has undergone rigorous cross-validation by multiple experts to ensure it meets at least the difficulty standards of IMO. We also incorporate an LLM-based difficulty filtering stage to exclude questions that do not present sufficient challenge to current reasoning models. ‚Ä¢ Final-answer based grading. Each problem in AMO-Bench requires a final answer rather than a full proof, enabling efficient automatic grading. For each problem, we employ a parser-based or LLM-based grading method according to its answer type, balancing the grading cost and generalizability. ‚Ä¢ Human-annotated reasoning paths. In addition to the final answer, each problem also includes a detailed reasoning path written by human experts. These additional annotations enhance solution transparency and could support further",
  "employ a parser-based or LLM-based grading method according to its answer type, balancing the grading cost and generalizability. ‚Ä¢ Human-annotated reasoning paths. In addition to the final answer, each problem also includes a detailed reasoning path written by human experts. These additional annotations enhance solution transparency and could support further explorations on AMO-Bench, such as prompt engineering and error analysis. Experimental results across various LLMs demonstrate that contemporary LLMs still struggle with the significant challenges presented by AMO-Bench. Among 26 evaluated models, the state-of-the-art accuracy on AMO-Bench is only 52.4%, achieved by GPT-5-Thinking (High), with most models scoring below 40%. Figure 1 illustrates the performance of several leading models on AMO-Bench as well as the comparison with other mathematical benchmarks. Beyond their limited final performances on AMO-Bench, LLMs consume substantially more output tokens in AMO-Bench compared to existing evaluation datasets. For example, GPT-5-Thinking (High) generates an average of approximately 37K output tokens for AMO-Bench, whereas it produces only about 7K and 6K tokens for AIME25 and AIME24, respectively. This exceptionally high token consumption further underscores the difficulty of AMO-Bench for current LLMs. Despite the poor performances of current LLMs, our analysis also reveals considerable potential for further improvements. Notably, top-tier models achieve pass@32 rates exceeding 70%, suggesting they possess the initial capability to solve these challenging problems even if they do not consistently identify the correct reasoning path at present. Furthermore, we show that the model performances exhibit a near-linear growth trend relative to the logarithm of output length, indicating continued benefits from test-time scaling. These analyses suggest substantial opportunities remain to enhance reasoning capabilities in future generations of language models. The data and evaluation code of AMO-Bench are publicly available at amo-bench.github.io. We hope this novel and challenging benchmark will facilitate further research into advancing the reasoning abilities of language models. 2",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Human Experts Example 1: Problem: Let ùë•1, ùë•2, ‚Ä¶, ùë•2024 be positive real numbers such that ùë•ùëò+ ùë•ùëö‚â•ùëòùëö ‚Ä¶ Solution: From the assumption: for any positive integer 1 ‚â§ùëò‚â§1012, we have ‚Ä¶ Final Answer: 1382935444 Example 2: Problem: Find all positive integers n such that for any: ùëéùëõ‚â• ùëéùëõ‚àí1 ‚â• ‚Ä¶ ‚â• ùëé2 ‚â• ùëé1 > 0 ‚Ä¶ Solution: For ùëõ= 1 , it clearly holds. For ùëõ= 2, we have ùëé2ùëé1 = 1, so ùëé1 ‚àôùëé2 2 = ùëé2 ‚Ä¶ Final Answer: {1, 2, 3} Data Creation Manually Review Model Performance Difficulty Review Parser-Based LLM-Based Grading Method Data Correctness MO Syllabus Validation Quality Review Exist Competitions Web Search Originality Review Figure 2: The construction and grading pipeline of AMO-Bench. 2 AMO-Bench In this section, we first introduce the construction process of AMO-Bench (Section 2.1) and present the basic statistics of this dataset (Section 2.2). Then, we elaborate on the grading methodology designed for AMO-Bench (Section 2.3). Figure 2 briefly illustrate the construction and grading pipeline of AMO-Bench. 2.1 Construction Pipeline To ensure the high standards of quality, originality, and difficulty level in our dataset, we have built up a comprehensive multi-stage construction pipeline that covers the entire process from question creation to final inclusion. This pipeline comprises four major stages: data creation, quality review, originality review, and difficulty review. Data creation. All problems are independently designed by mathematics experts from top universities and educational institutions. These experts have extensive backgrounds in high school mathematics competitions, either having won MO-level mathematics competition awards or possessing experience in competition problem design. Beyond the final answer, each problem author must provide a detailed step-by-step solution. These annotated solutions will be utilized in the subsequent quality review stage and will also aid in assessing the overall difficulty of AMO-Bench (see Section 2.2 for details). Quality review. Each candidate problem undergoes blind review by at least three experts to assess its quality. This quality review stage focuses primarily on two aspects: ‚Ä¢ Whether the problem statement and solution are semantically unambiguous and logically correct. ‚Ä¢ Whether the mathematical knowledge required for the problem is within the scope typically covered in MO-level competitions such as IMO. Originality review. The originality review stage aims to ensure that these newly created problems are not mere rewrites of publicly available materials, but demonstrate genuine originality. To this end, we assess the originality of each problem through the following methods: ‚Ä¢ Compare it against problems in existing datasets (e.g., AIME24/25) with 10-gram matching. ‚Ä¢ Conduct web searches to identify any similar online content. Additionally, during the quality review stage, experts are also required to indicate whether they have encountered highly similar questions in past competitions. Difficulty review. To ensure that AMO-Bench presents a sufficient challenge to state-of-the-art LLMs, we implement a difficulty review stage to filter out problems lacking adequate complexity (even if they may be suitable for some MO-level competitions, e.g., the first 10 questions in AIME). Specifically, each selected problem must satisfy the following two criteria: ‚Ä¢ The",
  "AMO-Bench presents a sufficient challenge to state-of-the-art LLMs, we implement a difficulty review stage to filter out problems lacking adequate complexity (even if they may be suitable for some MO-level competitions, e.g., the first 10 questions in AIME). Specifically, each selected problem must satisfy the following two criteria: ‚Ä¢ The problem must meet or exceed the IMO difficulty standards, as verified by the human expert. ‚Ä¢ We employed multiple advanced reasoning models (such as GPT, DeepSeek, and Gemini series models) for prelimi- nary evaluation, requiring that at least two such models fail to correctly and consistently solve the problem3. 3For each model, our preliminary evaluation involves three samples. If all three samples are correct, the model is deemed capable of consistently solving the problem. 3",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Geometry 10% Number Theory 18% Combinatorics 24% Algebraic Equations & Inequalities 22% Functions & Sequences 26% (a) Distribution of problem categories. <512 512-1,024 1,024-2,048 2,048-4,096 >4,096 0 20 40 60 80 100 Solution Length Ratio (%) 93% 7% 0% 0% 0% 47% 40% 10% 3% 0% 20% 40% 26% 12% 2% MATH500 AIME2024 AMO-Bench (b) Comparison of solution lengths. Figure 3: Basic statistics of AMO-Bench. (a) The distribution of problem categories in AMO-Bench. (b) The distribution of human-annotated solutions in AMO-Bench as well as the comparison with MATH500 and AIME24. 2.2 Dataset Statistics Problem categories. Referring several official competition syllabus, we categorize the 50 problems of AMO-Bench into the following five primary categories: Algebraic Equations & Inequalities (11/50), Functions & Sequences (13/50), Geometry (5/50), Number Theory (9/50), and Combinatorics (12/50). Figure 3a show the overall distribution of problem categories in AMO-Bench. Length distribution of human-annotated solutions. Since the problems in our AMO-Bench are equipped with manually annotated solutions, we can preliminarily analyze the reasoning complexity of these problems from the view of solution length. We measure solution length in terms of token count4. Additionally, we compare the distribution of solution lengths with those from AIME245 and MATH5006. Figure 3b illustrates the solution length distributions across these benchmarks. It reveals that solutions in AMO-Bench exhibit significantly higher lengths, indicating that problems in this benchmark are inherently more challenging and require more complex reasoning to arrive at the final answer. We conduct a further analysis of the model solution lengths in Section 3.2. 2.3 Grading Method For evaluating answers generated by LLMs, prior work has primarily utilized two approaches: parser-based grading and LLM-based grading. Parser-based grading offers high efficiency and accuracy when the model‚Äôs response can be successfully parsed; however, its applicability is limited to simple answer formats such as numerical values or sets, making it challenging to assess more complex answers. In contrast, LLM-based grading provides greater flexibility across diverse answer types but may be less efficient and does not consistently guarantee accuracy. To fully leverage the strengths of both grading methods, AMO-Bench employs different grading approaches based on the specific answer type for each problem. Specifically, problems in AMO-Bench are divided into four main answer types: numerical answers (e.g., Example 1), set answers (e.g., Example 2), variable-expression answers (e.g., Example 3 which requires providing the general formula for an arithmetic sequence), and descriptive answers (e.g., Example 4 which involves comprehensively considering multiple scenarios). The prompt templates for used for grading are contained in Appendix A. Example 1: Problem with Numerical Answer Question: Let x1, x2, ¬∑ ¬∑ ¬∑ , x2024 be positive real numbers such that xk + xm ‚â•km for any 1 ‚â§k < m ‚â§2024. Find the minimum value of x1 + x2 + ¬∑ ¬∑ ¬∑ + x2024. Answer: 1382935444 4We use the tokenizer of DeepSeek-V3.1 model to count tokens in solutions. 5https://huggingface.co/datasets/HuggingFaceH4/aime_2024. 6https://huggingface.co/datasets/HuggingFaceH4/MATH-500. 4",
  "of x1 + x2 + ¬∑ ¬∑ ¬∑ + x2024. Answer: 1382935444 4We use the tokenizer of DeepSeek-V3.1 model to count tokens in solutions. 5https://huggingface.co/datasets/HuggingFaceH4/aime_2024. 6https://huggingface.co/datasets/HuggingFaceH4/MATH-500. 4",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Example 2: Problem with Set Answer Question: Find all positive integers n such that for any: an ‚â•an‚àí1 ‚â•an‚àí2 ‚â•¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ a2 ‚â•a1 > 0, satisfying nP k=1 ak = nP k=1 1 ak , the inequality nQ k=1 ak k ‚â•1 holds. Answer: {1, 2, 3} Example 3: Problem with Variable-Expression Answer Question: The sequence {an}‚àû n=1 consists of positive terms, with a1 = 7, a2 = 2, and satisfies the recurrence relation 8a4 n+2 = 3 + 4an+1 + an (n ‚ààN‚àó). Find the general term formula for this sequence. Answer: (2 + ‚àö 3)22‚àín + (2 ‚àí ‚àö 3)22‚àín 2 Example 4: Problem with Descriptive Answer Question: Let n be an integer with n > 2. Real numbers a1, a2, . . . , an satisfy n X k=1 ak = 2n, n X k=1 k |ak| = 4n. Find the minimum value of a2 1 + a2 2 + ¬∑ ¬∑ ¬∑ + a2 n. Answer: For n = 3, the minimum of a2 1 + a2 2 + a2 3 is 12. For n ‚â•4, the minimum of a2 1 + a2 2 + ¬∑ ¬∑ ¬∑ + a2 n is 6n2 5 . For problems requiring numerical, set, or variable-expression answers (39 out of 50), we employ the parser-based grading. The evaluated LLMs are instructed to format their final responses as \\boxed{<answer>}. We then utilize the tools provided by math-verify7 to parse these answers and verify the equivalence with the ground truth. Moreover, if the model answer containing decimal values, we require an accuracy of at least four decimal places. For variable- expression answers, we assign multiple sets of values to the variables in the expression, then verify whether the values of the generated expression match that of the ground-truth expression. We also manually review the parsing results during the preliminary evaluation and adjust the post-processing algorithms. For problems requiring descriptive answers (11 out of 50), we use LLM-based grading with o4-mini (Low) serving as the grading model. To ensure robust assessment, majority voting is performed across five independent grading samples for each response. Additionally, during preliminary evaluation, we manually verify the correctness of LLM-based grades for all descriptive answers and revise answer descriptions where needed to enhance grading accuracy. Grading accuracy. Prior to conducting the large-scale evaluation, we performed a manual quality check to ensure the reliability of the designed grading method. This assessment included 1,000 responses generated by 10 different LLMs. The results indicate that the grading accuracy reached 99.2%, providing strong validation for the effectiveness of the grading method on AMO-Bench. 3 Experiments In this section, we present the experimental results on AMO-Bench. We first describe the experimental setup (Section 3.1), followed by a discussion of the main results and analysis (Section 3.2). 7https://github.com/huggingface/Math-Verify. 5",
  "setup (Section 3.1), followed by a discussion of the main results and analysis (Section 3.2). 7https://github.com/huggingface/Math-Verify. 5",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions GPT-5-Thinking (High) Qwen3-235B-A22B-Thinking-2507 DeepSeek-V3.1-Thinking GPT-5-Thinking (Medium) LongCat-Flash-Thinking o4-mini (High) Gemini-2.5-Pro GLM-4.5 Qwen3-Next-80B-Thinking DeepSeek-R1-0528 o3-mini (High) o4-mini (Medium) Qwen3-Max-Instruct GPT-5-Thinking (Low) Qwen3-Next-80B-Instruct Gemini-2.5-Flash Claude-Sonnet-4.5 LongCat-Flash o3-mini (Medium) DeepSeek-R1 Claude-Opus-4 DeepSeek-V3.1 Kimi-K2 DeepSeek-V3-0324 GPT-4.1 GPT-4o-20241120 0 10 20 30 40 50 AVG@32 (%) 52.4 47.8 47.6 47.3 43.6 40.2 38.7 36.8 34.8 34.3 32.3 30.0 28.8 25.9 18.2 18.1 17.6 14.6 13.1 10.9 10.6 9.8 7.5 5.2 4.1 1.5 Proprietary Models Open Source Models Reasoning Models Non-Reasoning Models Figure 4: The AVG@32 performance of various LLMs on AMO-Bench. 3.1 Experimental Setup Models. To conduct a comprehensive and representative evaluation on AMO-Bench, we select a diverse set of leading LLMs, encompassing both open-source models and proprietary models. Specifically, the evaluation includes top-tier models provided by OpenAI [OpenAI, 2025], Gemini [Gemini Team, 2025], Anthropic [Anthropic, 2025], DeepSeek [Guo et al., 2025], Qwen [Yang et al., 2025], GLM [GLM-4.5 Team, 2025], Moonshot [Kimi Team, 2025], and LongCat [Meituan LongCat Team, 2025a]. In addition to evaluating reasoning models that have been specifically enhanced for long-term thinking tasks, we also incorporated several powerful non-reasoning models to demonstrate their potential in tackling complex reasoning challenges. Sampling settings. We set the temperature of sampling to 1.0 for reasoning models and 0.7 for non-reasoning models. For all evaluated models, we use top-k=50 and top-p=0.95 during sampling. We configure the maximum context/output length to the highest allowable limit for each model during inference. This avoids underestimating the reasoning capabilities of the model due to restrictions on the token budget. To ensure the stability of the final evaluation results, we sampled the results from each model 32 times and reported the average performance of these 32 results as the final metric (denoted as AVG@32). Appendix B illustrates the fluctuation of the average result across different sampling times. It demonstrates that when sampling 32 times, the average model performance exhibits a relatively small fluctuation and rarely appears to reverse the model ranking order. 3.2 Results and Analysis Main results. Figure 4 presents the AVG@32 performance of various leading LLMs, categorized by proprietary/open- source status and reasoning/non-reasoning properties8. Overall, all these models still struggle with the significant challenges presented by AMO-Bench. Even the highest performing model GPT-5-Thinking (High) reaches just 52.4%, while most others score below 40%. This indicates substantial room for improvement in complex reasoning abilities across all current language models. Moreover, both proprietary and open-source reasoning models occupy top ranks in the leaderboard, indicating that recent open-source advancements are closing the gap with leading commercial models. The best-performing open-source model is only about 5% lower than the top proprietary result. Besides reasoning models, some non-reasoning models demonstrate a performance exceeding expectations, such as Qwen3-Max-Instruct 8To facilitate easier reproduction and utilization of AMO-Bench, you can take a fast try on the AMO-Bench-P subset, which includes only the 39 parser-based grading problems from AMO-Bench. Appendix C presents the AVG@32 performance of LLMs on AMO-Bench-P. 6",
  "can take a fast try on the AMO-Bench-P subset, which includes only the 39 parser-based grading problems from AMO-Bench. Appendix C presents the AVG@32 performance of LLMs on AMO-Bench-P. 6",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 0 10000 20000 30000 40000 50000 Average Output Tokens 0 10 20 30 40 50 AVG@32 (%) Qwen3-Next-Thinking Qwen3-Max-Instruct Qwen3-Next-Instruct Claude-Opus-4 GPT-5-Thinking (High) o4-mini (High) o3-mini (High) Gemini-2.5-Pro Gemini-2.5-Flash DeepSeek-R1-0528 DeepSeek-V3.1-Thinking Qwen3-235B-Thinking GLM-4.5 DeepSeek-R1 LongCat-Flash-Thinking Claude-Sonnet-4.5 DeepSeek-V3-0324 DeepSeek-V3.1 Kimi-K2 LongCat-Flash GPT-4.1 GPT-4o-20241120 Figure 5: The AVG@32 performance of LLMs vs. the average model output length. and LongCat-Flash. These non-reasoning models even outperforms several reasoning models such as o3-mini (Medium), indicating their significant potential in tackling complex reasoning tasks. Comparison of reasoning efficiency. Figure 5 shows the average output length and the AVG@32 performance of each model. Overall, it demonstrates a clear trend that higher-performing models tend to require more output tokens. The first-tier models that reach higher than 40% AVG@32 scores utilize more than 35K completion tokens. Even among non-reasoning models, those with superior performance are distinguished by their ability to process more tokens, sometimes reaching levels comparable to reasoning models. Additionally, when examining models within the same series, there are notable improvements in reasoning efficiency over time. For example, o4-mini (High) outperforms o3-mini (High) at similar or slightly increased token counts. Likewise, DeepSeek-V3.1-Thinking shows significant gains compared to DeepSeek-R1-0528 with even significantly less output tokens. Beyond the main results outlined above, we also provide further analysis and insights based on the AMO-Bench experimental findings. The model output length could indicate the reasoning challenge of the benchmark. Section 2.2 provides a pre-analysis of benchmark difficulty based on annotated solution lengths. Here, we offer a post-hoc analysis of benchmark difficulty based on the relationship between model performance and model output length. Figure 6 clearly demonstrates that the average output length of each model increases as the reasoning benchmark becomes more challenging. Specifically, across six models, benchmarks with higher accuracy scores (such as MAH500 and AIME24) correspond to shorter average outputs, while those with lower scores (like AMO-Bench) require significantly longer responses. This suggests that harder benchmarks demand more elaborate reasoning steps or explanations from the models, resulting in increased token usage. These results demonstrate that the model output length could be an indicator of reasoning challenge in the benchmark. Performance on AMO-Bench still benefits from test-time scaling. The reasoning efficiency results discussed above indicate a correlation between model performance and output length. Here, we conduct a more rigorous analysis by directly controlling the reasoning effort for the same model. As shown in the Figure 7, all three models (GPT-5, o4-mini, and o3-mini) exhibit a near-linear growth trend in AVG@32 as the logarithm of average output length increases. Such a trend is highly aligned with earlier experimental observations from existing benchmarks such as MATH500 7",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 10000 20000 30000 40000 50000 Average Output tokens 50 60 70 80 90 100 Accuracy (%) AMO-Bench BeyondAIME HMMT25 AIME25 AIME24 MAH500 LongCat-Flash-Thinking 5000 10000 15000 20000 25000 30000 35000 40000 45000 Average Output tokens 40 50 60 70 80 90 Accuracy (%) AMO-Bench BeyondAIME HMMT25 AIME25 AIME24 MAH500 GLM-4.5 10000 20000 30000 40000 Average Output tokens 50 60 70 80 90 100 Accuracy (%) AMO-Bench BeyondAIME HMMT25 AIME25 AIME24 MAH500 Qwen3-235B-A22B-Thinking-2507 5000 10000 15000 20000 25000 30000 Average Output tokens 40 50 60 70 80 90 100 Accuracy (%) AMO-Bench BeyondAIME HMMT25 AIME25 AIME24 MAH500 Gemini-2.5-Pro 5000 10000 15000 20000 25000 30000 35000 Average Output tokens 50 60 70 80 90 100 Accuracy (%) AMO-Bench BeyondAIME HMMT25 AIME25 AIME24 MAH500 DeepSeek-V3.1-Thinking 5000 10000 15000 20000 25000 30000 35000 Average Output tokens 60 70 80 90 100 Accuracy (%) AMO-Bench BeyondAIME HMMT25 AIME25 AIME24 MAH500 GPT-5-Thinking (High) Figure 6: The relationship between accuracy and average output length on different math benchmarks. 8,192 16,384 32,768 Logarithm of Average Output Length 25 30 35 40 45 50 AVG@32 (%) GPT-5 (Low) GPT-5 (Medium) GPT-5 (High) Model GPT-5 2048 4,096 8,192 16,384 32,768 Logarithm of Average Output Length 10 15 20 25 30 35 40 AVG@32 (%) o4-mini (Low) o4-mini (Medium) o4-mini (High) Model o4-mini 2048 4,096 8,192 16,384 32,768 Logarithm of Average Output Length 5 10 15 20 25 30 AVG@32 (%) o3-mini (Low) o3-mini (Medium) o3-mini (High) Model o3-mini Figure 7: The model performance and output length under different reasoning effort settings. and AIME24 [Muennighoff et al., 2025]. This indicates that further increasing the inference budget will further drive improvements on AMO-Bench. Top-tier models demonstrate promising potential for improvement on AMO-Bench. Existing work reveals that the pass@k performance of the model can reflect its inherent potential to achieve further improvement through reinforcement learning. Inspired by this, we illustrate the pass@k of evaluated models to indicate their inner potential. As shown in Figure 8, the pass@k metric exhibits rapid growth as k increases from 1 to 8, followed by a sustained but gradual improvement as k continues to rise. Notably, the top-tier reasoning models achieve over 70% performance on the pass@32 metric. These results highlight the significant room for improvement in the reasoning capabilities of LLMs. 4 Related Work Evaluating LLMs on mathematical problem solving has been a critical aspect for assessing advancements in reasoning capabilities. Early datasets such as GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] provided 8",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 0 5 10 15 20 25 30 K 20 40 60 80 Pass@K (%) GPT-5-Thinking (High) DeepSeek-V3.1-Thinking Qwen3-235B-Thinking LongCat-Flash-Thinking Qwen3-Max-Instruct LongCat-Flash DeepSeek-V3.1 GPT-4.1 52.4 73.6 78.0 80.3 82.0 47.6 71.3 74.9 76.8 78.0 47.8 70.0 72.4 73.4 74.0 43.6 76.6 80.9 83.5 86.0 28.8 53.8 61.0 65.1 68.0 14.6 35.0 43.9 49.6 54.0 9.8 27.1 35.3 40.5 44.0 4.1 14.3 19.3 22.9 26.0 Figure 8: The the pass@k trend of various LLMs with increasing k. initial explorations to evaluate these abilities. However, model performance on these benchmarks has quickly reached saturation. To further advance the study of mathematical proficiency in LLMs, recent work has shifted toward more challenging benchmarks. In terms of increasing difficulty, two primary lines of work have emerged. One line focuses on Mathematical Olympiad (MO)-level problems, which rely on a specific range of math knowledge and require complex and intuitive reasoning skills. For instance, Omni-MATH [Gao et al., 2024] introduces a multi-subject evaluation suite designed to rigorously test mathematical reasoning and generalization; OlympiadBench [He et al., 2024] focuses on evaluating the bilingual and multi-modal reasoning abilities with Olympid-level challenges; OlymMATH [Sun et al., 2025] collects MO-level problems from printed publications and evaluates mathematical reasoning by offering problems of two difficulty levels; MathOdyssey [Fang et al., 2025] broadens the scope to include more complex tasks, with a particular focus on long-range and compositional reasoning; BeyondAIME [ByteDance-Seed, 2025] collects problems similar in style to AIME with increased difficulty and expanded data scale; MathArena [Balunovi¬¥c et al., 2025] rapidly tracks model performance in newly held MO-level competitions and explores evaluation paradigms for proof-based competitions such as the IMO and USAMO. Our proposed AMO-Bench also falls within this category and it stands as one of the most challenging benchmarks at the time of writing. The other line of work focuses on problems derived from graduate-level examinations or advanced mathematical research. For instance, RealMath [Zhang et al., 2025] provides a comprehensive evaluation of LLMs in real-world mathematical tasks, assessing their reasoning capabilities across a diverse range of research-level content; Frontier- Math [Glazer et al., 2024] covers computationally intensive problems and abstract questions across most branches of mathematics, highlighting the significant gap between LLMs and the prowess of the mathematical community; HARDMath2 [Roggeveen et al., 2025] focuses on approximation-based mathematical problems, particularly those commonly encountered in applied sciences and engineering; HLE [Phan et al., 2025] constructs a final closed-ended academic benchmark spanning multiple subjects, evaluating reasoning capabilities on human frontier knowledge. Beside requiring the reasoning abilities, these datasets also challenge models by demanding extensive and deep mathematical knowledge. 9",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 5 Conclusion We introduce AMO-Bench, an advanced mathematical reasoning benchmark featuring problems at the level of mathematical Olympiads or higher. The benchmark consists of 50 human-crafted questions designed to rigorously assess advanced mathematical reasoning. Compared with existing benchmarks, AMO-Bench offers more challenging assessments by ensuring that all 50 problems are entirely original and meet or exceed IMO difficulty standards. Each problem in AMO-Bench requires only a final answer rather than a full proof, enabling automatic and robust grading for evaluation purposes. Experimental results across various LLMs demonstrate that contemporary LLMs still struggle with the significant challenges presented by AMO-Bench. Despite these low performances, our further analysis underscore substantial opportunities for advancing mathematical reasoning capabilities in current LLMs. Acknowledgments We thank Zijian Zhang, Jun Kuang, Yiyang Li, Siyu Ren, Zongyu Wang, Yaoming Zhu, Ziyi Zhao, Linsen Guo, Yuhuai Wei, Cunguang Wang, Jiaming Wang and Mengjie Cao for their insightful suggestions regarding the construction and analysis of AMO-Bench. We are grateful to Wei Wang, Wenjie Shi, Jiaqi Zhang, Xiangyu Xi, Xiangzhou Huang, Rongxiang Weng, and Jingang Wang for the valuable discussions and insights on model performance. We also appreciate the engineering support provided by Yunke Zhao and Dengchang Zhao, and open-source assistance from Qi Li, Peng Wang and Xiangyang Ji. References Meituan LongCat Team. Longcat-flash-thinking technical report, 2025a. URL https://arxiv.org/abs/2509.18883. OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. OpenAI. Gpt-5 system card, 2025. URL https://cdn.openai.com/gpt-5-system-card.pdf. Anthropic. System card: Claude opus 4 and claude sonnet 4, 2025. URL https://www-cdn.anthropic.com/4263b 940cabb546aa0e3283f35b686f4f3b2ff47.pdf. xAI. Grok 4 model card, 2025. URL https://data.x.ai/2025-08-20-grok-4-model-card.pdf. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633‚Äì638, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Meituan LongCat Team. Longcat-flash technical report, 2025b. URL https://arxiv.org/abs/2509.01322. GLM-4.5 Team. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/ abs/2508.06471. ByteDance Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. Tencent Hunyuan Team. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kimi Team. Kimi k2:",
  "URL https://arxiv.org/abs/2509.01322. GLM-4.5 Team. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/ abs/2508.06471. ByteDance Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. Tencent Hunyuan Team. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Mislav Balunovi¬¥c, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi¬¥c, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. 10",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828‚Äì3850, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, 2024. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem- solving skills in large language models using odyssey math data. Scientific Data, 12(1):1392, 2025. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models. arXiv preprint arXiv:2503.21380, 2025. Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi¬¥c, Nikola Jovanovi¬¥c, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025. URL https: //arxiv.org/abs/2503.21934. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple test-time scaling. In Workshop on Reasoning and Planning for Large Language Models, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads, 2025. URL https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME. Jie Zhang, Cezara Petrui, Kristina Nikoli¬¥c, and Florian Tram√®r. Realmath: A continuous benchmark for evaluating language models on research-level mathematics. arXiv preprint arXiv:2505.12575, 2025. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. James V Roggeveen, Erik Y Wang, Will Flintoft, Peter Donets, Lucy S Nathwani, Nickholas Gutierrez, David Ettel, Anton Marius Graf, Siddharth Dandavate, Arjun Nageswaran, et al. Hardmath2: A benchmark for applied mathematics built by students as part of a graduate class. arXiv preprint arXiv:2505.11774, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity‚Äôs last exam. arXiv preprint arXiv:2501.14249, 2025. 11",
  "preprint arXiv:2501.14249, 2025. 11",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions A Prompt Templates Query prompt template. In order to guide LLMs in generating answers in a parser-readable format, we use the following prompt template guide the model generation. There are mainly three requirements in the instruction: the answer prefix (i.e., ### The final answer is:), the LaTeX box environment (i.e., \\boxed{}), and the precision requirement. Example 5: Query Prompt Template ... After solving the above problem, please output your final answer in the following format: ### The final answer is: $\\boxed{<your answer>}$ Example: ### The final answer is: $\\boxed{123}$ The final answer should be given as precisely as possible (using LaTeX symbols such as \\sqrt, \\frac, \\pi, etc.). If the final answer involves a decimal approximation, it must be accurate to at least four decimal places. Grading prompt template. We employ the LLM-based grading using o4-mini (Low) as the grading model, and use the following grading prompt to verify the equivalence between the LLM output and the reference answer. Example 6: Grading Prompt Template For the following math problem, we have the reference answer and the student‚Äôs answer. Determine whether the student‚Äôs answer is equivalent to the reference answer. If equivalent, output \"Correct\". If not equivalent, output \"Incorrect\". ### Problem ... ### Reference Answer ... ### Student Answer ... Now, please provide your judgment. Please strictly follow the format below to summarize your conclusion at the end of your judgment: ### Conclusion: Correct/Incorrect If the answer involves a decimal approximation, it must be accurate to at least four decimal places. B Analysis of AVG@k Figure 9 illustrates the fluctuation of the average performance across different sampling times. It shows that as the sampling time grows, the models‚Äô performance become more stable. When sampling 32 times, it rarely appears the reverse-order phenomenon. C Performance on AMO-Bench-P Subset To facilitate easier reproduction and use of AMO-Bench, you can utilize the AMO-Bench-P subset, which includes only the 39 parser-based grading problems from AMO-Bench. Table 1 presents the AVG@32 performance of LLMs on AMO-Bench-P. In general, performance on AMO-Bench-P tends to be slightly higher than on the full AMO-Bench, as problems requiring complex descriptive answers are inherently more challenging than those with simple-format answers. 12",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 0 5 10 15 20 25 30 K 10 20 30 40 50 60 AVG@K (%) GPT-5-Thinking (High) DeepSeek-V3.1-Thinking LongCat-Flash-Thinking GLM-4.5 Qwen3-Max-Instruct LongCat-Flash DeepSeek-V3.1 GPT-4.1 58.0 56.0 53.3 52.5 52.4 53.0 53.7 53.2 52.9 52.6 52.7 52.3 52.6 52.1 52.5 52.1 52.0 51.7 52.0 51.7 51.9 52.2 52.2 52.0 52.0 52.2 51.9 52.1 52.2 52.3 52.5 52.4 50.0 50.0 48.0 48.0 48.0 47.7 47.4 47.5 47.3 48.0 48.5 47.7 48.0 48.0 47.5 47.0 46.7 46.8 47.1 47.1 47.3 47.4 47.1 47.2 47.2 47.4 47.5 47.4 47.2 47.3 47.4 47.6 40.0 42.0 42.7 46.0 46.0 46.7 46.0 46.0 46.0 46.4 46.7 46.0 45.2 45.0 44.8 45.1 45.3 45.1 44.6 44.2 44.2 44.0 44.2 44.5 44.5 44.2 43.9 43.7 43.6 43.5 43.7 43.6 42.0 41.0 40.0 38.5 36.8 37.0 36.9 35.5 34.9 35.0 35.6 36.0 35.8 35.9 35.9 36.0 36.0 36.8 37.1 36.7 36.9 36.9 37.0 37.0 37.0 36.8 36.9 36.9 36.9 36.9 36.9 36.8 26.0 27.0 27.3 26.5 26.8 26.7 26.6 27.5 27.1 28.4 28.5 28.5 28.2 27.6 27.9 27.8 28.0 27.4 28.1 28.0 28.1 28.4 28.5 28.7 28.9 29.0 29.0 28.8 28.6 28.8 28.8 28.8 20.0 16.0 14.0 13.0 13.6 14.7 14.3 14.3 14.4 14.8 14.7 15.0 15.2 15.0 15.3 15.6 15.5 15.2 15.3 15.2 15.3 15.2 15.0 15.0 14.9 14.8 14.7 14.7 14.5 14.5 14.6 14.6 12.0 13.0 11.3 10.5 10.4 11.0 11.4 10.5 9.8 10.0 10.4 10.2 10.0 9.9 10.3 10.4 10.4 10.2 10.3 10.3 10.2 10.1 10.1 10.2 10.2 10.2 10.1 10.1 10.0 9.9 9.9 9.8 6.0 6.0 4.0 4.0 4.8 5.3 5.4 5.5 5.3 5.4 5.3 5.0 4.8 4.6 4.4 4.1 4.4 4.4 4.4 4.3 4.3 4.3 4.2 4.2 4.0 4.1 4.1 4.1 4.0 4.1 4.0 4.1 Figure 9: The AVG@k trend of various LLMs with increasing k. 13",
  "AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Table 1: The AVG@32 performance of LLMs on the AMO-Bench and AMO-Bench-P, the latter of which contains only 39 parser-based grading problems. Model AMO-Bench AMO-Bench-P GPT-5-Thinking (High) 52.4 54.8 Qwen3-235B-A22B-Thinking-2507 47.8 56.2 DeepSeek-V3.1-Thinking 47.6 53.0 LongCat-Flash-Thinking 43.6 45.3 o4-mini (High) 40.2 43.8 Gemini-2.5-Pro 38.7 41.7 GLM-4.5 36.8 41.0 Qwen3-Next-80B-Thinking 34.8 37.4 DeepSeek-R1-0528 34.3 37.1 o3-mini (High) 32.3 34.0 Qwen3-Max-Instruct 28.8 30.9 Qwen3-Next-80B-Instruct 18.2 17.8 Gemini-2.5-Flash 18.1 18.0 Claude-Sonnet-4.5 17.6 18.1 LongCat-Flash 14.6 14.9 DeepSeek-R1 10.9 11.7 Claude-Opus-4 10.6 11.4 DeepSeek-V3.1 9.8 9.6 Kimi-K2 7.5 8.4 DeepSeek-V3-0324 5.2 5.4 GPT-4.1 4.1 4.8 GPT-4o-20241120 1.5 1.9 14"
]