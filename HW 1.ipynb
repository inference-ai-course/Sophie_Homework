{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "856cb073-df21-44a6-a863-09e70e3ed765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at various locations, including:\n",
      "\n",
      "* Dodger Stadium in Los Angeles, California (home of the LA Dodgers)\n",
      "* Fenway Park in Boston, Massachusetts (home of the Boston Red Sox)\n",
      "* Globe Life Field in Arlington, Texas (home of the Texas Rangers)\n",
      "* Nationals Park in Washington, D.C. (home of the Washington Nationals)\n",
      "* Oracle Park in San Francisco, California (home of the San Francisco Giants)\n",
      "\n",
      "The series was played from October 21 to November 5, 2020, with the LA Dodgers winning in 6 games over the Boston Red Sox.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a86f0f-acce-4dbb-86b5-15bd48296569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python314\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "C:\\Users\\FeiFei\\AppData\\Local\\Temp\\ipykernel_14220\\3867333074.py:12: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"llama2\")  # Using Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: 'What is the capital of Germany?'\n",
      "Model answer: The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama \n",
    "\n",
    "# 2. Define the prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the capital of {topic}?\"\n",
    ")\n",
    "\n",
    "# 3. Define the model\n",
    "model = ChatOllama(model=\"llama2\")  # Using Ollama \n",
    "\n",
    "# 4. Chain the components together using LCEL\n",
    "chain = (\n",
    "    # LCEL syntax: use the pipe operator | to connect each step\n",
    "    {\"topic\": RunnablePassthrough()}  # Accept user input\n",
    "    | prompt                          # Transform it into a prompt message\n",
    "    | model                           # Call the model\n",
    "    | StrOutputParser()               # Parse the output as a string\n",
    ")\n",
    "\n",
    "# 5. Execute\n",
    "result = chain.invoke(\"Germany\")\n",
    "print(\"User prompt: 'What is the capital of Germany?'\")\n",
    "print(\"Model answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fed1cef-9b28-4baf-ac49-54d714e46d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# app.py\n",
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "# Optional: import LangChain (if installed and you want to use it)\n",
    "try:\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    from langchain.schema import HumanMessage, SystemMessage\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "\n",
    "# === Configure OpenAI client to point to Ollama ===\n",
    "# Either set env vars or hardcode base_url below.\n",
    "OLLAMA_BASE = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n",
    "\n",
    "# Create an OpenAI client that will talk to Ollama\n",
    "client = OpenAI(\n",
    "    base_url=OLLAMA_BASE,\n",
    "    api_key=\"ollama\"  # some sdk versions require a key param; Ollama ignores it\n",
    ")\n",
    "\n",
    "# Optional: LangChain wrapper configured to talk to Ollama\n",
    "def make_langchain_model(model_name: str):\n",
    "    if not LANGCHAIN_AVAILABLE:\n",
    "        raise RuntimeError(\"LangChain not installed. pip install langchain to enable.\")\n",
    "    # Many LangChain wrappers pick up OPENAI_API_BASE from env.\n",
    "    # We'll instruct the user to set it; but we can also construct ChatOpenAI with model_name directly.\n",
    "    # NOTE: Behavior depends on LangChain version. If this errors, use the Direct mode below.\n",
    "    return ChatOpenAI(model_name=model_name, openai_api_base=OLLAMA_BASE, openai_api_key=\"ollama\", temperature=0.2)\n",
    "\n",
    "# === Helper: direct chat call to Ollama via OpenAI SDK ===\n",
    "def call_ollama_direct(model: str, messages: list, max_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    Calls Ollama using the OpenAI-compatible SDK (openai.OpenAI).\n",
    "    Returns the text string returned by the model (best attempt).\n",
    "    \"\"\"\n",
    "    # Use the chat.completions endpoint\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    # Debug: if you need to inspect the full object, you can return json.dumps(resp, default=str)\n",
    "    try:\n",
    "        # Standard structured response path\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception:\n",
    "        # Fallback for streaming or delta-style responses\n",
    "        try:\n",
    "            return resp.choices[0].delta.get(\"content\", \"\")\n",
    "        except Exception:\n",
    "            return str(resp)\n",
    "\n",
    "# === Example translator helper (very simple) ===\n",
    "def translate_text(text: str, source_lang: str, target_lang: str, model=\"llama3\"):\n",
    "    prompt = (\n",
    "        f\"You are a translator. Translate the following text from {source_lang} to {target_lang}.\\n\\n\"\n",
    "        f\"Text:\\n{text}\\n\\n\"\n",
    "        \"Return only the translation (no extra commentary).\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful translator.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    return call_ollama_direct(model=model, messages=messages, max_tokens=512)\n",
    "\n",
    "# === Gradio UI callbacks ===\n",
    "def handle_direct_chat(user_input: str, model_name: str, translate: bool, src_lang: str, tgt_lang: str):\n",
    "    # Prepare messages for the model (example few-shot + conversation)\n",
    "    if translate:\n",
    "        # translate the user input first\n",
    "        translated = translate_text(user_input, src_lang, tgt_lang, model=model_name)\n",
    "        user_for_model = f\"(Translated input from {src_lang} to {tgt_lang}):\\n{translated}\"\n",
    "    else:\n",
    "        user_for_model = user_input\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise and helpful.\"},\n",
    "        {\"role\": \"user\", \"content\": user_for_model}\n",
    "    ]\n",
    "    answer = call_ollama_direct(model=model_name, messages=messages)\n",
    "    return answer\n",
    "\n",
    "def handle_langchain_chat(user_input: str, model_name: str, translate: bool, src_lang: str, tgt_lang: str):\n",
    "    if not LANGCHAIN_AVAILABLE:\n",
    "        return \"LangChain not installed. Install it with: pip install langchain\"\n",
    "    # Build LangChain model wrapper\n",
    "    lc_model = make_langchain_model(model_name)\n",
    "    if translate:\n",
    "        translated = translate_text(user_input, src_lang, tgt_lang, model=model_name)\n",
    "        final_input = f\"(Translated input from {src_lang} to {tgt_lang}):\\n{translated}\"\n",
    "    else:\n",
    "        final_input = user_input\n",
    "\n",
    "    # Create simple LangChain chat invocation\n",
    "    messages = [SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=final_input)]\n",
    "    lc_resp = lc_model(messages)\n",
    "    # The returned type depends on LangChain version; try common attributes\n",
    "    try:\n",
    "        return lc_resp.content\n",
    "    except Exception:\n",
    "        try:\n",
    "            return str(lc_resp)\n",
    "        except Exception:\n",
    "            return \"LangChain returned an unexpected response; inspect logs.\"\n",
    "\n",
    "# === Gradio layout ===\n",
    "with gr.Blocks(title=\"Ollama + LangChain → Gradio Proxy Demo\") as demo:\n",
    "    gr.Markdown(\"# Ollama + LangChain Proxy UI (demo)\\n\"\n",
    "                \"Use **Direct** mode to call Ollama via the OpenAI SDK, or **LangChain** mode to route through LangChain.\\n\\n\"\n",
    "                \"Make sure Ollama is running (`ollama serve`) and you have pulled a model (e.g. `ollama pull llama3`).\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            user_input = gr.Textbox(label=\"User input\", placeholder=\"Enter prompt / question\", lines=4)\n",
    "            model_name = gr.Textbox(label=\"Model name (Ollama)\", value=\"llama3\")\n",
    "            translate_toggle = gr.Checkbox(label=\"Translate first (proxy translator)\", value=False)\n",
    "            src_lang = gr.Textbox(label=\"Source language (when translate)\", value=\"Chinese\")\n",
    "            tgt_lang = gr.Textbox(label=\"Target language (when translate)\", value=\"English\")\n",
    "            run_direct = gr.Button(\"Run — Direct Ollama\")\n",
    "            run_langchain = gr.Button(\"Run — LangChain (optional)\")\n",
    "        with gr.Column(scale=3):\n",
    "            output = gr.Textbox(label=\"Model output\", lines=12)\n",
    "            raw_debug = gr.Textbox(label=\"Raw debug output (optional)\", lines=8)\n",
    "\n",
    "    # Button callbacks\n",
    "    run_direct.click(fn=lambda inp, m, t, s, g: handle_direct_chat(inp, m, t, s, g),\n",
    "                     inputs=[user_input, model_name, translate_toggle, src_lang, tgt_lang],\n",
    "                     outputs=[output])\n",
    "\n",
    "    run_langchain.click(fn=lambda inp, m, t, s, g: handle_langchain_chat(inp, m, t, s, g),\n",
    "                        inputs=[user_input, model_name, translate_toggle, src_lang, tgt_lang],\n",
    "                        outputs=[output])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: set environment variable for LangChain (if using it)\n",
    "    os.environ.setdefault(\"OPENAI_API_BASE\", OLLAMA_BASE)\n",
    "    os.environ.setdefault(\"OPENAI_API_KEY\", \"ollama\")\n",
    "\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
