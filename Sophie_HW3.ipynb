{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fe7325-7da2-48c2-bafb-551c57b7da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastapi uvicorn[standard] whisper transformers cosyvoice torchaudio nest-asyncio pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2db51-d40e-463a-b962-2d6907f2a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install required packages (run once)\n",
    "# !pip install openai-whisper transformers torch torchaudio --quiet\n",
    "# Note: Skip CozyVoice for now, we'll use a placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f98b734-156c-4341-b391-620f118c37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  Hello, where's the capital of Canada?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot Response: user:  Hello, where's the capital of Canada?\n",
      "                                                  \n",
      "Audio saved at: response.wav\n"
     ]
    }
   ],
   "source": [
    "# Issue: echo user's content\n",
    "\n",
    "#Version 3: change to use tiny model with test.wav\n",
    "\n",
    "# Step 1: Imports\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import torchaudio\n",
    "\n",
    "# Step 2: ASR - Whisper Tiny\n",
    "asr_model = whisper.load_model(\"tiny\")  # lightweight\n",
    "\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Test ASR\n",
    "#with open(\"test.wav\", \"rb\") as f:\n",
    "#    audio_bytes = f.read()\n",
    "\n",
    "user_text = transcribe_audio(audio_bytes)\n",
    "print(\"Transcribed Text:\", user_text)\n",
    "\n",
    "# Step 3: LLM - lightweight text generation\n",
    "llm = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\")  # tiny model\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(user_text):\n",
    "    conversation_history.append({\"role\": \"user\", \"text\": user_text})\n",
    "    prompt = \"\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        prompt += f\"{turn['role']}: {turn['text']}\\n\"\n",
    "    outputs = llm(prompt, max_new_tokens=50)\n",
    "    bot_response = outputs[0][\"generated_text\"]\n",
    "    conversation_history.append({\"role\": \"assistant\", \"text\": bot_response})\n",
    "    return bot_response\n",
    "\n",
    "bot_text = generate_response(user_text)\n",
    "print(\"Bot Response:\", bot_text)\n",
    "\n",
    "# Step 4: TTS placeholder\n",
    "# Since CozyVoice is heavy, we can generate a simple TTS using torchaudio + pre-recorded voice, or just save text\n",
    "def synthesize_speech(text):\n",
    "    # placeholder: save text to a file (or use simple pyttsx3 for local TTS)\n",
    "    audio_path = \"response.wav\"\n",
    "    with open(\"response.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    return audio_path\n",
    "\n",
    "audio_path = synthesize_speech(bot_text)\n",
    "print(\"Audio saved at:\", audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1763827-2ea9-496a-b633-7d14aa918031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  Hello, where is the capital of Canada?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot Response: The capital of Canada is Ottawa.\n",
      "Audio saved at: response.wav\n"
     ]
    }
   ],
   "source": [
    "# Good\n",
    "# model=\"EleutherAI/gpt-neo-1.3B\",\n",
    "# model=\"EleutherAI/gpt-neo-125M\",\n",
    "\n",
    "# Step 1: Imports\n",
    "import whisper\n",
    "from transformers import pipeline\n",
    "import torchaudio\n",
    "\n",
    "asr_model = whisper.load_model(\"small\")  # lightweight\n",
    "\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Test ASR\n",
    "with open(\"test.wav\", \"rb\") as f:\n",
    "   audio_bytes = f.read()\n",
    "\n",
    "user_text = transcribe_audio(audio_bytes)\n",
    "print(\"Transcribed Text:\", user_text)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use FLAN-T5 instead of GPT-Neo\n",
    "# small = faster but weaker; base = better but slower\n",
    "llm = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    device=-1  # -1 = CPU; change to 0 for GPU\n",
    ")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(user_text):\n",
    "    conversation_history.append({\"role\": \"user\", \"text\": user_text})\n",
    "\n",
    "    # Build concise prompt for instruction-tuned model\n",
    "    prompt = \"You are a friendly, factual assistant.\\n\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        prompt += f\"User: {turn['text']}\\n\"\n",
    "    prompt += \"Assistant:\"\n",
    "\n",
    "    outputs = llm(prompt, max_new_tokens=150)\n",
    "    bot_response = outputs[0][\"generated_text\"].strip()\n",
    "    conversation_history.append({\"role\": \"assistant\", \"text\": bot_response})\n",
    "    return bot_response\n",
    "\n",
    "bot_text = generate_response(user_text)\n",
    "print(\"Bot Response:\", bot_text)\n",
    "\n",
    "# Step 4: TTS placeholder\n",
    "# Since CozyVoice is heavy, we can generate a simple TTS using torchaudio + pre-recorded voice, or just save text\n",
    "def synthesize_speech(text):\n",
    "    # placeholder: save text to a file (or use simple pyttsx3 for local TTS)\n",
    "    audio_path = \"response.wav\"\n",
    "    with open(\"response.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    return audio_path\n",
    "\n",
    "audio_path = synthesize_speech(bot_text)\n",
    "print(\"Audio saved at:\", audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b979143-42bb-4790-be8f-0b1da5951245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 112, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\responses.py\", line 365, in __call__\n",
      "    await self._handle_simple(send, send_header_only, send_pathsend)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\responses.py\", line 396, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 507, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_connection.py\", line 538, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_connection.py\", line 571, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 112, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\responses.py\", line 365, in __call__\n",
      "    await self._handle_simple(send, send_header_only, send_pathsend)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\responses.py\", line 396, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 507, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_connection.py\", line 538, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_connection.py\", line 571, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said:  Hello, who is the President of the United States?\n",
      "Bot responds: The President of the United States is Donald Trump.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Temp\\ipykernel_11836\\2046052876.py\", line 16, in voice_chat\n",
      "    audio_path = synthesize_speech(bot_text, \"gradio_response.wav\")\n",
      "TypeError: synthesize_speech() takes 1 positional argument but 2 were given\n",
      "C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said:  How many populations are there in Canada?\n",
      "Bot responds: User: Canada has a population of over 1.2 million people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\FeiFei\\AppData\\Local\\Temp\\ipykernel_11836\\2046052876.py\", line 16, in voice_chat\n",
      "    audio_path = synthesize_speech(bot_text, \"gradio_response.wav\")\n",
      "TypeError: synthesize_speech() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def voice_chat(audio):\n",
    "    if audio is None:\n",
    "        return None, \"\"\n",
    "    \n",
    "    with open(audio, \"rb\") as f:\n",
    "        audio_bytes = f.read()\n",
    "    \n",
    "    user_text = transcribe_audio(audio_bytes)\n",
    "    print(f\"User said: {user_text}\")\n",
    "    \n",
    "    bot_text = generate_response(user_text)\n",
    "    print(f\"Bot responds: {bot_text}\")\n",
    "    \n",
    "    audio_path = synthesize_speech(bot_text, \"gradio_response.wav\")\n",
    "    \n",
    "    return audio_path, bot_text\n",
    "\n",
    "gr.Interface(\n",
    "    fn=voice_chat,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"Bot Voice Response\"),\n",
    "        gr.Textbox(label=\"Bot Text Response\", lines=5)\n",
    "    ]\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
